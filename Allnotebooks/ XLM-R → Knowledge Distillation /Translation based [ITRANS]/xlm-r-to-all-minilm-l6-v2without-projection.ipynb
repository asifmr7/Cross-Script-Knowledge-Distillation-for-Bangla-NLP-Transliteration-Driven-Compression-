{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceId":13581507,"sourceType":"datasetVersion","datasetId":8628620}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir transformers==4.45.2 -q\n!pip install --no-cache-dir scikit-learn pandas tqdm matplotlib sentencepiece -q\n\nimport os, json, random, numpy as np, torch\nfrom pathlib import Path\n\n# Repro & device\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nWORK_DIR = Path(\"/kaggle/working\"); WORK_DIR.mkdir(parents=True, exist_ok=True)\n\n# Models\nTEACHER_MODEL_ID = \"xlm-roberta-base\"\nSTUDENT_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"  # student head will be freshly initialized\n\n# Data / training config\nMAX_LEN   = 128\nBATCH_SIZE = 16\n\n# Teacher FT\nEPOCHS_TEACHER   = 3\nLR_TEACHER       = 2e-5\nWARMUP_RATIO_T   = 0.1\nWEIGHT_DECAY_T   = 0.01\n\n# KD (logits-only)\nEPOCHS_STUDENT   = 5\nLR_STUDENT       = 3e-5\nWARMUP_RATIO_S   = 0.1\nWEIGHT_DECAY_S   = 0.01\nPATIENCE         = 2\n\nKD_T      = 3.0\nKD_ALPHA  = 0.5\n\nprint(\"‚úÖ Device:\", DEVICE)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:23:10.547746Z","iopub.execute_input":"2025-11-02T13:23:10.548019Z","iopub.status.idle":"2025-11-02T13:23:32.142331Z","shell.execute_reply.started":"2025-11-02T13:23:10.547997Z","shell.execute_reply":"2025-11-02T13:23:32.141552Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m139.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports, Config","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nDATA_DIR = Path(\"/kaggle/input/dataaaaaa\")\nPOS_FILE = DATA_DIR / \"all_positive_8500.txt\"\nNEG_FILE = DATA_DIR / \"all_negative_3307.txt\"\nassert POS_FILE.exists() and NEG_FILE.exists(), f\"Missing files:\\n{POS_FILE}\\n{NEG_FILE}\"\n\ndef read_txt(p: Path):\n    with open(p, encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos = read_txt(POS_FILE)\nneg = read_txt(NEG_FILE)\n\ndf = pd.DataFrame({\n    \"text\":  pos + neg,\n    \"label\": [1]*len(pos) + [0]*len(neg)\n}).sample(frac=1, random_state=SEED).reset_index(drop=True)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\nval_df,   test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)\n\nprint(f\"Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}\")\ntrain_df.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:23:32.143636Z","iopub.execute_input":"2025-11-02T13:23:32.143965Z","iopub.status.idle":"2025-11-02T13:23:33.104123Z","shell.execute_reply.started":"2025-11-02T13:23:32.143947Z","shell.execute_reply":"2025-11-02T13:23:33.103548Z"}},"outputs":[{"name":"stdout","text":"Train=9445 | Val=1181 | Test=1181\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                    text  label\n11010                          ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞‡ßá ‡¶†‡ßç‡¶Ø‡¶æ‡¶Ç  ‡¶Ü‡¶¨‡¶æ‡¶∞‡¶ì ‡¶†‡ßç‡¶Ø‡¶æ‡¶Ç      1\n2147   ‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡ßç‡¶Æ‡¶§ ‡¶®‡¶æ‡¶ü‡¶ï ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®‡•§‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶á‡¶Æ‡ßá‡¶ú ‡¶®‡¶∑‡ßç‡¶ü ‡¶ï‡¶∞‡¶¨‡ßá‡¶®...      0\n2509                              ‡¶Ö‡¶®‡ßá‡¶ï ‡¶Ö‡¶®‡ßá‡¶ï  ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶π‡ßü‡¶õ‡¶ø      1\n8380   ‡¶è‡¶ï ‡¶ï‡¶•‡¶æ‡ßü ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶® ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶æ‡¶ü‡¶ï...‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶æ best ‡¶®‡¶æ‡¶ü...      1\n3082                          ‡¶®‡¶æ‡¶ü‡¶ï‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ù‡ßá ‡¶è‡¶° ‡¶¶‡ßá‡ßü ‡¶ï‡ßá‡¶® ‡¶¨‡¶æ‡¶≤      0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>11010</th>\n      <td>‡¶Ü‡¶™‡¶®‡¶æ‡¶∞‡ßá ‡¶†‡ßç‡¶Ø‡¶æ‡¶Ç  ‡¶Ü‡¶¨‡¶æ‡¶∞‡¶ì ‡¶†‡ßç‡¶Ø‡¶æ‡¶Ç</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2147</th>\n      <td>‡¶Æ‡¶æ‡¶® ‡¶∏‡¶Æ‡ßç‡¶Æ‡¶§ ‡¶®‡¶æ‡¶ü‡¶ï ‡¶§‡ßà‡¶∞‡¶ø ‡¶ï‡¶∞‡ßÅ‡¶®‡•§‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶á‡¶Æ‡ßá‡¶ú ‡¶®‡¶∑‡ßç‡¶ü ‡¶ï‡¶∞‡¶¨‡ßá‡¶®...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2509</th>\n      <td>‡¶Ö‡¶®‡ßá‡¶ï ‡¶Ö‡¶®‡ßá‡¶ï  ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶π‡ßü‡¶õ‡¶ø</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8380</th>\n      <td>‡¶è‡¶ï ‡¶ï‡¶•‡¶æ‡ßü ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶® ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶æ‡¶ü‡¶ï...‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¶‡ßá‡¶ñ‡¶æ best ‡¶®‡¶æ‡¶ü...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3082</th>\n      <td>‡¶®‡¶æ‡¶ü‡¶ï‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ù‡ßá ‡¶è‡¶° ‡¶¶‡ßá‡ßü ‡¶ï‡ßá‡¶® ‡¶¨‡¶æ‡¶≤</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"# Fine-tune Teacher xlmr","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\nclass TxtClsDataset(Dataset):\n    def __init__(self, df, tok, max_len):\n        self.texts = df.text.tolist()\n        self.labels = df.label.tolist()\n        self.tok = tok; self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\",\n                       max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\n# Load teacher\nteacher_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\nteacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=2).to(DEVICE)\n\n# Dataloaders\ntrain_teacher_loader = DataLoader(TxtClsDataset(train_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nval_teacher_loader   = DataLoader(TxtClsDataset(val_df,   teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\ntest_teacher_loader  = DataLoader(TxtClsDataset(test_df,  teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\n# Optimizer & scheduler (PyTorch AdamW = no deprecation warning)\nopt_t = torch.optim.AdamW(teacher.parameters(), lr=LR_TEACHER, weight_decay=WEIGHT_DECAY_T)\nsteps_t = len(train_teacher_loader) * EPOCHS_TEACHER\nsch_t   = get_linear_schedule_with_warmup(opt_t, int(WARMUP_RATIO_T*steps_t), steps_t)\n\nbest_f1 = -1.0\nfor ep in range(1, EPOCHS_TEACHER+1):\n    teacher.train(); run = 0.0\n    for b in tqdm(train_teacher_loader, desc=f\"Teacher Epoch {ep}/{EPOCHS_TEACHER}\"):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward()\n        opt_t.step(); sch_t.step(); opt_t.zero_grad()\n        run += loss.item()\n\n    # Validate\n    teacher.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in val_teacher_loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            logits = teacher(**b).logits\n            preds += logits.argmax(-1).cpu().tolist()\n            gold  += b[\"labels\"].cpu().tolist()\n    acc = accuracy_score(gold, preds)\n    f1m = f1_score(gold, preds, average=\"macro\")\n    print(f\"[Val] Acc={acc:.4f} | F1_macro={f1m:.4f}\")\n\n    if f1m > best_f1:\n        best_f1 = f1m\n        save_dir = WORK_DIR / \"finetuned_xlmr_teacher\"\n        save_dir.mkdir(parents=True, exist_ok=True)\n        teacher.save_pretrained(save_dir); teacher_tok.save_pretrained(save_dir)\n        print(\"üíæ Saved best teacher to\", save_dir)\n\n# Quick test\nteacher.eval(); preds, gold = [], []\nwith torch.no_grad():\n    for b in test_teacher_loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        logits = teacher(**b).logits\n        preds += logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\nprint(\"‚úÖ Teacher[Test]: Acc={:.4f} | F1_macro={:.4f}\".format(\n    accuracy_score(gold, preds), f1_score(gold, preds, average=\"macro\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:23:33.104888Z","iopub.execute_input":"2025-11-02T13:23:33.105281Z","iopub.status.idle":"2025-11-02T13:37:07.095775Z","shell.execute_reply.started":"2025-11-02T13:23:33.105259Z","shell.execute_reply":"2025-11-02T13:37:07.095063Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ae7f4e87cd34347ad2061c678bbe83a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"083816427fee4990b1b79c6216de3978"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53d707b664eb4b9891b67acf51335b34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f686f7edea5c4b8f9a8ad4b8a2af0a78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc19dbc4f80b41d0b91dd0170ff28ae6"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 1/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d964648219544b408b1b3678dae0574f"}},"metadata":{}},{"name":"stdout","text":"[Val] Acc=0.9433 | F1_macro=0.9318\nüíæ Saved best teacher to /kaggle/working/finetuned_xlmr_teacher\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 2/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85f04bfa0e374d99a30c03b1ecbbc334"}},"metadata":{}},{"name":"stdout","text":"[Val] Acc=0.9610 | F1_macro=0.9518\nüíæ Saved best teacher to /kaggle/working/finetuned_xlmr_teacher\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 3/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f947dca90d6b4ee6a929c6828b7bfde5"}},"metadata":{}},{"name":"stdout","text":"[Val] Acc=0.9644 | F1_macro=0.9562\nüíæ Saved best teacher to /kaggle/working/finetuned_xlmr_teacher\n‚úÖ Teacher[Test]: Acc=0.9543 | F1_macro=0.9438\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Transliteration KD Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\n# Student tokenizer\nstudent_tok = AutoTokenizer.from_pretrained(STUDENT_MODEL_ID)\n\nclass KDDataset(Dataset):\n    def __init__(self, df, teacher_tok, student_tok, max_len=128):\n        self.texts = df[\"text\"].tolist()\n        self.labels = df[\"label\"].tolist()\n        self.ttok = teacher_tok; self.stok = student_tok; self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, idx):\n        txt = self.texts[idx]; label = self.labels[idx]\n        t_enc = self.ttok(txt, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        s_enc = self.stok(txt, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"t_input_ids\": t_enc[\"input_ids\"].squeeze(0),\n            \"t_attention_mask\": t_enc[\"attention_mask\"].squeeze(0),\n            \"s_input_ids\": s_enc[\"input_ids\"].squeeze(0),\n            \"s_attention_mask\": s_enc[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(label, dtype=torch.long)\n        }\n\ntrain_loader = DataLoader(KDDataset(train_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(KDDataset(val_df,   teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(KDDataset(test_df,  teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\nprint(\"‚úÖ KD dataloaders ready (XLM-R ‚Üí MiniLM, logits-only)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:37:07.097479Z","iopub.execute_input":"2025-11-02T13:37:07.097891Z","iopub.status.idle":"2025-11-02T13:37:07.759899Z","shell.execute_reply.started":"2025-11-02T13:37:07.097871Z","shell.execute_reply":"2025-11-02T13:37:07.759367Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06fb1d483f43417c8818a5fc07490c83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e862ffdc8ea4ac98bee1a3f192178d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6633147a350b4ac8a596b8abaef16867"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0700607be31f4cb587088a49edbecaf1"}},"metadata":{}},{"name":"stdout","text":"‚úÖ KD dataloaders ready (XLM-R ‚Üí MiniLM, logits-only)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Student model + Logit + Hidden ","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# This will initialize a fresh classification head for 2 labels\nstudent = AutoModelForSequenceClassification.from_pretrained(\n    STUDENT_MODEL_ID, num_labels=2\n).to(DEVICE)\n\nprint(\"‚úÖ Student ready:\", STUDENT_MODEL_ID)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:37:07.760559Z","iopub.execute_input":"2025-11-02T13:37:07.760741Z","iopub.status.idle":"2025-11-02T13:37:08.881248Z","shell.execute_reply.started":"2025-11-02T13:37:07.760727Z","shell.execute_reply":"2025-11-02T13:37:08.880607Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea961bc8db894a338c8fcbfd93d76b60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ee39270124141c69aad609377d622cd"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Student ready: sentence-transformers/all-MiniLM-L6-v2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# KD training loop enabling attentions","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass KDLossLogitsOnly(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5):\n        super().__init__()\n        self.T = T\n        self.alpha = alpha\n        self.ce = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n\n    def forward(self, s_pack, t_pack, labels):\n        logits_s = s_pack[\"logits\"] if isinstance(s_pack, dict) else s_pack.logits\n        logits_t = t_pack[\"logits\"] if isinstance(t_pack, dict) else t_pack.logits\n\n        hard = self.ce(logits_s, labels)\n        soft = self.kld(\n            F.log_softmax(logits_s / self.T, dim=-1),\n            F.softmax(logits_t / self.T, dim=-1)\n        ) * (self.T ** 2)\n\n        return (1 - self.alpha) * hard + self.alpha * soft\n\ncriterion = KDLossLogitsOnly(T=KD_T, alpha=KD_ALPHA)\nprint(\"‚úÖ KD criterion (logits-only) ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:37:08.881880Z","iopub.execute_input":"2025-11-02T13:37:08.882124Z","iopub.status.idle":"2025-11-02T13:37:08.889028Z","shell.execute_reply.started":"2025-11-02T13:37:08.882099Z","shell.execute_reply":"2025-11-02T13:37:08.888303Z"}},"outputs":[{"name":"stdout","text":"‚úÖ KD criterion (logits-only) ready.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from transformers import get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Freeze teacher\nteacher.eval()\nfor p in teacher.parameters(): \n    p.requires_grad = False\n\n# Optimizer & scheduler for student (PyTorch AdamW)\nopt_s = torch.optim.AdamW(student.parameters(), lr=LR_STUDENT, weight_decay=WEIGHT_DECAY_S)\nsteps_s = len(train_loader) * EPOCHS_STUDENT\nsch_s   = get_linear_schedule_with_warmup(opt_s, int(WARMUP_RATIO_S*steps_s), steps_s)\n\ndef compute_metrics(preds, gold):\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\"),\n    }\n\n@torch.no_grad()\ndef eval_student(loader):\n    student.eval()\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = student(input_ids=b[\"s_input_ids\"], attention_mask=b[\"s_attention_mask\"])\n        preds += out.logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return compute_metrics(np.array(preds), np.array(gold))\n\nbest_f1, wait = -1.0, 0\nfor ep in range(1, EPOCHS_STUDENT+1):\n    student.train(); run = 0.0\n    for b in tqdm(train_loader, desc=f\"[KD Epoch {ep}/{EPOCHS_STUDENT}]\"):\n        labels = b[\"labels\"].to(DEVICE)\n\n        # Student forward\n        s_out = student(input_ids=b[\"s_input_ids\"].to(DEVICE),\n                        attention_mask=b[\"s_attention_mask\"].to(DEVICE),\n                        output_hidden_states=False)\n\n        # Teacher forward (frozen)\n        with torch.no_grad():\n            t_out = teacher(input_ids=b[\"t_input_ids\"].to(DEVICE),\n                            attention_mask=b[\"t_attention_mask\"].to(DEVICE),\n                            output_hidden_states=False,\n                            return_dict=True)\n\n        loss = criterion(s_out, t_out, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n        opt_s.step(); sch_s.step(); opt_s.zero_grad()\n        run += loss.item()\n\n    val = eval_student(val_loader)\n    print(f\"[KD] loss={run/len(train_loader):.4f} | \"\n          f\"Val Acc={val['accuracy']:.4f} | F1m={val['f1_macro']:.4f} | F1w={val['f1_weighted']:.4f}\")\n\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save(student.state_dict(), WORK_DIR / \"student_best_logitsKD.pt\")\n        print(\"üíæ Saved best student.\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"‚è∏Ô∏è Early stopping.\")\n            break\n\n# Reload best\nstudent.load_state_dict(torch.load(WORK_DIR / \"student_best_logitsKD.pt\", map_location=DEVICE))\nstudent.eval()\nprint(\"‚úÖ KD training complete (logits-only).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:37:08.889819Z","iopub.execute_input":"2025-11-02T13:37:08.890070Z","iopub.status.idle":"2025-11-02T13:46:23.882705Z","shell.execute_reply.started":"2025-11-02T13:37:08.890049Z","shell.execute_reply":"2025-11-02T13:46:23.881867Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"[KD Epoch 1/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38bfd51f9fec4cd694d6a1e094c65460"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=1.2559 | Val Acc=0.8349 | F1m=0.7748 | F1w=0.8259\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 2/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01c182d67d83474993ad286354e2ce32"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.8311 | Val Acc=0.8645 | F1m=0.8159 | F1w=0.8575\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 3/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5a19ce2cc7624841a5f0f9e6a2a30013"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.6040 | Val Acc=0.8831 | F1m=0.8491 | F1w=0.8806\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 4/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2355bebddd5b499ba46e702633cb61f1"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.5084 | Val Acc=0.8967 | F1m=0.8700 | F1w=0.8959\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 5/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"927df325c1664d8b8649b3866f02abf5"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.4545 | Val Acc=0.8950 | F1m=0.8706 | F1w=0.8953\nüíæ Saved best student.\n‚úÖ KD training complete (logits-only).\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from scipy.special import softmax\nfrom scipy.spatial.distance import cosine\n\n@torch.no_grad()\ndef evaluate_model(model, loader, mode=\"student\"):\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        inp = {\"input_ids\": b[\"t_input_ids\"], \"attention_mask\": b[\"t_attention_mask\"]} if mode==\"teacher\" else \\\n              {\"input_ids\": b[\"s_input_ids\"], \"attention_mask\": b[\"s_attention_mask\"]}\n        out = model(**inp)\n        logits = out.logits if hasattr(out, \"logits\") else out[\"logits\"]\n        preds += logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\"),\n    }\n\nprint(\"üß™ Evaluating on test‚Ä¶\")\nteacher_test = evaluate_model(teacher, test_loader, mode=\"teacher\")\nstudent_test = evaluate_model(student, test_loader, mode=\"student\")\nprint(\"[Teacher][Test]:\", teacher_test)\nprint(\"[Student][Test]:\", student_test)\n\n@torch.no_grad()\ndef evaluate_alignment(teacher, student, loader):\n    cos_list, corr_list, agree_list = [], [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        t_out = teacher(b[\"t_input_ids\"], b[\"t_attention_mask\"])\n        s_out = student(b[\"s_input_ids\"], b[\"s_attention_mask\"])\n        t_logits = t_out.logits.detach().cpu().numpy()\n        s_logits = s_out.logits.detach().cpu().numpy()\n        t_probs  = softmax(t_logits, axis=-1)\n        s_probs  = softmax(s_logits, axis=-1)\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            cos_list.append(1 - cosine(tl, sl))\n            corr_list.append(np.corrcoef(tp, sp)[0, 1])\n            agree_list.append(np.argmax(tp) == np.argmax(sp))\n    return {\n        \"logit_cosine\": float(np.nanmean(cos_list)),\n        \"prob_corr\": float(np.nanmean(corr_list)),\n        \"pred_alignment\": float(np.mean(agree_list))\n    }\n\nalignment = evaluate_alignment(teacher, student, test_loader)\nprint(f\"\"\"\n===== üîó Alignment (Test) =====\nüîπ Logit Cosine Similarity : {alignment['logit_cosine']:.4f}\nüîπ Probability Correlation : {alignment['prob_corr']:.4f}\nüîπ Prediction Agreement    : {alignment['pred_alignment']:.4f}\n\"\"\")\n\n# Save artifacts\nSAVE_DIR = WORK_DIR / \"student_minilm_logitsKD\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\ntorch.save(student.state_dict(), SAVE_DIR / \"pytorch_model.bin\")\nstudent_tok.save_pretrained(SAVE_DIR)\n\n# serialize to pure Python types\ndef to_py(obj):\n    if isinstance(obj, dict): return {k: to_py(v) for k,v in obj.items()}\n    if hasattr(obj, \"item\"): return obj.item()\n    return obj\n\nwith open(WORK_DIR / \"metrics_minilm_logitsKD.json\", \"w\") as f:\n    json.dump({\"teacher_test\": to_py(teacher_test),\n               \"student_test\": to_py(student_test),\n               \"alignment\": to_py(alignment)}, f, indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Saved model to\", SAVE_DIR, \"and metrics JSON to\", WORK_DIR / \"metrics_minilm_logitsKD.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:46:23.883600Z","iopub.execute_input":"2025-11-02T13:46:23.883815Z","iopub.status.idle":"2025-11-02T13:46:44.722008Z","shell.execute_reply.started":"2025-11-02T13:46:23.883798Z","shell.execute_reply":"2025-11-02T13:46:44.721378Z"}},"outputs":[{"name":"stdout","text":"üß™ Evaluating on test‚Ä¶\n[Teacher][Test]: {'accuracy': 0.9542760372565622, 'f1_macro': 0.9438450972104819, 'f1_weighted': 0.9544809673164066}\n[Student][Test]: {'accuracy': 0.88653683319221, 'f1_macro': 0.8604018912529552, 'f1_weighted': 0.8869459907959556}\n\n===== üîó Alignment (Test) =====\nüîπ Logit Cosine Similarity : 0.8091\nüîπ Probability Correlation : 0.8103\nüîπ Prediction Agreement    : 0.9052\n\n‚úÖ Saved model to /kaggle/working/student_minilm_logitsKD and metrics JSON to /kaggle/working/metrics_minilm_logitsKD.json\n","output_type":"stream"}],"execution_count":8}]}