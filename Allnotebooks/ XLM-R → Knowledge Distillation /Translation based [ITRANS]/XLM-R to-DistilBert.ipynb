{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceType":"datasetVersion","sourceId":13581507,"datasetId":8628620}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir transformers==4.45.2 accelerate datasets sentencepiece indic-transliteration -q\n!pip install scikit-learn pandas tqdm matplotlib -q\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T22:31:03.823678Z","iopub.execute_input":"2025-11-01T22:31:03.824321Z","iopub.status.idle":"2025-11-01T22:32:27.621287Z","shell.execute_reply.started":"2025-11-01T22:31:03.824295Z","shell.execute_reply":"2025-11-01T22:32:27.620221Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m100.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m344.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m280.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m358.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m335.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m317.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m137.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m331.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m172.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m144.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m198.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m217.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m189.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m218.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m163.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m168.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m159.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m116.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports, Config","metadata":{}},{"cell_type":"code","source":"import os, json, torch, random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\nfrom transformers import (\n    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\n\n# Reproducibility\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Paths\nDATA_DIR = Path(\"/kaggle/input/dataaaaaa\")\nPOS_FILE = DATA_DIR / \"all_positive_8500.txt\"\nNEG_FILE = DATA_DIR / \"all_negative_3307.txt\"\n\n# Training settings\nBATCH_SIZE = 16\nMAX_LEN = 128\nEPOCHS_TEACHER = 3\nEPOCHS_STUDENT = 4\nLR_TEACHER = 2e-5\nLR_STUDENT = 3e-5\nWEIGHT_DECAY = 0.01\nWARMUP_RATIO = 0.1\nKD_T = 3.0\nKD_ALPHA = 0.5\nGAMMA_HIDDEN = 1.0\nGAMMA_ATT = 1.0\n\n# --------------------------------------------------------------\n# Load Bangla dataset\n# --------------------------------------------------------------\ndef read_txt(p):\n    with open(p, encoding=\"utf-8\") as f:\n        return [line.strip() for line in f if line.strip()]\n\npos_texts = read_txt(POS_FILE)\nneg_texts = read_txt(NEG_FILE)\n\ndf = pd.DataFrame({\n    \"text\": pos_texts + neg_texts,\n    \"label\": [1]*len(pos_texts) + [0]*len(neg_texts)\n})\ndf = df.sample(frac=1, random_state=SEED).reset_index(drop=True)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\nval_df, test_df   = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)\n\nprint(f\"âœ… Dataset loaded: train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T22:36:02.620877Z","iopub.execute_input":"2025-11-01T22:36:02.621722Z","iopub.status.idle":"2025-11-01T22:36:07.385886Z","shell.execute_reply.started":"2025-11-01T22:36:02.621687Z","shell.execute_reply":"2025-11-01T22:36:07.385242Z"}},"outputs":[{"name":"stdout","text":"âœ… Dataset loaded: train=9445, val=1181, test=1181\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Train teacher (BanglaBERT) and save best","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# ğŸ§  Fine-tune Teacher Model (XLM-R)\n# ==============================================================\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\nimport torch.nn as nn\n\nMODEL_ID = \"xlm-roberta-base\"\nNUM_LABELS = 2\nMAX_LEN = 128\nBATCH_SIZE = 16\nEPOCHS = 3\nLR = 2e-5\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\n\nteacher_tok = AutoTokenizer.from_pretrained(MODEL_ID)\nteacher = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=NUM_LABELS).to(DEVICE)\n\nclass TextDataset(Dataset):\n    def __init__(self, df, tok, max_len):\n        self.texts = df.text.tolist()\n        self.labels = df.label.tolist()\n        self.tok = tok\n        self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\"input_ids\": enc[\"input_ids\"].squeeze(), \"attention_mask\": enc[\"attention_mask\"].squeeze(),\n                \"labels\": torch.tensor(self.labels[i], dtype=torch.long)}\n\ntrain_loader = DataLoader(TextDataset(train_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(TextDataset(val_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(TextDataset(test_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\noptimizer = AdamW(teacher.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nsteps = len(train_loader)*EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, int(WARMUP_RATIO*steps), steps)\ncriterion = nn.CrossEntropyLoss()\n\nSAVE_DIR = \"/kaggle/working/fine_tuned_xlmr\"\nos.makedirs(SAVE_DIR, exist_ok=True)\n\nbest_val_f1 = 0.0\nfor ep in range(1, EPOCHS+1):\n    teacher.train(); total_loss = 0\n    for b in tqdm(train_loader, desc=f\"Epoch {ep}/{EPOCHS}\"):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward(); optimizer.step(); scheduler.step(); optimizer.zero_grad()\n        total_loss += loss.item()\n    print(f\"Train Loss: {total_loss/len(train_loader):.4f}\")\n\n    teacher.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in val_loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = teacher(**b)\n            preds += out.logits.argmax(-1).cpu().tolist()\n            gold  += b[\"labels\"].cpu().tolist()\n    acc = accuracy_score(gold, preds); f1 = f1_score(gold, preds, average=\"macro\")\n    print(f\"Val Acc={acc:.4f} | F1={f1:.4f}\")\n    if f1>best_val_f1:\n        best_val_f1=f1\n        teacher.save_pretrained(SAVE_DIR)\n        teacher_tok.save_pretrained(SAVE_DIR)\n        print(f\"âœ… Saved best model to {SAVE_DIR}\")\n\nprint(f\"Best Val F1={best_val_f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T22:36:43.169052Z","iopub.execute_input":"2025-11-01T22:36:43.169488Z","iopub.status.idle":"2025-11-01T22:49:28.983147Z","shell.execute_reply.started":"2025-11-01T22:36:43.169465Z","shell.execute_reply":"2025-11-01T22:49:28.982159Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7af3eaffda24f13b1f5b918300631e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b92b1d6a6d646cf9e5875582b2d364a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59593aa4bcc94fa381abeeee854c5f54"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e96a2868d574cc9a6591cd9a2cd4be8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32e68d84d14545d884551d1d66b378b1"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 1/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32d92b5f20f042d8a056cc477e71e928"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.3318\nVal Acc=0.9526 | F1=0.9416\nâœ… Saved best model to /kaggle/working/fine_tuned_xlmr\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1503d2490c464b838097d098c0004d54"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.1241\nVal Acc=0.9602 | F1=0.9506\nâœ… Saved best model to /kaggle/working/fine_tuned_xlmr\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"471cfa25988e4802803c8485d0666c1b"}},"metadata":{}},{"name":"stdout","text":"Train Loss: 0.0795\nVal Acc=0.9636 | F1=0.9548\nâœ… Saved best model to /kaggle/working/fine_tuned_xlmr\nBest Val F1=0.9548\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Transliteration KD Dataset","metadata":{}},{"cell_type":"code","source":"from indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\ndef transliterate_bn_text(text: str) -> str:\n    try:\n        return transliterate(text, sanscript.BENGALI, sanscript.ITRANS)\n    except Exception:\n        return text\n\nSTUDENT_MODEL_ID = \"distilbert-base-uncased\"\nstudent_tok = AutoTokenizer.from_pretrained(STUDENT_MODEL_ID)\n\nclass KDDataset(Dataset):\n    def __init__(self, df, teacher_tok, student_tok, max_len):\n        self.texts = df.text.tolist()\n        self.labels = df.label.tolist()\n        self.teacher_tok = teacher_tok\n        self.student_tok = student_tok\n        self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        text_bn = self.texts[i]; text_en = transliterate_bn_text(text_bn)\n        t_enc = self.teacher_tok(text_bn, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        s_enc = self.student_tok(text_en, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\"t_input_ids\": t_enc[\"input_ids\"].squeeze(0), \"t_attention_mask\": t_enc[\"attention_mask\"].squeeze(0),\n                \"s_input_ids\": s_enc[\"input_ids\"].squeeze(0), \"s_attention_mask\": s_enc[\"attention_mask\"].squeeze(0),\n                \"labels\": torch.tensor(self.labels[i], dtype=torch.long)}\n\ndef pad_collate(batch, t_pad_id, s_pad_id):\n    out = {}\n    for k in batch[0].keys():\n        if k==\"labels\":\n            out[k]=torch.stack([b[k] for b in batch])\n        elif k.startswith(\"t_\"):\n            pad_val=0 if \"attention\" in k else t_pad_id\n            out[k]=nn.utils.rnn.pad_sequence([b[k] for b in batch],batch_first=True,padding_value=pad_val)\n        elif k.startswith(\"s_\"):\n            pad_val=0 if \"attention\" in k else s_pad_id\n            out[k]=nn.utils.rnn.pad_sequence([b[k] for b in batch],batch_first=True,padding_value=pad_val)\n    return out\n\ntrain_loader = DataLoader(KDDataset(train_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nval_loader   = DataLoader(KDDataset(val_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\ntest_loader  = DataLoader(KDDataset(test_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\n\nprint(\"âœ… KD dataloaders ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T22:49:32.432088Z","iopub.execute_input":"2025-11-01T22:49:32.432840Z","iopub.status.idle":"2025-11-01T22:49:33.542140Z","shell.execute_reply.started":"2025-11-01T22:49:32.432814Z","shell.execute_reply":"2025-11-01T22:49:33.541481Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0eff3bf74f240548b9fda38a23cecdb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad3a0ca0ec6f43a0a534e239c0a903be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf77146ebf634d67a2acacbcedb6124c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39cd43659c3044939a380e0cf8815221"}},"metadata":{}},{"name":"stdout","text":"âœ… KD dataloaders ready.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Student model + Logit + Hidden + Attention KD loss","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# ğŸ“ Knowledge Distillation Training (Logits + Hidden + Attention)\n# ==============================================================\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nfrom pathlib import Path\n\n# ==============================================================\n# ğŸ“ Define Working Directory\n# ==============================================================\nWORK_DIR = Path(\"/kaggle/working\")\nWORK_DIR.mkdir(parents=True, exist_ok=True)\nprint(f\"ğŸ“‚ Working directory set to: {WORK_DIR}\")\n\n# ==============================================================\n# ğŸ§  KD Loss Definition\n# ==============================================================\nclass KDLossFull(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5, gamma_h=1.0, gamma_a=1.0):\n        super().__init__()\n        self.T = T\n        self.alpha = alpha\n        self.gamma_h = gamma_h\n        self.gamma_a = gamma_a\n        self.ce = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n        self.mse = nn.MSELoss()\n\n    @staticmethod\n    def map_layers(n_s, n_t):\n        \"\"\"Map student layers to teacher layers proportionally.\"\"\"\n        s_idx = list(range(1, n_s + 1))\n        t_idx = torch.linspace(1, n_t, steps=n_s).round().long().tolist()\n        return list(zip(s_idx, t_idx))\n\n    def forward(self, s_pack, t_pack, labels):\n        # ---------- Logits KD ----------\n        logits_s, logits_t = s_pack[\"logits\"], t_pack[\"logits\"]\n        hard = self.ce(logits_s, labels)\n        soft = self.kld(F.log_softmax(logits_s / self.T, dim=-1),\n                        F.softmax(logits_t / self.T, dim=-1)) * (self.T ** 2)\n        total = (1 - self.alpha) * hard + self.alpha * soft\n\n        # ---------- Hidden State KD ----------\n        h_s, h_t = s_pack.get(\"hidden_states\", []), t_pack.get(\"hidden_states\", [])\n        if h_s and h_t:\n            pairs = self.map_layers(len(h_s) - 1, len(h_t) - 1)\n            h_losses = []\n            for i_s, i_t in pairs:\n                hs, ht = h_s[i_s], h_t[i_t]\n                seq = min(hs.size(1), ht.size(1))\n                h_losses.append(self.mse(hs[:, :seq, :], ht[:, :seq, :]))\n            if h_losses:\n                total += self.gamma_h * torch.stack(h_losses).mean()\n\n        # ---------- Attention Map KD (Safe) ----------\n        a_s, a_t = s_pack.get(\"attentions\", []), t_pack.get(\"attentions\", [])\n        if a_s and a_t:\n            pairs = self.map_layers(len(a_s), len(a_t))\n            a_losses = []\n            for i_s, i_t in pairs:\n                try:\n                    as_ = a_s[i_s]\n                    at_ = a_t[i_t]\n                    if as_ is None or at_ is None or as_.dim() != 4 or at_.dim() != 4:\n                        continue\n                    seq = min(as_.size(-1), at_.size(-1))\n                    a_losses.append(self.mse(as_[:, :, :seq, :seq], at_[:, :, :seq, :seq]))\n                except Exception:\n                    continue\n            if a_losses:\n                total += self.gamma_a * torch.stack(a_losses).mean()\n\n        return total\n\n\n# ==============================================================\n# âš™ï¸ Setup Optimizer / Scheduler\n# ==============================================================\nLR_STUDENT = 3e-5\nEPOCHS_STUDENT = 5\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\nPATIENCE = 2\n\ncriterion = KDLossFull(T=3.0, alpha=0.5, gamma_h=1.0, gamma_a=1.0)\n\n# Freeze teacher\nteacher.eval()\nfor p in teacher.parameters():\n    p.requires_grad = False\n\nopt_s = AdamW(student.parameters(), lr=LR_STUDENT, weight_decay=WEIGHT_DECAY)\nnum_steps_s = EPOCHS_STUDENT * len(train_loader)\nwarm_steps_s = int(WARMUP_RATIO * num_steps_s)\nsch_s = get_linear_schedule_with_warmup(opt_s, warm_steps_s, num_steps_s)\n\nbest_f1, wait = -1, 0\n\n\n# ==============================================================\n# ğŸ“ Helper: Compute Metrics\n# ==============================================================\ndef compute_metrics(preds, gold):\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\")\n    }\n\n@torch.no_grad()\ndef eval_student(loader):\n    student.eval()\n    preds, gold = [], []\n    for b in loader:\n        b = {k: v.to(DEVICE) for k, v in b.items()}\n        out = student(input_ids=b[\"s_input_ids\"], attention_mask=b[\"s_attention_mask\"])\n        preds += out[\"logits\"].argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return compute_metrics(np.array(preds), np.array(gold))\n\n\n# ==============================================================\n# ğŸš€ KD Training Loop\n# ==============================================================\nfor ep in range(1, EPOCHS_STUDENT + 1):\n    student.train()\n    run_loss = 0.0\n\n    for b in tqdm(train_loader, desc=f\"[KD Epoch {ep}/{EPOCHS_STUDENT}]\", leave=False):\n        labels = b[\"labels\"].to(DEVICE)\n\n        # ---- Forward Student ----\n        s_out = student(\n            input_ids=b[\"s_input_ids\"].to(DEVICE),\n            attention_mask=b[\"s_attention_mask\"].to(DEVICE),\n            output_hidden_states=True,\n            output_attentions=True\n        )\n\n        # ---- Forward Teacher ----\n        with torch.no_grad():\n            t_raw = teacher(\n                input_ids=b[\"t_input_ids\"].to(DEVICE),\n                attention_mask=b[\"t_attention_mask\"].to(DEVICE),\n                output_hidden_states=True,\n                output_attentions=True,\n                return_dict=True\n            )\n            t_out = {\n                \"logits\": t_raw.logits,\n                \"hidden_states\": t_raw.hidden_states,\n                \"attentions\": t_raw.attentions\n            }\n\n        # ---- Compute KD Loss ----\n        loss = criterion(s_out, t_out, labels)\n        loss.backward()\n\n        nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n        opt_s.step(); sch_s.step(); opt_s.zero_grad()\n        run_loss += loss.item()\n\n    # ---- Validation ----\n    val = eval_student(val_loader)\n    print(f\"[KD Epoch {ep}] loss={run_loss/len(train_loader):.4f} | \"\n          f\"Val F1m={val['f1_macro']:.4f} Acc={val['accuracy']:.4f}\")\n\n    # ---- Early Stopping ----\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save(student.state_dict(), WORK_DIR / \"student_best_kd.pt\")\n        print(\"âœ… Saved best student checkpoint.\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"â¸ï¸ Early stopping triggered.\")\n            break\n\n# ==============================================================\n# ğŸ§© Reload Best Model\n# ==============================================================\nstudent.load_state_dict(torch.load(WORK_DIR / \"student_best_kd.pt\", map_location=DEVICE))\nstudent.eval()\nprint(\"âœ… Student KD training complete (logits + hidden + attention).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:04:19.001605Z","iopub.execute_input":"2025-11-01T23:04:19.001922Z","iopub.status.idle":"2025-11-01T23:20:33.526564Z","shell.execute_reply.started":"2025-11-01T23:04:19.001896Z","shell.execute_reply":"2025-11-01T23:20:33.525918Z"}},"outputs":[{"name":"stdout","text":"ğŸ“‚ Working directory set to: /kaggle/working\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 1/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 1] loss=0.8378 | Val F1m=0.9248 Acc=0.9382\nâœ… Saved best student checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 2/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 2] loss=0.5507 | Val F1m=0.9267 Acc=0.9416\nâœ… Saved best student checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 3/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 3] loss=0.4178 | Val F1m=0.9300 Acc=0.9450\nâœ… Saved best student checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 4/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 4] loss=0.3469 | Val F1m=0.9352 Acc=0.9475\nâœ… Saved best student checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 5/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 5] loss=0.3095 | Val F1m=0.9392 Acc=0.9509\nâœ… Saved best student checkpoint.\nâœ… Student KD training complete (logits + hidden + attention).\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# KD training loop enabling attentions","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# ğŸ“Š Evaluate Teacher vs Student (Performance + Alignment) + Save\n# ==============================================================\n\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom scipy.special import softmax\nfrom scipy.spatial.distance import cosine\nimport numpy as np\nimport json\nfrom pathlib import Path\nimport torch\n\nteacher.eval(); student.eval()\n\n# ---------- helpers ----------\n@torch.no_grad()\ndef evaluate_model(model, loader, mode=\"student\"):\n    preds, gold, probs = [], [], []\n    for b in loader:\n        b = {k: v.to(DEVICE) for k, v in b.items()}\n        if mode == \"teacher\":\n            inp = {\"input_ids\": b[\"t_input_ids\"], \"attention_mask\": b[\"t_attention_mask\"]}\n        else:\n            inp = {\"input_ids\": b[\"s_input_ids\"], \"attention_mask\": b[\"s_attention_mask\"]}\n        out = model(**inp)\n        logits = out[\"logits\"] if isinstance(out, dict) else out.logits\n        p = logits.softmax(-1)\n        preds += p.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n        probs += p[:, 1].cpu().tolist()\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\")\n    }, np.array(probs), np.array(preds), np.array(gold)\n\n@torch.no_grad()\ndef evaluate_alignment(teacher, student, loader):\n    logits_cos, prob_corr = [], []\n    t_preds_all, s_preds_all = [], []\n    for b in loader:\n        b = {k: v.to(DEVICE) for k, v in b.items()}\n        t = teacher(input_ids=b[\"t_input_ids\"], attention_mask=b[\"t_attention_mask\"])\n        s = student(input_ids=b[\"s_input_ids\"], attention_mask=b[\"s_attention_mask\"])\n        t_logits = t.logits.detach().cpu().numpy()\n        s_logits = s[\"logits\"].detach().cpu().numpy()\n        t_probs = softmax(t_logits, axis=-1)\n        s_probs = softmax(s_logits, axis=-1)\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            logits_cos.append(1 - cosine(tl, sl))\n            prob_corr.append(np.corrcoef(tp, sp)[0, 1])\n            t_preds_all.append(np.argmax(tp))\n            s_preds_all.append(np.argmax(sp))\n    return {\n        \"logit_cosine\": float(np.nanmean(logits_cos)),\n        \"prob_corr\": float(np.nanmean(prob_corr)),\n        \"pred_alignment\": float((np.array(t_preds_all) == np.array(s_preds_all)).mean())\n    }\n\n# ---------- evaluate ----------\nprint(\"Evaluating Teacher on Test set ...\")\nteacher_test, t_probs, t_preds, gold = evaluate_model(teacher, test_loader, mode=\"teacher\")\nprint(\"Evaluating Student on Test set ...\")\nstudent_test, s_probs, s_preds, _   = evaluate_model(student, test_loader, mode=\"student\")\n\nprint(\"===== ğŸ“ˆ Test Metrics =====\")\nprint(f\"ğŸ§  Teacher (XLM-R):      Acc={teacher_test['accuracy']:.4f} | \"\n      f\"F1_macro={teacher_test['f1_macro']:.4f} | F1_weighted={teacher_test['f1_weighted']:.4f}\")\nprint(f\"ğŸ“ Student (DistilBERT): Acc={student_test['accuracy']:.4f} | \"\n      f\"F1_macro={student_test['f1_macro']:.4f} | F1_weighted={student_test['f1_weighted']:.4f}\")\n\nalignment = evaluate_alignment(teacher, student, test_loader)\nprint(\"\\n===== ğŸ”— Alignment Metrics =====\")\nprint(f\"ğŸ”¹ Logit Cosine Similarity : {alignment['logit_cosine']:.4f}\")\nprint(f\"ğŸ”¹ Probability Correlation : {alignment['prob_corr']:.4f}\")\nprint(f\"ğŸ”¹ Prediction Agreement    : {alignment['pred_alignment']:.4f}\")\n\n# ---------- save artifacts ----------\nSAVE_DIR = WORK_DIR / \"student_model_translit_attKD\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\n\ntorch.save(student.state_dict(), SAVE_DIR / \"pytorch_model.bin\")\nstudent_tok.save_pretrained(SAVE_DIR)\n\nwith open(SAVE_DIR / \"student_config.json\", \"w\") as f:\n    json.dump({\n        \"teacher_model\": \"xlm-roberta-base\",\n        \"student_model\": \"distilbert-base-uncased\",\n        \"num_labels\": 2,\n        \"kd_temperature\": 3.0,\n        \"alpha\": 0.5,\n        \"gamma_hidden\": 1.0,\n        \"gamma_attention\": 1.0\n    }, f, indent=2, ensure_ascii=False)\n\ndef _to_py(o):\n    # ensure json-serializable (avoid numpy dtypes)\n    if isinstance(o, dict): return {k: _to_py(v) for k, v in o.items()}\n    if isinstance(o, (np.floating, np.float32, np.float64)): return float(o)\n    return o\n\nwith open(WORK_DIR / \"metrics_summary_translit_attKD.json\", \"w\") as f:\n    json.dump({\n        \"teacher_test\": _to_py(teacher_test),\n        \"student_test\": _to_py(student_test),\n        \"alignment\": _to_py(alignment)\n    }, f, indent=2, ensure_ascii=False)\n\nprint(f\"\\nâœ… Saved student model + metrics â†’ {WORK_DIR}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:20:34.448473Z","iopub.execute_input":"2025-11-01T23:20:34.449126Z","iopub.status.idle":"2025-11-01T23:21:01.258538Z","shell.execute_reply.started":"2025-11-01T23:20:34.449103Z","shell.execute_reply":"2025-11-01T23:21:01.257836Z"}},"outputs":[{"name":"stdout","text":"Evaluating Teacher on Test set ...\nEvaluating Student on Test set ...\n===== ğŸ“ˆ Test Metrics =====\nğŸ§  Teacher (XLM-R):      Acc=0.9577 | F1_macro=0.9478 | F1_weighted=0.9578\nğŸ“ Student (DistilBERT): Acc=0.9483 | F1_macro=0.9358 | F1_weighted=0.9483\n\n===== ğŸ”— Alignment Metrics =====\nğŸ”¹ Logit Cosine Similarity : 0.9094\nğŸ”¹ Probability Correlation : 0.9136\nğŸ”¹ Prediction Agreement    : 0.9568\n\nâœ… Saved student model + metrics â†’ /kaggle/working\n","output_type":"stream"}],"execution_count":9}]}