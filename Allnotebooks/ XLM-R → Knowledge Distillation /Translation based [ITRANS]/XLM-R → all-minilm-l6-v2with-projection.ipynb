{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceId":13581507,"sourceType":"datasetVersion","datasetId":8628620},{"sourceId":13581607,"sourceType":"datasetVersion","datasetId":8628682}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir transformers==4.45.2 indic-transliteration -q\n!pip install --no-cache-dir scikit-learn pandas tqdm matplotlib sentencepiece -q\n\nimport os, json, random, numpy as np, torch\nfrom pathlib import Path\n\n# ---- Repro / device / dirs\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nWORK_DIR = Path(\"/kaggle/working\"); WORK_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---- Config (from your table)\nTEACHER_MODEL_ID = \"xlm-roberta-base\"\nSTUDENT_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\nMAX_LEN = 128\nBATCH_SIZE = 16\n\n# Teacher FT\nEPOCHS_TEACHER = 3\nLR_TEACHER = 2e-5\nWARMUP_RATIO_T = 0.1\nWEIGHT_DECAY_T = 0.01\n\n# KD\nEPOCHS_STUDENT = 5   # 4‚Äì5 works well\nLR_STUDENT = 3e-5\nWARMUP_RATIO_S = 0.1\nWEIGHT_DECAY_S = 0.01\nPATIENCE = 2\n\nKD_T = 3.0\nKD_ALPHA = 0.5\nGAMMA_HIDDEN = 1.0\n\nprint(\"‚úÖ Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T10:55:50.402756Z","iopub.execute_input":"2025-11-02T10:55:50.403409Z","iopub.status.idle":"2025-11-02T10:56:12.911702Z","shell.execute_reply.started":"2025-11-02T10:55:50.403380Z","shell.execute_reply":"2025-11-02T10:56:12.910813Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m119.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m332.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m312.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m291.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nDATA_DIR = Path(\"/kaggle/input/dataaaaaaa\")\nPOS_FILE = DATA_DIR / \"all_positive_8500.txt\"\nNEG_FILE = DATA_DIR / \"all_negative_3307.txt\"\nassert POS_FILE.exists() and NEG_FILE.exists(), f\"Missing files:\\n{POS_FILE}\\n{NEG_FILE}\"\n\ndef read_txt(p: Path):\n    with open(p, encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos = read_txt(POS_FILE)\nneg = read_txt(NEG_FILE)\n\ndf = pd.DataFrame({\"text\": pos + neg,\n                   \"label\": [1]*len(pos) + [0]*len(neg)}).sample(frac=1, random_state=SEED).reset_index(drop=True)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\nval_df,   test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)\n\nprint(f\"Train={len(train_df)}  Val={len(val_df)}  Test={len(test_df)}\")\ndf.head()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T10:58:46.777008Z","iopub.execute_input":"2025-11-02T10:58:46.777910Z","iopub.status.idle":"2025-11-02T10:58:47.814145Z","shell.execute_reply.started":"2025-11-02T10:58:46.777885Z","shell.execute_reply":"2025-11-02T10:58:47.813302Z"}},"outputs":[{"name":"stdout","text":"Train=9445  Val=1181  Test=1181\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"                                                text  label\n0  ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶®‡¶æ‡¶ü‡¶ï‡ßá‡¶∞ ‡¶≠‡ßÅ‡¶§‡ßá ‡¶ß‡¶∞‡ßá‡¶õ‡ßá, ‡¶ì‡¶ù‡¶æ ‡¶ï‡ßã‡¶•‡¶æ‡ßü ‡¶™...      1\n1  ‡¶≠‡¶æ‡¶á, ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶π‡¶æ‡¶∞‡ßç‡¶° ‡¶´‡ßç‡¶Ø‡¶æ‡¶® ‡¶Ü‡¶Æ‡¶ø!‡¶¨‡¶æ‡¶ü ‡¶è‡¶á‡¶Ø‡ßá ‡¶è‡¶á ‡¶®‡¶æ‡¶ü‡¶ï‡ßá ‡¶Ø...      0\n2  ‡¶Ö‡¶®‡ßá‡¶ï ‡¶≠‡¶æ‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶æ‡¶ü‡¶ï ‡•§‡•§‡¶Ö‡¶®‡ßá‡¶ï ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø ‡¶≠‡¶æ‡¶á ‡¶§‡ßÅ‡¶Æ‡¶æ‡¶ï‡ßá...      1\n3                           ‡¶¨‡¶æ‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶ü‡¶ï ‡¶¨‡¶æ‡¶≤‡ßá‡¶∞ ‡¶ï‡¶®‡¶∏‡ßá‡¶™‡ßç‡¶ü      0\n4             ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶® ‡¶π‡ßü‡ßá‡¶õ‡ßá, ‡¶è‡¶á ‡¶∞‡¶ï‡¶Æ ‡¶®‡¶æ‡¶ü‡¶ï ‡¶Ü‡¶∞‡ßã ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ö‡¶æ‡¶á      1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂‡ßá‡¶∞ ‡¶®‡¶æ‡¶ü‡¶ï‡ßá‡¶∞ ‡¶≠‡ßÅ‡¶§‡ßá ‡¶ß‡¶∞‡ßá‡¶õ‡ßá, ‡¶ì‡¶ù‡¶æ ‡¶ï‡ßã‡¶•‡¶æ‡ßü ‡¶™...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>‡¶≠‡¶æ‡¶á, ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶π‡¶æ‡¶∞‡ßç‡¶° ‡¶´‡ßç‡¶Ø‡¶æ‡¶® ‡¶Ü‡¶Æ‡¶ø!‡¶¨‡¶æ‡¶ü ‡¶è‡¶á‡¶Ø‡ßá ‡¶è‡¶á ‡¶®‡¶æ‡¶ü‡¶ï‡ßá ‡¶Ø...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>‡¶Ö‡¶®‡ßá‡¶ï ‡¶≠‡¶æ‡¶≤‡ßã ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶æ‡¶ü‡¶ï ‡•§‡•§‡¶Ö‡¶®‡ßá‡¶ï ‡¶≠‡¶æ‡¶≤‡ßã‡¶¨‡¶æ‡¶∏‡¶ø ‡¶≠‡¶æ‡¶á ‡¶§‡ßÅ‡¶Æ‡¶æ‡¶ï‡ßá...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>‡¶¨‡¶æ‡¶≤‡ßá‡¶∞ ‡¶®‡¶æ‡¶ü‡¶ï ‡¶¨‡¶æ‡¶≤‡ßá‡¶∞ ‡¶ï‡¶®‡¶∏‡ßá‡¶™‡ßç‡¶ü</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>‡¶¶‡¶æ‡¶∞‡ßÅ‡¶® ‡¶π‡ßü‡ßá‡¶õ‡ßá, ‡¶è‡¶á ‡¶∞‡¶ï‡¶Æ ‡¶®‡¶æ‡¶ü‡¶ï ‡¶Ü‡¶∞‡ßã ‡¶¶‡ßá‡¶ñ‡¶§‡ßá ‡¶ö‡¶æ‡¶á</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# If you already uploaded a fine-tuned teacher, enable this:\nUSE_PRETRAINED_TEACHER = False\nPRETRAINED_TEACHER_DIR = Path(\"/kaggle/input/finetuned-xlmr\")  # change if needed\n\nclass TxtClsDataset(Dataset):\n    def __init__(self, df, tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.tok, self.max_len = tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\",\n                       max_length=self.max_len, return_tensors=\"pt\")\n        return {\"input_ids\": enc[\"input_ids\"].squeeze(0),\n                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n                \"labels\": torch.tensor(self.labels[i], dtype=torch.long)}\n\nif USE_PRETRAINED_TEACHER and PRETRAINED_TEACHER_DIR.exists():\n    print(\"üì• Loading pre-fine-tuned teacher from:\", PRETRAINED_TEACHER_DIR)\n    teacher_tok = AutoTokenizer.from_pretrained(PRETRAINED_TEACHER_DIR)\n    teacher     = AutoModelForSequenceClassification.from_pretrained(PRETRAINED_TEACHER_DIR).to(DEVICE)\nelse:\n    print(\"üõ†Ô∏è Fine-tuning teacher (XLM-R)‚Ä¶\")\n    teacher_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\n    teacher     = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=2).to(DEVICE)\n\n    tr_loader = DataLoader(TxtClsDataset(train_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\n    va_loader = DataLoader(TxtClsDataset(val_df,   teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n    te_loader = DataLoader(TxtClsDataset(test_df,  teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\n    opt = AdamW(teacher.parameters(), lr=LR_TEACHER, weight_decay=WEIGHT_DECAY_T)\n    steps = len(tr_loader)*EPOCHS_TEACHER\n    sch   = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO_T*steps), steps)\n\n    best_f1 = -1.0\n    for ep in range(1, EPOCHS_TEACHER+1):\n        teacher.train(); run=0.0\n        for b in tqdm(tr_loader, desc=f\"Teacher Epoch {ep}/{EPOCHS_TEACHER}\"):\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = teacher(**b)\n            loss = out.loss\n            loss.backward()\n            opt.step(); sch.step(); opt.zero_grad()\n            run += loss.item()\n\n        # validate\n        teacher.eval(); preds, gold = [], []\n        with torch.no_grad():\n            for b in va_loader:\n                b = {k:v.to(DEVICE) for k,v in b.items()}\n                out = teacher(**b)\n                preds += out.logits.argmax(-1).cpu().tolist()\n                gold  += b[\"labels\"].cpu().tolist()\n        acc = accuracy_score(gold, preds)\n        f1m = f1_score(gold, preds, average=\"macro\")\n        print(f\"Val: Acc={acc:.4f} | F1_macro={f1m:.4f}\")\n        if f1m > best_f1:\n            best_f1 = f1m\n            save_dir = WORK_DIR / \"fine_tuned_xlmr\"\n            save_dir.mkdir(parents=True, exist_ok=True)\n            teacher.save_pretrained(save_dir); teacher_tok.save_pretrained(save_dir)\n            print(\"üíæ Saved best teacher to\", save_dir)\n\n    # quick test\n    teacher.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in te_loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = teacher(**b)\n            preds += out.logits.argmax(-1).cpu().tolist()\n            gold  += b[\"labels\"].cpu().tolist()\n    print(\"Teacher[Test]: Acc=\", accuracy_score(gold,preds), \"F1_macro=\", f1_score(gold,preds,average=\"macro\"))\n\nprint(\"‚úÖ Teacher ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T10:58:55.285310Z","iopub.execute_input":"2025-11-02T10:58:55.286160Z","iopub.status.idle":"2025-11-02T11:12:06.055164Z","shell.execute_reply.started":"2025-11-02T10:58:55.286134Z","shell.execute_reply":"2025-11-02T11:12:06.054521Z"}},"outputs":[{"name":"stdout","text":"üõ†Ô∏è Fine-tuning teacher (XLM-R)‚Ä¶\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3e146aba5b7424d9e13cde4aa5280f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0f7419724d6849dd90861c7700d805ff"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17b255f45f8d42608ca90c0bb88f0843"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d89d03c6eea844a09d5cdb5d1ee7853e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76e2e0b4083942e58aaa215e21e5aefd"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 1/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f741cf5d919453cbafacaa581adbff0"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9526 | F1_macro=0.9416\nüíæ Saved best teacher to /kaggle/working/fine_tuned_xlmr\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 2/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39e80170fdfb426aa9248e909abc91b4"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9602 | F1_macro=0.9506\nüíæ Saved best teacher to /kaggle/working/fine_tuned_xlmr\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 3/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a2f1730dd25b4d79b160ea308611e524"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9636 | F1_macro=0.9548\nüíæ Saved best teacher to /kaggle/working/fine_tuned_xlmr\nTeacher[Test]: Acc= 0.9576629974597799 F1_macro= 0.9478168809959774\n‚úÖ Teacher ready.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# KD Data (Transliteration, 2 tokenizers)","metadata":{}},{"cell_type":"code","source":"from indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\ndef transliterate_bn_text(txt: str) -> str:\n    try:\n        return transliterate(txt, sanscript.BENGALI, sanscript.ITRANS)\n    except Exception:\n        return txt\n\n# Student tokenizer (MiniLM-L6-v2)\nfrom transformers import AutoTokenizer\nstudent_tok = AutoTokenizer.from_pretrained(STUDENT_MODEL_ID)\n\nclass KDDataset(Dataset):\n    def __init__(self, df, t_tok, s_tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.ttok, self.stok, self.max_len = t_tok, s_tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        txt_bn = self.texts[i]\n        txt_en = transliterate_bn_text(txt_bn)\n        t = self.ttok(txt_bn, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        s = self.stok(txt_en, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"t_input_ids\": t[\"input_ids\"].squeeze(0),\n            \"t_attention_mask\": t[\"attention_mask\"].squeeze(0),\n            \"s_input_ids\": s[\"input_ids\"].squeeze(0),\n            \"s_attention_mask\": s[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\ndef pad_collate(batch, t_pad, s_pad):\n    out = {}\n    for k in batch[0]:\n        if k == \"labels\":\n            out[k] = torch.stack([b[k] for b in batch])\n        elif k.startswith(\"t_\"):\n            padv = 0 if \"attention\" in k else t_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n        elif k.startswith(\"s_\"):\n            padv = 0 if \"attention\" in k else s_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n    return out\n\ntrain_loader = DataLoader(KDDataset(train_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nval_loader   = DataLoader(KDDataset(val_df,   teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\ntest_loader  = DataLoader(KDDataset(test_df,  teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\n\nprint(\"‚úÖ KD dataloaders ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:12:08.845486Z","iopub.execute_input":"2025-11-02T11:12:08.846262Z","iopub.status.idle":"2025-11-02T11:12:09.599069Z","shell.execute_reply.started":"2025-11-02T11:12:08.846235Z","shell.execute_reply":"2025-11-02T11:12:09.598114Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b3d8fbbd56fd465f839f4851cb308b46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5312608fea4247d6a266c51b5b18eee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d4cda4076748451d89a0ac1ca5473793"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"859fe4ebb401400f926e0f583cc77deb"}},"metadata":{}},{"name":"stdout","text":"‚úÖ KD dataloaders ready.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Student (MiniLM-L6-v2) head (logits + hidden)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel\n\nclass StudentClassifier(nn.Module):\n    def __init__(self, base_model_id, num_labels=2, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_id)\n        s_H = self.encoder.config.hidden_size            # MiniLM hidden size (often 384)\n        self.s_hidden = s_H\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(s_H, num_labels)\n    def forward(self, input_ids=None, attention_mask=None, **_):\n        out = self.encoder(input_ids=input_ids,\n                           attention_mask=attention_mask,\n                           output_hidden_states=True,\n                           return_dict=True)\n        cls = out.last_hidden_state[:, 0, :]\n        logits = self.fc(self.dropout(cls))\n        return {\"logits\": logits, \"hidden_states\": out.hidden_states}\n\nstudent = StudentClassifier(STUDENT_MODEL_ID).to(DEVICE)\nprint(\"‚úÖ Student initialized. Hidden size =\", student.s_hidden)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:12:14.258329Z","iopub.execute_input":"2025-11-02T11:12:14.258638Z","iopub.status.idle":"2025-11-02T11:12:15.437494Z","shell.execute_reply.started":"2025-11-02T11:12:14.258614Z","shell.execute_reply":"2025-11-02T11:12:15.436656Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"658d0f241b1a47639023c5c61210bfb3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e3616c4eb64449b2b420faa8b18a5b6a"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Student initialized. Hidden size = 384\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# KD Projection + Loss (CE + KL + HiddenProj)","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# discover sizes\nt_hidden = teacher.config.hidden_size    # XLM-R = 768\ns_hidden = student.s_hidden              # MiniLM-L6-v2 = 384\n\nclass KDProjectionHead(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.bridge = nn.Sequential(\n            nn.Linear(in_dim, out_dim),\n            nn.GELU(),\n            nn.LayerNorm(out_dim)\n        )\n    def forward(self, x):\n        return self.bridge(x)\n\nproj_head = KDProjectionHead(s_hidden, t_hidden).to(DEVICE)\n\nclass KDLossProj(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5, gamma_h=1.0):\n        super().__init__()\n        self.T, self.alpha, self.gamma_h = T, alpha, gamma_h\n        self.ce  = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n        self.mse = nn.MSELoss()\n\n    @staticmethod\n    def map_layers(n_s, n_t):\n        # skip embeddings index 0; map hidden layers 1..n\n        s_idx = list(range(1, n_s))  # student hidden_states length includes embeddings at 0\n        t_idx = torch.linspace(1, n_t-1, steps=len(s_idx)).round().long().tolist()\n        return list(zip(s_idx, t_idx))\n\n    def forward(self, s_pack, t_pack, labels):\n        # logits: CE + KL\n        logits_s, logits_t = s_pack[\"logits\"], t_pack[\"logits\"]\n        hard = self.ce(logits_s, labels)\n        soft = self.kld(F.log_softmax(logits_s/self.T, dim=-1),\n                        F.softmax(logits_t/self.T,  dim=-1)) * (self.T**2)\n        loss = (1 - self.alpha)*hard + self.alpha*soft\n\n        # hidden: MSE(proj(student_h), teacher_h) with proportional mapping\n        hs, ht = s_pack.get(\"hidden_states\", []), t_pack.get(\"hidden_states\", [])\n        if hs and ht:\n            pairs = self.map_layers(len(hs), len(ht))\n            h_losses = []\n            for i_s, i_t in pairs:\n                s_h = proj_head(hs[i_s])           # [B, L, t_hidden]\n                t_h = ht[i_t]\n                L = min(s_h.size(1), t_h.size(1))\n                h_losses.append(self.mse(s_h[:, :L, :], t_h[:, :L, :]))\n            if h_losses:\n                loss = loss + GAMMA_HIDDEN * torch.stack(h_losses).mean()\n\n        return loss\n\ncriterion = KDLossProj(T=KD_T, alpha=KD_ALPHA, gamma_h=GAMMA_HIDDEN)\nprint(\"‚úÖ KD loss & projection ready. (student‚Üíteacher dims: {}‚Üí{})\".format(s_hidden, t_hidden))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:12:20.680324Z","iopub.execute_input":"2025-11-02T11:12:20.681253Z","iopub.status.idle":"2025-11-02T11:12:20.697126Z","shell.execute_reply.started":"2025-11-02T11:12:20.681222Z","shell.execute_reply":"2025-11-02T11:12:20.696195Z"}},"outputs":[{"name":"stdout","text":"‚úÖ KD loss & projection ready. (student‚Üíteacher dims: 384‚Üí768)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# KD Training (teacher frozen)","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# freeze teacher for KD\nteacher.eval()\nfor p in teacher.parameters(): p.requires_grad = False\n\nopt = AdamW(list(student.parameters()) + list(proj_head.parameters()),\n            lr=LR_STUDENT, weight_decay=WEIGHT_DECAY_S)\nnum_steps = EPOCHS_STUDENT * len(train_loader)\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO_S * num_steps), num_steps)\n\ndef metrics(preds, gold):\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\"),\n    }\n\n@torch.no_grad()\ndef eval_student(loader):\n    student.eval(); proj_head.eval()\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = student(input_ids=b[\"s_input_ids\"], attention_mask=b[\"s_attention_mask\"])\n        preds += out[\"logits\"].argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return metrics(np.array(preds), np.array(gold))\n\nbest_f1, wait = -1.0, 0\n\nfor ep in range(1, EPOCHS_STUDENT+1):\n    student.train(); proj_head.train()\n    run = 0.0\n\n    for b in tqdm(train_loader, desc=f\"[KD Epoch {ep}/{EPOCHS_STUDENT}]\"):\n        labels = b[\"labels\"].to(DEVICE)\n\n        s_out = student(input_ids=b[\"s_input_ids\"].to(DEVICE),\n                        attention_mask=b[\"s_attention_mask\"].to(DEVICE),\n                        )\n\n        with torch.no_grad():\n            t_raw = teacher(input_ids=b[\"t_input_ids\"].to(DEVICE),\n                            attention_mask=b[\"t_attention_mask\"].to(DEVICE),\n                            output_hidden_states=True,\n                            return_dict=True)\n            t_out = {\"logits\": t_raw.logits, \"hidden_states\": t_raw.hidden_states}\n\n        loss = criterion(s_out, t_out, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(list(student.parameters()) + list(proj_head.parameters()), 1.0)\n        opt.step(); sch.step(); opt.zero_grad()\n        run += loss.item()\n\n    val = eval_student(val_loader)\n    print(f\"[KD] loss={run/len(train_loader):.4f} | Val Acc={val['accuracy']:.4f} | \"\n          f\"F1m={val['f1_macro']:.4f} | F1w={val['f1_weighted']:.4f}\")\n\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save({\"student\": student.state_dict(), \"proj\": proj_head.state_dict()},\n                   WORK_DIR / \"student_minilm_kd_best.pt\")\n        print(\"üíæ Saved best student.\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"‚è∏Ô∏è Early stopping.\")\n            break\n\n# reload best\nckpt = torch.load(WORK_DIR / \"student_minilm_kd_best.pt\", map_location=DEVICE)\nstudent.load_state_dict(ckpt[\"student\"]); proj_head.load_state_dict(ckpt[\"proj\"])\nstudent.eval(); proj_head.eval()\nprint(\"‚úÖ KD complete & best reloaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:12:28.175370Z","iopub.execute_input":"2025-11-02T11:12:28.175644Z","iopub.status.idle":"2025-11-02T11:22:24.927492Z","shell.execute_reply.started":"2025-11-02T11:12:28.175622Z","shell.execute_reply":"2025-11-02T11:22:24.926693Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 1/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"241e37356ac64e589c7cef08a6fa2349"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=1.8280 | Val Acc=0.9263 | F1m=0.9079 | F1w=0.9260\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 2/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09acf09df1864981ae5aa67c3bf3f843"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.9712 | Val Acc=0.9331 | F1m=0.9133 | F1w=0.9315\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 3/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38fca391e85c4ca3be8eb3c0423f45be"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.8259 | Val Acc=0.9441 | F1m=0.9309 | F1w=0.9442\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 4/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b948da53d67e464cbe202b6848e85f61"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.7322 | Val Acc=0.9450 | F1m=0.9319 | F1w=0.9450\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 5/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d49c3651a984e4d9a70c535deb1a076"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.6921 | Val Acc=0.9475 | F1m=0.9351 | F1w=0.9476\nüíæ Saved best student.\n‚úÖ KD complete & best reloaded.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Test Metrics + Alignment + Save","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\nfrom scipy.spatial.distance import cosine\n\n@torch.no_grad()\ndef eval_model(model, loader, mode=\"student\"):\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        inp = {\"input_ids\": b[\"t_input_ids\"], \"attention_mask\": b[\"t_attention_mask\"]} if mode==\"teacher\" else \\\n              {\"input_ids\": b[\"s_input_ids\"], \"attention_mask\": b[\"s_attention_mask\"]}\n        out = model(**inp)\n        logits = out[\"logits\"] if isinstance(out, dict) else out.logits\n        preds += logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\")\n    }\n\nprint(\"üß™ Evaluating on test‚Ä¶\")\nteacher_test = eval_model(teacher, test_loader, mode=\"teacher\")\nstudent_test = eval_model(student, test_loader, mode=\"student\")\nprint(\"[Teacher][Test]:\", teacher_test)\nprint(\"[Student][Test]:\", student_test)\n\n@torch.no_grad()\ndef alignment_metrics(teacher, student, loader):\n    cos_list, corr_list, agree = [], [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        t = teacher(b[\"t_input_ids\"], b[\"t_attention_mask\"])\n        s = student(b[\"s_input_ids\"], b[\"s_attention_mask\"])\n        t_logits = t.logits.detach().cpu().numpy()\n        s_logits = s[\"logits\"].detach().cpu().numpy()\n        t_probs  = softmax(t_logits, axis=-1)\n        s_probs  = softmax(s_logits, axis=-1)\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            cos_list.append(1 - cosine(tl, sl))\n            corr_list.append(np.corrcoef(tp, sp)[0, 1])\n            agree.append(np.argmax(tp) == np.argmax(sp))\n    return {\n        \"logit_cosine\": float(np.nanmean(cos_list)),\n        \"prob_corr\": float(np.nanmean(corr_list)),\n        \"pred_alignment\": float(np.mean(agree))\n    }\n\nalign = alignment_metrics(teacher, student, test_loader)\nprint(f\"\"\"\nüß© Alignment (Test)\n  ‚Ä¢ Logit cosine : {align['logit_cosine']:.4f}\n  ‚Ä¢ Prob corr    : {align['prob_corr']:.4f}\n  ‚Ä¢ Agreement    : {align['pred_alignment']:.4f}\n\"\"\")\n\n# ---- Save artifacts\nSAVE_DIR = WORK_DIR / \"student_minilm_translit_kd_proj\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\ntorch.save(student.state_dict(), SAVE_DIR / \"pytorch_model.bin\")\nfrom transformers import AutoTokenizer\n# student tokenizer saving\nAutoTokenizer.from_pretrained(STUDENT_MODEL_ID).save_pretrained(SAVE_DIR)\n\nmeta = {\n    \"teacher_model\": TEACHER_MODEL_ID,\n    \"student_model\": STUDENT_MODEL_ID,\n    \"kd_temperature\": KD_T,\n    \"alpha\": KD_ALPHA,\n    \"gamma_hidden\": GAMMA_HIDDEN,\n    \"max_len\": MAX_LEN,\n    \"lr_student\": LR_STUDENT,\n    \"epochs_student\": EPOCHS_STUDENT\n}\njson.dump(meta, open(SAVE_DIR / \"student_config.json\", \"w\"), indent=2, ensure_ascii=False)\n\ndef to_py(o):\n    if isinstance(o, dict): return {k: to_py(v) for k,v in o.items()}\n    if hasattr(o, \"item\"): return o.item()\n    return o\n\njson.dump({\"teacher_test\": to_py(teacher_test),\n           \"student_test\": to_py(student_test),\n           \"alignment\": to_py(align)},\n          open(WORK_DIR / \"metrics_minilm_kd_proj.json\", \"w\"), indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Saved student + metrics to:\", SAVE_DIR, \"and\", WORK_DIR / \"metrics_minilm_kd_proj.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T11:45:26.281975Z","iopub.execute_input":"2025-11-02T11:45:26.282243Z","iopub.status.idle":"2025-11-02T11:45:48.546684Z","shell.execute_reply.started":"2025-11-02T11:45:26.282224Z","shell.execute_reply":"2025-11-02T11:45:48.545797Z"}},"outputs":[{"name":"stdout","text":"üß™ Evaluating on test‚Ä¶\n[Teacher][Test]: {'accuracy': 0.9576629974597799, 'f1_macro': 0.9478168809959774, 'f1_weighted': 0.9577781567166664}\n[Student][Test]: {'accuracy': 0.9415749364944962, 'f1_macro': 0.9276577385153364, 'f1_weighted': 0.9416018036720621}\n\nüß© Alignment (Test)\n  ‚Ä¢ Logit cosine : 0.8970\n  ‚Ä¢ Prob corr    : 0.9001\n  ‚Ä¢ Agreement    : 0.9500\n\n‚úÖ Saved student + metrics to: /kaggle/working/student_minilm_translit_kd_proj and /kaggle/working/metrics_minilm_kd_proj.json\n","output_type":"stream"}],"execution_count":9}]}