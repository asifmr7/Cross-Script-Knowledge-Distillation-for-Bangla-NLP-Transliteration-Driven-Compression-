{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceType":"datasetVersion","sourceId":13581507,"datasetId":8628620}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir transformers==4.45.2 indic-transliteration -q\n!pip install scikit-learn pandas tqdm matplotlib -q\n\nimport os, torch, random, numpy as np, json\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nWORK_DIR = Path(\"/kaggle/working\")\nWORK_DIR.mkdir(exist_ok=True, parents=True)\nprint(\"‚úÖ Device:\", DEVICE)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:40:02.191815Z","iopub.execute_input":"2025-11-01T23:40:02.192509Z","iopub.status.idle":"2025-11-01T23:40:23.657796Z","shell.execute_reply.started":"2025-11-01T23:40:02.192482Z","shell.execute_reply":"2025-11-01T23:40:23.656942Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m296.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m136.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m290.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports, Config","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nDATA_DIR = Path(\"/kaggle/input/dataaaaaa\")\nPOS_FILE = DATA_DIR / \"all_positive_8500.txt\"\nNEG_FILE = DATA_DIR / \"all_negative_3307.txt\"\nassert POS_FILE.exists() and NEG_FILE.exists(), \"Data files missing!\"\n\ndef read_txt(p):\n    with open(p, encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos, neg = read_txt(POS_FILE), read_txt(NEG_FILE)\ndf = pd.DataFrame({\"text\": pos + neg, \"label\": [1]*len(pos) + [0]*len(neg)}).sample(frac=1, random_state=SEED)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)\nprint(f\"Train={len(train_df)}, Val={len(val_df)}, Test={len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:41:51.657121Z","iopub.execute_input":"2025-11-01T23:41:51.657641Z","iopub.status.idle":"2025-11-01T23:41:52.612822Z","shell.execute_reply.started":"2025-11-01T23:41:51.657613Z","shell.execute_reply":"2025-11-01T23:41:52.612116Z"}},"outputs":[{"name":"stdout","text":"Train=9445, Val=1181, Test=1181\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Train teacher xlmRand save best","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\nimport torch.nn as nn\n\nTEACHER_MODEL_ID = \"xlm-roberta-base\"\nNUM_LABELS = 2\nMAX_LEN = 128\nBATCH_SIZE = 16\nEPOCHS = 3\nLR = 2e-5\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\n\nteacher_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\nteacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=NUM_LABELS).to(DEVICE)\n\nclass TextDataset(Dataset):\n    def __init__(self, df, tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.tok, self.max_len = tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\"input_ids\": enc[\"input_ids\"].squeeze(0),\n                \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n                \"labels\": torch.tensor(self.labels[i], dtype=torch.long)}\n\ntrain_loader = DataLoader(TextDataset(train_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(TextDataset(val_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\ntest_loader_ = DataLoader(TextDataset(test_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\nopt = AdamW(teacher.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\nsteps = len(train_loader)*EPOCHS\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO*steps), steps)\n\nbest_f1 = -1\nfor ep in range(1, EPOCHS+1):\n    teacher.train(); total_loss = 0\n    for b in tqdm(train_loader, desc=f\"Teacher Epoch {ep}/{EPOCHS}\"):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward()\n        opt.step(); sch.step(); opt.zero_grad()\n        total_loss += loss.item()\n\n    # Validation\n    teacher.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in val_loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = teacher(**b)\n            preds += out.logits.argmax(-1).cpu().tolist()\n            gold  += b[\"labels\"].cpu().tolist()\n    acc = accuracy_score(gold, preds)\n    f1m = f1_score(gold, preds, average=\"macro\")\n    print(f\"Val Acc={acc:.4f} | F1_macro={f1m:.4f}\")\n    if f1m > best_f1:\n        best_f1 = f1m\n        teacher.save_pretrained(WORK_DIR/\"fine_tuned_xlmr\")\n        teacher_tok.save_pretrained(WORK_DIR/\"fine_tuned_xlmr\")\n        print(\"‚úÖ Saved best teacher.\")\n\n# Evaluate on test\nteacher.eval(); preds, gold = [], []\nwith torch.no_grad():\n    for b in test_loader_:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        preds += out.logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\nprint(\"Teacher Test Acc=\", accuracy_score(gold,preds), \"MacroF1=\", f1_score(gold,preds,average=\"macro\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:44:22.458897Z","iopub.execute_input":"2025-11-01T23:44:22.459813Z","iopub.status.idle":"2025-11-01T23:57:30.441488Z","shell.execute_reply.started":"2025-11-01T23:44:22.459786Z","shell.execute_reply":"2025-11-01T23:57:30.440588Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbcdc231a3b844158d00668d0854401b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2bc6a255d3044d8f852c833da7ff5f7e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7455727187449babb526fb8df261cdf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1e1c8316c63f40d0a77d06fe400c9e78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6348ac8e4c214445a22a99168e193484"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 1/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5c23194b01344962a3d3cf3464510e84"}},"metadata":{}},{"name":"stdout","text":"Val Acc=0.9526 | F1_macro=0.9416\n‚úÖ Saved best teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 2/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8385d07ae6a49c5a0e42631c15e3e76"}},"metadata":{}},{"name":"stdout","text":"Val Acc=0.9602 | F1_macro=0.9506\n‚úÖ Saved best teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 3/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e87173e18d1428983b0fedcc77b3794"}},"metadata":{}},{"name":"stdout","text":"Val Acc=0.9636 | F1_macro=0.9548\n‚úÖ Saved best teacher.\nTeacher Test Acc= 0.9576629974597799 MacroF1= 0.9478168809959774\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Transliteration KD Dataset","metadata":{}},{"cell_type":"code","source":"from indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\ndef transliterate_bn_text(text: str):\n    try:\n        return transliterate(text, sanscript.BENGALI, sanscript.ITRANS)\n    except Exception:\n        return text\n\nSTUDENT_MODEL_ID = \"distilroberta-base\"\nfrom transformers import AutoTokenizer\nstudent_tok = AutoTokenizer.from_pretrained(STUDENT_MODEL_ID)\n\nclass KDDataset(Dataset):\n    def __init__(self, df, t_tok, s_tok, max_len):\n        self.texts, self.labels = df.text.tolist(), df.label.tolist()\n        self.ttok, self.stok, self.max_len = t_tok, s_tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        txt_bn = self.texts[i]\n        txt_en = transliterate_bn_text(txt_bn)\n        t = self.ttok(txt_bn, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        s = self.stok(txt_en, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"t_input_ids\": t[\"input_ids\"].squeeze(0),\n            \"t_attention_mask\": t[\"attention_mask\"].squeeze(0),\n            \"s_input_ids\": s[\"input_ids\"].squeeze(0),\n            \"s_attention_mask\": s[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\ndef pad_collate(batch, t_pad, s_pad):\n    out = {}\n    for k in batch[0]:\n        if k == \"labels\": out[k] = torch.stack([b[k] for b in batch])\n        elif k.startswith(\"t_\"):\n            padv = 0 if \"attention\" in k else t_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n        elif k.startswith(\"s_\"):\n            padv = 0 if \"attention\" in k else s_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n    return out\n\nBATCH_SIZE = 16\ntrain_loader = DataLoader(KDDataset(train_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nval_loader   = DataLoader(KDDataset(val_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\ntest_loader  = DataLoader(KDDataset(test_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nprint(\"‚úÖ KD data ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:57:37.362607Z","iopub.execute_input":"2025-11-01T23:57:37.363093Z","iopub.status.idle":"2025-11-01T23:57:38.429951Z","shell.execute_reply.started":"2025-11-01T23:57:37.363071Z","shell.execute_reply":"2025-11-01T23:57:38.429101Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c7d1a904a7a4f818acef5ed1746232f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/480 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b400ade3e68544a0a523b2b26b6b119f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddcfb509cda746f39f0b252e8623914a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32edfa0a77284608bdf67500ef685c83"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35db7561f7df4b97926e6225370e18a0"}},"metadata":{}},{"name":"stdout","text":"‚úÖ KD data ready.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel\n\nclass StudentClassifier(nn.Module):\n    def __init__(self, base_model_id, num_labels=2):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_id)\n        H = self.encoder.config.hidden_size\n        self.dropout = nn.Dropout(0.1)\n        self.classifier = nn.Linear(H, num_labels)\n    def forward(self, input_ids, attention_mask):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask,\n                           output_hidden_states=True, output_attentions=True, return_dict=True)\n        cls = out.last_hidden_state[:,0,:]\n        logits = self.classifier(self.dropout(cls))\n        return {\"logits\": logits, \"hidden_states\": out.hidden_states, \"attentions\": out.attentions}\n\nstudent = StudentClassifier(STUDENT_MODEL_ID).to(DEVICE)\nprint(\"‚úÖ Student ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:57:43.434472Z","iopub.execute_input":"2025-11-01T23:57:43.435166Z","iopub.status.idle":"2025-11-01T23:57:45.492597Z","shell.execute_reply.started":"2025-11-01T23:57:43.435142Z","shell.execute_reply":"2025-11-01T23:57:45.491933Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/331M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23c61d35f066402fb4901094688a06d5"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Student ready.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Student model + Logit + Hidden + Attention KD loss","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\nclass KDLossFull(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5, gamma_h=1.0, gamma_a=1.0):\n        super().__init__()\n        self.T, self.alpha, self.gamma_h, self.gamma_a = T, alpha, gamma_h, gamma_a\n        self.ce = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n        self.mse = nn.MSELoss()\n    @staticmethod\n    def map_layers(n_s, n_t):\n        s_idx = list(range(1, n_s + 1))\n        t_idx = torch.linspace(1, n_t, steps=n_s).round().long().tolist()\n        return list(zip(s_idx, t_idx))\n    def forward(self, s_pack, t_pack, labels):\n        logits_s, logits_t = s_pack[\"logits\"], t_pack[\"logits\"]\n        hard = self.ce(logits_s, labels)\n        soft = self.kld(F.log_softmax(logits_s/self.T, dim=-1),\n                        F.softmax(logits_t/self.T, dim=-1)) * (self.T**2)\n        total = (1-self.alpha)*hard + self.alpha*soft\n\n        hs, ht = s_pack.get(\"hidden_states\", []), t_pack.get(\"hidden_states\", [])\n        if hs and ht:\n            pairs = self.map_layers(len(hs)-1, len(ht)-1)\n            h_losses = [self.mse(hs[i_s][:,:min(hs[i_s].size(1),ht[i_t].size(1)),:],\n                                 ht[i_t][:,:min(hs[i_s].size(1),ht[i_t].size(1)),:])\n                        for i_s,i_t in pairs]\n            total += self.gamma_h * torch.stack(h_losses).mean()\n\n        as_, at_ = s_pack.get(\"attentions\", []), t_pack.get(\"attentions\", [])\n        if as_ and at_:\n            pairs = self.map_layers(len(as_), len(at_))\n            a_losses = []\n            for i_s,i_t in pairs:\n                try:\n                    s_a, t_a = as_[i_s], at_[i_t]\n                    if s_a.dim()!=4 or t_a.dim()!=4: continue\n                    L=min(s_a.size(-1),t_a.size(-1))\n                    a_losses.append(self.mse(s_a[:,:,:L,:L], t_a[:,:,:L,:L]))\n                except Exception: continue\n            if a_losses: total += self.gamma_a * torch.stack(a_losses).mean()\n        return total\n\ncriterion = KDLossFull()\nprint(\"‚úÖ KD loss ready.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:57:48.292890Z","iopub.execute_input":"2025-11-01T23:57:48.293191Z","iopub.status.idle":"2025-11-01T23:57:48.305826Z","shell.execute_reply.started":"2025-11-01T23:57:48.293168Z","shell.execute_reply":"2025-11-01T23:57:48.304966Z"}},"outputs":[{"name":"stdout","text":"‚úÖ KD loss ready.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# KD training loop enabling attentions","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\nLR, EPOCHS, WARMUP, WD, PATIENCE = 3e-5, 5, 0.1, 0.01, 2\n\nopt = AdamW(student.parameters(), lr=LR, weight_decay=WD)\nsteps = len(train_loader)*EPOCHS\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP*steps), steps)\n\nteacher.eval(); [setattr(p, \"requires_grad\", False) for p in teacher.parameters()]\n\ndef metrics(p,g):\n    return {\"accuracy\": accuracy_score(g,p),\n            \"f1_macro\": f1_score(g,p,average=\"macro\"),\n            \"f1_weighted\": f1_score(g,p,average=\"weighted\")}\n\n@torch.no_grad()\ndef eval_student(loader):\n    student.eval(); preds,gold=[],[]\n    for b in loader:\n        b={k:v.to(DEVICE) for k,v in b.items()}\n        out=student(b[\"s_input_ids\"],b[\"s_attention_mask\"])\n        preds+=out[\"logits\"].argmax(-1).cpu().tolist(); gold+=b[\"labels\"].cpu().tolist()\n    return metrics(np.array(preds),np.array(gold))\n\nbest_f1, wait = -1, 0\nfor ep in range(1,EPOCHS+1):\n    student.train(); run=0.0\n    for b in tqdm(train_loader,desc=f\"KD Epoch {ep}/{EPOCHS}\"):\n        labels=b[\"labels\"].to(DEVICE)\n        s_out=student(b[\"s_input_ids\"].to(DEVICE),b[\"s_attention_mask\"].to(DEVICE))\n        with torch.no_grad():\n            t_raw=teacher(b[\"t_input_ids\"].to(DEVICE),b[\"t_attention_mask\"].to(DEVICE),\n                          output_hidden_states=True,output_attentions=True,return_dict=True)\n            t_out={\"logits\":t_raw.logits,\"hidden_states\":t_raw.hidden_states,\"attentions\":t_raw.attentions}\n        loss=criterion(s_out,t_out,labels)\n        loss.backward(); nn.utils.clip_grad_norm_(student.parameters(),1.0)\n        opt.step(); sch.step(); opt.zero_grad(); run+=loss.item()\n    val=eval_student(val_loader)\n    print(f\"[KD] Loss={run/len(train_loader):.4f} | Val F1m={val['f1_macro']:.4f} | Acc={val['accuracy']:.4f}\")\n    if val[\"f1_macro\"]>best_f1:\n        best_f1=val[\"f1_macro\"]; wait=0; torch.save(student.state_dict(),WORK_DIR/\"student_best.pt\")\n    else:\n        wait+=1\n        if wait>=PATIENCE: print(\"‚è∏Ô∏è Early stopping.\"); break\n\nstudent.load_state_dict(torch.load(WORK_DIR/\"student_best.pt\", map_location=DEVICE))\nstudent.eval()\nprint(\"‚úÖ KD training complete.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T23:57:52.588631Z","iopub.execute_input":"2025-11-01T23:57:52.589410Z","iopub.status.idle":"2025-11-02T00:14:39.825014Z","shell.execute_reply.started":"2025-11-01T23:57:52.589372Z","shell.execute_reply":"2025-11-02T00:14:39.824284Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Epoch 1/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bd8520fabec9444d8b3c5a7cfad53618"}},"metadata":{}},{"name":"stderr","text":"RobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\nXLMRobertaSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"},{"name":"stdout","text":"[KD] Loss=1.8499 | Val F1m=0.8750 | Acc=0.8916\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Epoch 2/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"820b25eba8f5442ea1f827b4aff71dbd"}},"metadata":{}},{"name":"stdout","text":"[KD] Loss=0.8079 | Val F1m=0.9143 | Acc=0.9306\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Epoch 3/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7dd65fad15e4933940426d5eada8bd8"}},"metadata":{}},{"name":"stdout","text":"[KD] Loss=0.6049 | Val F1m=0.9193 | Acc=0.9348\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Epoch 4/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"31efdc02c7b54d00ba6ad0d0a6221b0d"}},"metadata":{}},{"name":"stdout","text":"[KD] Loss=0.5158 | Val F1m=0.9238 | Acc=0.9399\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Epoch 5/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e9f81b9442241d08d4f7f1579b71752"}},"metadata":{}},{"name":"stdout","text":"[KD] Loss=0.4512 | Val F1m=0.9237 | Acc=0.9382\n‚úÖ KD training complete.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from scipy.special import softmax\nfrom scipy.spatial.distance import cosine\n\n@torch.no_grad()\ndef eval_model(model, loader, mode):\n    preds,gold=[],[]\n    for b in loader:\n        b={k:v.to(DEVICE) for k,v in b.items()}\n        inp={\"input_ids\":b[\"t_input_ids\"],\"attention_mask\":b[\"t_attention_mask\"]} if mode==\"teacher\" else \\\n            {\"input_ids\":b[\"s_input_ids\"],\"attention_mask\":b[\"s_attention_mask\"]}\n        out=model(**inp); logits=out[\"logits\"] if isinstance(out,dict) else out.logits\n        preds+=logits.argmax(-1).cpu().tolist(); gold+=b[\"labels\"].cpu().tolist()\n    return metrics(preds,gold)\n\nteacher_test=eval_model(teacher,test_loader,\"teacher\")\nstudent_test=eval_model(student,test_loader,\"student\")\nprint(\"[Teacher][Test]:\",teacher_test)\nprint(\"[Student][Test]:\",student_test)\n\n@torch.no_grad()\ndef alignment_metrics(teacher,student,loader):\n    cos, corr, agree=[],[],[]\n    for b in loader:\n        b={k:v.to(DEVICE) for k,v in b.items()}\n        t=teacher(b[\"t_input_ids\"],b[\"t_attention_mask\"])\n        s=student(b[\"s_input_ids\"],b[\"s_attention_mask\"])\n        t_logits,s_logits=t.logits.cpu().numpy(),s[\"logits\"].cpu().numpy()\n        t_probs,s_probs=softmax(t_logits,axis=-1),softmax(s_logits,axis=-1)\n        for tl,sl,tp,sp in zip(t_logits,s_logits,t_probs,s_probs):\n            cos.append(1-cosine(tl,sl))\n            corr.append(np.corrcoef(tp,sp)[0,1])\n            agree.append(np.argmax(tp)==np.argmax(sp))\n    return {\"logit_cosine\":float(np.nanmean(cos)),\n            \"prob_corr\":float(np.nanmean(corr)),\n            \"pred_alignment\":float(np.mean(agree))}\n\nalign=alignment_metrics(teacher,student,test_loader)\nprint(f\"\"\"\nüß© Alignment Results\n  Logit cosine : {align['logit_cosine']:.4f}\n  Prob corr    : {align['prob_corr']:.4f}\n  Agreement    : {align['pred_alignment']:.4f}\n\"\"\")\n\nSAVE_DIR = WORK_DIR / \"distilroberta_student_translit_kd\"\nSAVE_DIR.mkdir(exist_ok=True)\ntorch.save(student.state_dict(), SAVE_DIR/\"pytorch_model.bin\")\nstudent_tok.save_pretrained(SAVE_DIR)\njson.dump({\"teacher_test\":teacher_test,\"student_test\":student_test,\"alignment\":align},\n          open(WORK_DIR/\"metrics_distilroberta.json\",\"w\"),indent=2)\nprint(\"‚úÖ Saved model + metrics.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T00:14:44.810270Z","iopub.execute_input":"2025-11-02T00:14:44.810789Z","iopub.status.idle":"2025-11-02T00:15:11.942642Z","shell.execute_reply.started":"2025-11-02T00:14:44.810765Z","shell.execute_reply":"2025-11-02T00:15:11.941936Z"}},"outputs":[{"name":"stdout","text":"[Teacher][Test]: {'accuracy': 0.9576629974597799, 'f1_macro': 0.9478168809959774, 'f1_weighted': 0.9577781567166664}\n[Student][Test]: {'accuracy': 0.9356477561388654, 'f1_macro': 0.9191876629644036, 'f1_weighted': 0.9352154084757462}\n\nüß© Alignment Results\n  Logit cosine : 0.8833\n  Prob corr    : 0.8950\n  Agreement    : 0.9475\n\n‚úÖ Saved model + metrics.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}