{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceId":13581507,"sourceType":"datasetVersion","datasetId":8628620},{"sourceId":13581607,"sourceType":"datasetVersion","datasetId":8628682}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir transformers==4.45.2 indic-transliteration -q\n!pip install --no-cache-dir scikit-learn pandas tqdm matplotlib sentencepiece -q\n\nimport os, json, random, numpy as np, torch\nfrom pathlib import Path\n\n# ---- Repro / device / dirs\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nWORK_DIR = Path(\"/kaggle/working\"); WORK_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---- Config\nTEACHER_MODEL_ID = \"csebuetnlp/banglabert\"\nSTUDENT_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\nMAX_LEN = 128\nBATCH_SIZE = 16\n\n# Teacher FT\nEPOCHS_TEACHER = 3\nLR_TEACHER = 2e-5\nWARMUP_RATIO_T = 0.1\nWEIGHT_DECAY_T = 0.01\n\n# KD\nEPOCHS_STUDENT = 5\nLR_STUDENT = 3e-5\nWARMUP_RATIO_S = 0.1\nWEIGHT_DECAY_S = 0.01\nPATIENCE = 2\n\nKD_T = 3.0\nKD_ALPHA = 0.5\nGAMMA_HIDDEN = 1.0\n\nprint(\"‚úÖ Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:20:29.847050Z","iopub.execute_input":"2025-11-02T12:20:29.847768Z","iopub.status.idle":"2025-11-02T12:20:58.549655Z","shell.execute_reply.started":"2025-11-02T12:20:29.847740Z","shell.execute_reply":"2025-11-02T12:20:58.548891Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m120.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m317.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m328.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m294.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\n\nDATA_DIR = Path(\"/kaggle/input/dataaaaaaa\")\nPOS_FILE = DATA_DIR / \"all_positive_8500.txt\"\nNEG_FILE = DATA_DIR / \"all_negative_3307.txt\"\nassert POS_FILE.exists() and NEG_FILE.exists(), f\"Missing data: {POS_FILE}, {NEG_FILE}\"\n\ndef read_txt(p: Path):\n    with open(p, encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos, neg = read_txt(POS_FILE), read_txt(NEG_FILE)\ndf = pd.DataFrame({\"text\": pos + neg, \"label\": [1]*len(pos) + [0]*len(neg)}).sample(frac=1, random_state=SEED)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\nval_df,   test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)\nprint(f\"Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:20:58.550904Z","iopub.execute_input":"2025-11-02T12:20:58.551261Z","iopub.status.idle":"2025-11-02T12:20:59.720137Z","shell.execute_reply.started":"2025-11-02T12:20:58.551220Z","shell.execute_reply":"2025-11-02T12:20:59.719298Z"}},"outputs":[{"name":"stdout","text":"Train=9445 | Val=1181 | Test=1181\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm.auto import tqdm\n\nclass TxtClsDataset(Dataset):\n    def __init__(self, df, tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.tok, self.max_len = tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\",\n                       max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\nteacher_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\nteacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=2).to(DEVICE)\n\ntr_loader = DataLoader(TxtClsDataset(train_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nva_loader = DataLoader(TxtClsDataset(val_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\nte_loader = DataLoader(TxtClsDataset(test_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\nopt = AdamW(teacher.parameters(), lr=LR_TEACHER, weight_decay=WEIGHT_DECAY_T)\nsteps = len(tr_loader) * EPOCHS_TEACHER\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO_T*steps), steps)\ncriterion = torch.nn.CrossEntropyLoss()\n\nbest_f1 = -1\nfor ep in range(1, EPOCHS_TEACHER+1):\n    teacher.train(); total = 0\n    for b in tqdm(tr_loader, desc=f\"Teacher Epoch {ep}/{EPOCHS_TEACHER}\"):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward()\n        opt.step(); sch.step(); opt.zero_grad()\n        total += loss.item()\n\n    teacher.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in va_loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = teacher(**b)\n            preds += out.logits.argmax(-1).cpu().tolist()\n            gold  += b[\"labels\"].cpu().tolist()\n    acc = accuracy_score(gold, preds)\n    f1m = f1_score(gold, preds, average=\"macro\")\n    print(f\"Val: Acc={acc:.4f} | F1_macro={f1m:.4f}\")\n    if f1m > best_f1:\n        best_f1 = f1m\n        save_dir = WORK_DIR / \"finetuned_banglabert\"\n        save_dir.mkdir(parents=True, exist_ok=True)\n        teacher.save_pretrained(save_dir)\n        teacher_tok.save_pretrained(save_dir)\n        print(\"üíæ Saved best BanglaBERT teacher.\")\n\n# quick test\nteacher.eval(); preds, gold = [], []\nwith torch.no_grad():\n    for b in te_loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        preds += out.logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\nprint(\"‚úÖ BanglaBERT Teacher [Test]: Acc={:.4f} | F1_macro={:.4f}\".format(\n    accuracy_score(gold, preds), f1_score(gold, preds, average=\"macro\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:20:59.721126Z","iopub.execute_input":"2025-11-02T12:20:59.721629Z","iopub.status.idle":"2025-11-02T12:32:46.309860Z","shell.execute_reply.started":"2025-11-02T12:20:59.721605Z","shell.execute_reply":"2025-11-02T12:32:46.309080Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6691c269c43f487fb52d523d0c864510"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b461295c131242168b5753e0ab3b5719"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7e75488fa654a9a9eab9728d287055d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53e9cf711df944938d2eeb1f4c688de8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf280bd8a3f641229b39237cc9c2ad22"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 1/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58561da661ee4f5e9266672557dc4c0b"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9577 | F1_macro=0.9489\nüíæ Saved best BanglaBERT teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 2/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"329cee748c4144e3894870b8bcbef235"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9670 | F1_macro=0.9586\nüíæ Saved best BanglaBERT teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 3/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef507ea643654759947a16e22f7f11e4"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9678 | F1_macro=0.9601\nüíæ Saved best BanglaBERT teacher.\n‚úÖ BanglaBERT Teacher [Test]: Acc=0.9687 | F1_macro=0.9611\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# KD Data (Transliteration, 2 tokenizers)","metadata":{}},{"cell_type":"code","source":"from indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\ndef transliterate_bn_text(txt: str) -> str:\n    try:\n        return transliterate(txt, sanscript.BENGALI, sanscript.ITRANS)\n    except Exception:\n        return txt\n\nfrom transformers import AutoTokenizer\nstudent_tok = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\nclass KDDataset(Dataset):\n    def __init__(self, df, t_tok, s_tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.ttok, self.stok, self.max_len = t_tok, s_tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        txt_bn = self.texts[i]; txt_en = transliterate_bn_text(txt_bn)\n        t = self.ttok(txt_bn, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        s = self.stok(txt_en, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"t_input_ids\": t[\"input_ids\"].squeeze(0),\n            \"t_attention_mask\": t[\"attention_mask\"].squeeze(0),\n            \"s_input_ids\": s[\"input_ids\"].squeeze(0),\n            \"s_attention_mask\": s[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\ndef pad_collate(batch, t_pad, s_pad):\n    out = {}\n    for k in batch[0]:\n        if k == \"labels\": out[k] = torch.stack([b[k] for b in batch])\n        elif k.startswith(\"t_\"):\n            padv = 0 if \"attention\" in k else t_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n        elif k.startswith(\"s_\"):\n            padv = 0 if \"attention\" in k else s_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n    return out\n\ntrain_loader = DataLoader(KDDataset(train_df, teacher_tok, student_tok, MAX_LEN),\n                          batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nval_loader = DataLoader(KDDataset(val_df, teacher_tok, student_tok, MAX_LEN),\n                        batch_size=BATCH_SIZE,\n                        collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\ntest_loader = DataLoader(KDDataset(test_df, teacher_tok, student_tok, MAX_LEN),\n                         batch_size=BATCH_SIZE,\n                         collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nprint(\"‚úÖ KD dataloaders ready (BanglaBERT‚ÜíMiniLM).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:32:46.311481Z","iopub.execute_input":"2025-11-02T12:32:46.311874Z","iopub.status.idle":"2025-11-02T12:32:47.090960Z","shell.execute_reply.started":"2025-11-02T12:32:46.311854Z","shell.execute_reply":"2025-11-02T12:32:47.090375Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62991d6d593f4c8798c43ec160e823ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5d638ad24ac4266876ec59fd763b126"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63c56acb55164a9e8e8df40a85dfbd61"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"643d46ea39634b79b40130f050582c30"}},"metadata":{}},{"name":"stdout","text":"‚úÖ KD dataloaders ready (BanglaBERT‚ÜíMiniLM).\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Student (MiniLM-L6-v2) head (logits + hidden)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel\n\nclass StudentClassifier(nn.Module):\n    def __init__(self, base_model_id, num_labels=2, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_id)\n        s_H = self.encoder.config.hidden_size            # MiniLM hidden size (often 384)\n        self.s_hidden = s_H\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(s_H, num_labels)\n    def forward(self, input_ids=None, attention_mask=None, **_):\n        out = self.encoder(input_ids=input_ids,\n                           attention_mask=attention_mask,\n                           output_hidden_states=True,\n                           return_dict=True)\n        cls = out.last_hidden_state[:, 0, :]\n        logits = self.fc(self.dropout(cls))\n        return {\"logits\": logits, \"hidden_states\": out.hidden_states}\n\nstudent = StudentClassifier(STUDENT_MODEL_ID).to(DEVICE)\nprint(\"‚úÖ Student initialized. Hidden size =\", student.s_hidden)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:32:47.091585Z","iopub.execute_input":"2025-11-02T12:32:47.091774Z","iopub.status.idle":"2025-11-02T12:32:48.330834Z","shell.execute_reply.started":"2025-11-02T12:32:47.091759Z","shell.execute_reply":"2025-11-02T12:32:48.330055Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe4d5b7ca3cf4a0fb11bd91701fd0f60"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9bac303a9a242c395da8c7564f9506d"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Student initialized. Hidden size = 384\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# KD Projection + Loss (CE + KL + HiddenProj)","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# discover sizes\nt_hidden = teacher.config.hidden_size    # XLM-R = 768\ns_hidden = student.s_hidden              # MiniLM-L6-v2 = 384\n\nclass KDProjectionHead(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.bridge = nn.Sequential(\n            nn.Linear(in_dim, out_dim),\n            nn.GELU(),\n            nn.LayerNorm(out_dim)\n        )\n    def forward(self, x):\n        return self.bridge(x)\n\nproj_head = KDProjectionHead(s_hidden, t_hidden).to(DEVICE)\n\nclass KDLossProj(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5, gamma_h=1.0):\n        super().__init__()\n        self.T, self.alpha, self.gamma_h = T, alpha, gamma_h\n        self.ce  = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n        self.mse = nn.MSELoss()\n\n    @staticmethod\n    def map_layers(n_s, n_t):\n        # skip embeddings index 0; map hidden layers 1..n\n        s_idx = list(range(1, n_s))  # student hidden_states length includes embeddings at 0\n        t_idx = torch.linspace(1, n_t-1, steps=len(s_idx)).round().long().tolist()\n        return list(zip(s_idx, t_idx))\n\n    def forward(self, s_pack, t_pack, labels):\n        # logits: CE + KL\n        logits_s, logits_t = s_pack[\"logits\"], t_pack[\"logits\"]\n        hard = self.ce(logits_s, labels)\n        soft = self.kld(F.log_softmax(logits_s/self.T, dim=-1),\n                        F.softmax(logits_t/self.T,  dim=-1)) * (self.T**2)\n        loss = (1 - self.alpha)*hard + self.alpha*soft\n\n        # hidden: MSE(proj(student_h), teacher_h) with proportional mapping\n        hs, ht = s_pack.get(\"hidden_states\", []), t_pack.get(\"hidden_states\", [])\n        if hs and ht:\n            pairs = self.map_layers(len(hs), len(ht))\n            h_losses = []\n            for i_s, i_t in pairs:\n                s_h = proj_head(hs[i_s])           # [B, L, t_hidden]\n                t_h = ht[i_t]\n                L = min(s_h.size(1), t_h.size(1))\n                h_losses.append(self.mse(s_h[:, :L, :], t_h[:, :L, :]))\n            if h_losses:\n                loss = loss + GAMMA_HIDDEN * torch.stack(h_losses).mean()\n\n        return loss\n\ncriterion = KDLossProj(T=KD_T, alpha=KD_ALPHA, gamma_h=GAMMA_HIDDEN)\nprint(\"‚úÖ KD loss & projection ready. (student‚Üíteacher dims: {}‚Üí{})\".format(s_hidden, t_hidden))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:32:48.331667Z","iopub.execute_input":"2025-11-02T12:32:48.331918Z","iopub.status.idle":"2025-11-02T12:32:48.346178Z","shell.execute_reply.started":"2025-11-02T12:32:48.331890Z","shell.execute_reply":"2025-11-02T12:32:48.345501Z"}},"outputs":[{"name":"stdout","text":"‚úÖ KD loss & projection ready. (student‚Üíteacher dims: 384‚Üí768)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# KD Training (teacher frozen)","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# freeze teacher for KD\nteacher.eval()\nfor p in teacher.parameters(): p.requires_grad = False\n\nopt = AdamW(list(student.parameters()) + list(proj_head.parameters()),\n            lr=LR_STUDENT, weight_decay=WEIGHT_DECAY_S)\nnum_steps = EPOCHS_STUDENT * len(train_loader)\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO_S * num_steps), num_steps)\n\ndef metrics(preds, gold):\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\"),\n    }\n\n@torch.no_grad()\ndef eval_student(loader):\n    student.eval(); proj_head.eval()\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = student(input_ids=b[\"s_input_ids\"], attention_mask=b[\"s_attention_mask\"])\n        preds += out[\"logits\"].argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return metrics(np.array(preds), np.array(gold))\n\nbest_f1, wait = -1.0, 0\n\nfor ep in range(1, EPOCHS_STUDENT+1):\n    student.train(); proj_head.train()\n    run = 0.0\n\n    for b in tqdm(train_loader, desc=f\"[KD Epoch {ep}/{EPOCHS_STUDENT}]\"):\n        labels = b[\"labels\"].to(DEVICE)\n\n        s_out = student(input_ids=b[\"s_input_ids\"].to(DEVICE),\n                        attention_mask=b[\"s_attention_mask\"].to(DEVICE),\n                        )\n\n        with torch.no_grad():\n            t_raw = teacher(input_ids=b[\"t_input_ids\"].to(DEVICE),\n                            attention_mask=b[\"t_attention_mask\"].to(DEVICE),\n                            output_hidden_states=True,\n                            return_dict=True)\n            t_out = {\"logits\": t_raw.logits, \"hidden_states\": t_raw.hidden_states}\n\n        loss = criterion(s_out, t_out, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(list(student.parameters()) + list(proj_head.parameters()), 1.0)\n        opt.step(); sch.step(); opt.zero_grad()\n        run += loss.item()\n\n    val = eval_student(val_loader)\n    print(f\"[KD] loss={run/len(train_loader):.4f} | Val Acc={val['accuracy']:.4f} | \"\n          f\"F1m={val['f1_macro']:.4f} | F1w={val['f1_weighted']:.4f}\")\n\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save({\"student\": student.state_dict(), \"proj\": proj_head.state_dict()},\n                   WORK_DIR / \"student_minilm_kd_best.pt\")\n        print(\"üíæ Saved best student.\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"‚è∏Ô∏è Early stopping.\")\n            break\n\n# reload best\nckpt = torch.load(WORK_DIR / \"student_minilm_kd_best.pt\", map_location=DEVICE)\nstudent.load_state_dict(ckpt[\"student\"]); proj_head.load_state_dict(ckpt[\"proj\"])\nstudent.eval(); proj_head.eval()\nprint(\"‚úÖ KD complete & best reloaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:32:48.346864Z","iopub.execute_input":"2025-11-02T12:32:48.347073Z","iopub.status.idle":"2025-11-02T12:43:34.148715Z","shell.execute_reply.started":"2025-11-02T12:32:48.347058Z","shell.execute_reply":"2025-11-02T12:43:34.147982Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 1/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b88450b3be1440ab6d0932fae5aaec0"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=1.8668 | Val Acc=0.9170 | F1m=0.8952 | F1w=0.9162\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 2/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dddf505fbaa3475fbcdab9083966f608"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=1.0859 | Val Acc=0.9136 | F1m=0.8837 | F1w=0.9096\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 3/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"50703e067ba94c4e8456e70a4454ded8"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.9249 | Val Acc=0.9399 | F1m=0.9247 | F1w=0.9396\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 4/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc10051ad1214cd89e54fca8a5f14328"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.8058 | Val Acc=0.9458 | F1m=0.9331 | F1w=0.9459\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 5/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c44d5540d534f9782eff35a0849e9f4"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.7441 | Val Acc=0.9450 | F1m=0.9322 | F1w=0.9451\n‚úÖ KD complete & best reloaded.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Test Metrics + Alignment + Save","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\nfrom scipy.spatial.distance import cosine\n\n@torch.no_grad()\ndef eval_model(model, loader, mode=\"student\"):\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        inp = {\"input_ids\": b[\"t_input_ids\"], \"attention_mask\": b[\"t_attention_mask\"]} if mode==\"teacher\" else \\\n              {\"input_ids\": b[\"s_input_ids\"], \"attention_mask\": b[\"s_attention_mask\"]}\n        out = model(**inp)\n        logits = out[\"logits\"] if isinstance(out, dict) else out.logits\n        preds += logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\")\n    }\n\nprint(\"üß™ Evaluating on test‚Ä¶\")\nteacher_test = eval_model(teacher, test_loader, mode=\"teacher\")\nstudent_test = eval_model(student, test_loader, mode=\"student\")\nprint(\"[Teacher][Test]:\", teacher_test)\nprint(\"[Student][Test]:\", student_test)\n\n@torch.no_grad()\ndef alignment_metrics(teacher, student, loader):\n    cos_list, corr_list, agree = [], [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        t = teacher(b[\"t_input_ids\"], b[\"t_attention_mask\"])\n        s = student(b[\"s_input_ids\"], b[\"s_attention_mask\"])\n        t_logits = t.logits.detach().cpu().numpy()\n        s_logits = s[\"logits\"].detach().cpu().numpy()\n        t_probs  = softmax(t_logits, axis=-1)\n        s_probs  = softmax(s_logits, axis=-1)\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            cos_list.append(1 - cosine(tl, sl))\n            corr_list.append(np.corrcoef(tp, sp)[0, 1])\n            agree.append(np.argmax(tp) == np.argmax(sp))\n    return {\n        \"logit_cosine\": float(np.nanmean(cos_list)),\n        \"prob_corr\": float(np.nanmean(corr_list)),\n        \"pred_alignment\": float(np.mean(agree))\n    }\n\nalign = alignment_metrics(teacher, student, test_loader)\nprint(f\"\"\"\nüß© Alignment (Test)\n  ‚Ä¢ Logit cosine : {align['logit_cosine']:.4f}\n  ‚Ä¢ Prob corr    : {align['prob_corr']:.4f}\n  ‚Ä¢ Agreement    : {align['pred_alignment']:.4f}\n\"\"\")\n\n# ---- Save artifacts\nSAVE_DIR = WORK_DIR / \"student_minilm_translit_kd_proj\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\ntorch.save(student.state_dict(), SAVE_DIR / \"pytorch_model.bin\")\nfrom transformers import AutoTokenizer\n# student tokenizer saving\nAutoTokenizer.from_pretrained(STUDENT_MODEL_ID).save_pretrained(SAVE_DIR)\n\nmeta = {\n    \"teacher_model\": TEACHER_MODEL_ID,\n    \"student_model\": STUDENT_MODEL_ID,\n    \"kd_temperature\": KD_T,\n    \"alpha\": KD_ALPHA,\n    \"gamma_hidden\": GAMMA_HIDDEN,\n    \"max_len\": MAX_LEN,\n    \"lr_student\": LR_STUDENT,\n    \"epochs_student\": EPOCHS_STUDENT\n}\njson.dump(meta, open(SAVE_DIR / \"student_config.json\", \"w\"), indent=2, ensure_ascii=False)\n\ndef to_py(o):\n    if isinstance(o, dict): return {k: to_py(v) for k,v in o.items()}\n    if hasattr(o, \"item\"): return o.item()\n    return o\n\njson.dump({\"teacher_test\": to_py(teacher_test),\n           \"student_test\": to_py(student_test),\n           \"alignment\": to_py(align)},\n          open(WORK_DIR / \"metrics_minilm_kd_proj.json\", \"w\"), indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Saved student + metrics to:\", SAVE_DIR, \"and\", WORK_DIR / \"metrics_minilm_kd_proj.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:45:45.498081Z","iopub.execute_input":"2025-11-02T12:45:45.498414Z","iopub.status.idle":"2025-11-02T12:46:08.849679Z","shell.execute_reply.started":"2025-11-02T12:45:45.498392Z","shell.execute_reply":"2025-11-02T12:46:08.848925Z"}},"outputs":[{"name":"stdout","text":"üß™ Evaluating on test‚Ä¶\n[Teacher][Test]: {'accuracy': 0.9686706181202371, 'f1_macro': 0.961136147554033, 'f1_weighted': 0.9686561287537637}\n[Student][Test]: {'accuracy': 0.9458086367485182, 'f1_macro': 0.9329615280911632, 'f1_weighted': 0.9458583354280438}\n\nüß© Alignment (Test)\n  ‚Ä¢ Logit cosine : 0.9036\n  ‚Ä¢ Prob corr    : 0.9069\n  ‚Ä¢ Agreement    : 0.9534\n\n‚úÖ Saved student + metrics to: /kaggle/working/student_minilm_translit_kd_proj and /kaggle/working/metrics_minilm_kd_proj.json\n","output_type":"stream"}],"execution_count":9}]}