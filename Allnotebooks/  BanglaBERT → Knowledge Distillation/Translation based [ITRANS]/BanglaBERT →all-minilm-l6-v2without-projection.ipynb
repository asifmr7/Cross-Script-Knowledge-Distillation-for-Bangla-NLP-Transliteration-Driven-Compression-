{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceType":"datasetVersion","sourceId":13581507,"datasetId":8628620}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir transformers==4.45.2 -q\n!pip install --no-cache-dir scikit-learn pandas tqdm matplotlib sentencepiece -q\n\nimport os, json, random, numpy as np, torch\nfrom pathlib import Path\n\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nWORK_DIR = Path(\"/kaggle/working\"); WORK_DIR.mkdir(parents=True, exist_ok=True)\n\n# --- Model config\nTEACHER_MODEL_ID = \"csebuetnlp/banglabert\"\nSTUDENT_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\nMAX_LEN = 128\nBATCH_SIZE = 16\n\n# --- Fine-tuning + KD parameters\nEPOCHS_TEACHER = 3\nLR_TEACHER = 2e-5\nEPOCHS_STUDENT = 5\nLR_STUDENT = 3e-5\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\nPATIENCE = 2\n\nKD_T = 3.0\nKD_ALPHA = 0.5\nGAMMA_HIDDEN = 1.0\n\nprint(\"‚úÖ Device:\", DEVICE)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:19:25.432296Z","iopub.execute_input":"2025-11-02T12:19:25.432847Z","iopub.status.idle":"2025-11-02T12:19:52.563663Z","shell.execute_reply.started":"2025-11-02T12:19:25.432823Z","shell.execute_reply":"2025-11-02T12:19:52.562867Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m114.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m311.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m299.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports, Config","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\n\nDATA_DIR = Path(\"/kaggle/input/dataaaaaa\")\nPOS_FILE = DATA_DIR / \"all_positive_8500.txt\"\nNEG_FILE = DATA_DIR / \"all_negative_3307.txt\"\nassert POS_FILE.exists() and NEG_FILE.exists(), f\"Missing files!\"\n\ndef read_txt(p):\n    with open(p, encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos = read_txt(POS_FILE)\nneg = read_txt(NEG_FILE)\n\ndf = pd.DataFrame({\"text\": pos + neg, \"label\": [1]*len(pos) + [0]*len(neg)}).sample(frac=1, random_state=SEED)\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)\nprint(f\"Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:19:52.564977Z","iopub.execute_input":"2025-11-02T12:19:52.565280Z","iopub.status.idle":"2025-11-02T12:19:53.648687Z","shell.execute_reply.started":"2025-11-02T12:19:52.565262Z","shell.execute_reply":"2025-11-02T12:19:53.648028Z"}},"outputs":[{"name":"stdout","text":"Train=9445 | Val=1181 | Test=1181\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Fine-tune Teacher (BanglaBERT)","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm.auto import tqdm\n\nclass TxtClsDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.tok, self.max_len = tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\nteacher_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\nteacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=2).to(DEVICE)\n\ntrain_loader = DataLoader(TxtClsDataset(train_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(TxtClsDataset(val_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\ntest_loader = DataLoader(TxtClsDataset(test_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\nopt = AdamW(teacher.parameters(), lr=LR_TEACHER, weight_decay=WEIGHT_DECAY)\nsteps = len(train_loader) * EPOCHS_TEACHER\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO * steps), steps)\n\nbest_f1 = -1.0\nfor ep in range(1, EPOCHS_TEACHER+1):\n    teacher.train(); run = 0.0\n    for b in tqdm(train_loader, desc=f\"Teacher Epoch {ep}/{EPOCHS_TEACHER}\"):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward()\n        opt.step(); sch.step(); opt.zero_grad()\n        run += loss.item()\n\n    teacher.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in val_loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = teacher(**b)\n            preds += out.logits.argmax(-1).cpu().tolist()\n            gold  += b[\"labels\"].cpu().tolist()\n    acc = accuracy_score(gold, preds)\n    f1m = f1_score(gold, preds, average=\"macro\")\n    print(f\"[Val] Acc={acc:.4f} | F1_macro={f1m:.4f}\")\n    if f1m > best_f1:\n        best_f1 = f1m\n        save_dir = WORK_DIR / \"finetuned_banglabert\"\n        save_dir.mkdir(parents=True, exist_ok=True)\n        teacher.save_pretrained(save_dir)\n        teacher_tok.save_pretrained(save_dir)\n        print(\"üíæ Saved best teacher.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:19:53.649532Z","iopub.execute_input":"2025-11-02T12:19:53.649922Z","iopub.status.idle":"2025-11-02T12:31:04.250012Z","shell.execute_reply.started":"2025-11-02T12:19:53.649893Z","shell.execute_reply":"2025-11-02T12:31:04.249315Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14f01d4b49be4020ba6f1dc68d021ca5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"87b879d3bab84de2b9979be9ca0dc20f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"214a3ef603984e0b8c00b1055bd83dfb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f3f2c0668aef438ca4e041e4e99a02d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f99ff439f6314676a2c6d9b43e30b173"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 1/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5332bea165fb4614bb0cff0249793f2d"}},"metadata":{}},{"name":"stdout","text":"[Val] Acc=0.9577 | F1_macro=0.9489\nüíæ Saved best teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 2/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e43e9533a8894278a56974666beb8a1b"}},"metadata":{}},{"name":"stdout","text":"[Val] Acc=0.9670 | F1_macro=0.9586\nüíæ Saved best teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 3/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ea83d806cbc42d883087794c8815cd3"}},"metadata":{}},{"name":"stdout","text":"[Val] Acc=0.9678 | F1_macro=0.9601\nüíæ Saved best teacher.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Transliteration KD Dataset","metadata":{}},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\n\nclass KDDataset(Dataset):\n    def __init__(self, df, teacher_tok, student_tok, max_len=128):\n        self.texts = df[\"text\"].tolist()\n        self.labels = df[\"label\"].tolist()\n        self.ttok = teacher_tok\n        self.stok = student_tok\n        self.max_len = max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        t_enc = self.ttok(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        s_enc = self.stok(text, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"t_input_ids\": t_enc[\"input_ids\"].squeeze(),\n            \"t_attention_mask\": t_enc[\"attention_mask\"].squeeze(),\n            \"s_input_ids\": s_enc[\"input_ids\"].squeeze(),\n            \"s_attention_mask\": s_enc[\"attention_mask\"].squeeze(),\n            \"labels\": torch.tensor(label, dtype=torch.long)\n        }\n\nfrom transformers import AutoTokenizer\nstudent_tok = AutoTokenizer.from_pretrained(STUDENT_MODEL_ID)\n\ntrain_loader = DataLoader(KDDataset(train_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nval_loader   = DataLoader(KDDataset(val_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE)\ntest_loader  = DataLoader(KDDataset(test_df, teacher_tok, student_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\nprint(\"‚úÖ KD Dataloaders ready (BanglaBERT ‚Üí MiniLM)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:31:04.251796Z","iopub.execute_input":"2025-11-02T12:31:04.252234Z","iopub.status.idle":"2025-11-02T12:31:04.847266Z","shell.execute_reply.started":"2025-11-02T12:31:04.252213Z","shell.execute_reply":"2025-11-02T12:31:04.846503Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8cff6d85fa074fa5939c6b3adc8a80cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59ab6123a1e640ef9055019f1c0f33fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"894347abfe8f468d8d8bc427bf31d3ab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c22f7f1e3d654770ae890c85ab6a30ef"}},"metadata":{}},{"name":"stdout","text":"‚úÖ KD Dataloaders ready (BanglaBERT ‚Üí MiniLM)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from transformers import AutoModel\nimport torch.nn.functional as F\nimport torch.nn as nn\n\nclass StudentClassifier(nn.Module):\n    def __init__(self, base_model, num_labels=2, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model)\n        hidden = self.encoder.config.hidden_size\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden, num_labels)\n    def forward(self, input_ids=None, attention_mask=None, output_hidden_states=True):\n        out = self.encoder(input_ids=input_ids, attention_mask=attention_mask,\n                           output_hidden_states=output_hidden_states, return_dict=True)\n        cls = out.last_hidden_state[:, 0, :]\n        logits = self.fc(self.dropout(cls))\n        return {\"logits\": logits, \"hidden_states\": out.hidden_states}\n\nstudent = StudentClassifier(STUDENT_MODEL_ID).to(DEVICE)\nprint(\"‚úÖ Student loaded:\", STUDENT_MODEL_ID)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:31:04.847943Z","iopub.execute_input":"2025-11-02T12:31:04.848139Z","iopub.status.idle":"2025-11-02T12:31:06.095442Z","shell.execute_reply.started":"2025-11-02T12:31:04.848123Z","shell.execute_reply":"2025-11-02T12:31:06.094656Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14463be95cf944b48b3dfa507f6e1bbe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3988d51fc07444f9adabf7d79198028"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Student loaded: sentence-transformers/all-MiniLM-L6-v2\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Student model + Logit + Hidden ","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\nimport torch.nn as nn\n\n# üéØ Logit-based KD Loss (no hidden-state matching)\nclass KDLossLogitsOnly(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5):\n        super().__init__()\n        self.T = T\n        self.alpha = alpha\n        self.ce = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n\n    def forward(self, s_pack, t_pack, labels):\n        logits_s, logits_t = s_pack[\"logits\"], t_pack[\"logits\"]\n\n        # Standard cross-entropy loss (hard labels)\n        hard_loss = self.ce(logits_s, labels)\n\n        # KL divergence loss (soft labels from teacher)\n        soft_loss = self.kld(\n            F.log_softmax(logits_s / self.T, dim=-1),\n            F.softmax(logits_t / self.T, dim=-1)\n        ) * (self.T ** 2)\n\n        # Weighted sum\n        total_loss = (1 - self.alpha) * hard_loss + self.alpha * soft_loss\n        return total_loss\n\ncriterion = KDLossLogitsOnly(T=KD_T, alpha=KD_ALPHA)\nprint(\"‚úÖ KD Loss ready (logits only ‚Äî no hidden KD)\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:49:07.733271Z","iopub.execute_input":"2025-11-02T12:49:07.733850Z","iopub.status.idle":"2025-11-02T12:49:07.740738Z","shell.execute_reply.started":"2025-11-02T12:49:07.733824Z","shell.execute_reply":"2025-11-02T12:49:07.740092Z"}},"outputs":[{"name":"stdout","text":"‚úÖ KD Loss ready (logits only ‚Äî no hidden KD)\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# KD training loop enabling attentions","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# Freeze teacher\nteacher.eval()\nfor p in teacher.parameters(): \n    p.requires_grad = False\n\n# Optimizer + Scheduler\nopt = AdamW(student.parameters(), lr=LR_STUDENT, weight_decay=WEIGHT_DECAY)\nsteps = len(train_loader) * EPOCHS_STUDENT\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO * steps), steps)\n\n# Helper metrics\ndef compute_metrics(preds, gold):\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\")\n    }\n\n@torch.no_grad()\ndef eval_student(loader):\n    student.eval()\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = student(input_ids=b[\"s_input_ids\"], attention_mask=b[\"s_attention_mask\"])\n        preds += out[\"logits\"].argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return compute_metrics(np.array(preds), np.array(gold))\n\nbest_f1, wait = -1, 0\nfor ep in range(1, EPOCHS_STUDENT + 1):\n    student.train(); run_loss = 0.0\n    for b in tqdm(train_loader, desc=f\"[KD Epoch {ep}/{EPOCHS_STUDENT}]\"):\n        labels = b[\"labels\"].to(DEVICE)\n\n        # Student forward\n        s_out = student(\n            input_ids=b[\"s_input_ids\"].to(DEVICE),\n            attention_mask=b[\"s_attention_mask\"].to(DEVICE),\n            output_hidden_states=False\n        )\n\n        # Teacher forward (frozen)\n        with torch.no_grad():\n            t_raw = teacher(\n                input_ids=b[\"t_input_ids\"].to(DEVICE),\n                attention_mask=b[\"t_attention_mask\"].to(DEVICE),\n                output_hidden_states=False,\n                return_dict=True\n            )\n            t_out = {\"logits\": t_raw.logits}\n\n        # Compute KD loss\n        loss = criterion(s_out, t_out, labels)\n        loss.backward()\n        nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n        opt.step(); sch.step(); opt.zero_grad()\n        run_loss += loss.item()\n\n    # Validation\n    val = eval_student(val_loader)\n    print(f\"[KD Epoch {ep}] loss={run_loss/len(train_loader):.4f} | \"\n          f\"Val Acc={val['accuracy']:.4f} | F1m={val['f1_macro']:.4f} | F1w={val['f1_weighted']:.4f}\")\n\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save(student.state_dict(), WORK_DIR / \"student_best_logitsKD.pt\")\n        print(\"üíæ Saved best student checkpoint.\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"‚è∏Ô∏è Early stopping.\")\n            break\n\nstudent.load_state_dict(torch.load(WORK_DIR / \"student_best_logitsKD.pt\", map_location=DEVICE))\nstudent.eval()\nprint(\"‚úÖ KD training complete (logits-only).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T12:49:34.723788Z","iopub.execute_input":"2025-11-02T12:49:34.724618Z","iopub.status.idle":"2025-11-02T12:59:04.526037Z","shell.execute_reply.started":"2025-11-02T12:49:34.724586Z","shell.execute_reply":"2025-11-02T12:59:04.525206Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"[KD Epoch 1/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ad0bab37e3346349355b2da0d0b984f"}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 1] loss=1.1971 | Val Acc=0.8679 | F1m=0.8246 | F1w=0.8629\nüíæ Saved best student checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 2/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"575892fe4a044b1a89662d577e288d65"}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 2] loss=0.7433 | Val Acc=0.8840 | F1m=0.8608 | F1w=0.8858\nüíæ Saved best student checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 3/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec78e815fb424fbc8b7933964eb96d8b"}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 3] loss=0.6146 | Val Acc=0.8848 | F1m=0.8624 | F1w=0.8868\nüíæ Saved best student checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 4/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f38c70cabb74fca8831e0b1ad201bd6"}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 4] loss=0.5398 | Val Acc=0.8975 | F1m=0.8745 | F1w=0.8981\nüíæ Saved best student checkpoint.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 5/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42a89dd847d44b1889742993d29b6e90"}},"metadata":{}},{"name":"stdout","text":"[KD Epoch 5] loss=0.4751 | Val Acc=0.9018 | F1m=0.8796 | F1w=0.9023\nüíæ Saved best student checkpoint.\n‚úÖ KD training complete (logits-only).\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from scipy.special import softmax\nfrom scipy.spatial.distance import cosine\nimport json\nimport numpy as np\nimport torch\n\n# --------------------------------------------------------------\n# üßæ Evaluate classification metrics\n# --------------------------------------------------------------\n@torch.no_grad()\ndef evaluate_model(model, loader, mode=\"student\"):\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        inp = {\n            \"input_ids\": b[\"t_input_ids\"] if mode == \"teacher\" else b[\"s_input_ids\"],\n            \"attention_mask\": b[\"t_attention_mask\"] if mode == \"teacher\" else b[\"s_attention_mask\"],\n        }\n        out = model(**inp)\n        logits = out[\"logits\"] if isinstance(out, dict) else out.logits\n        preds += logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\"),\n    }\n\nprint(\"Evaluating Teacher (BanglaBERT) ...\")\nteacher_test = evaluate_model(teacher, test_loader, mode=\"teacher\")\nprint(\"Evaluating Student (MiniLM) ...\")\nstudent_test = evaluate_model(student, test_loader, mode=\"student\")\n\nprint(\"\\n===== üìà Test Metrics =====\")\nprint(f\"üß† Teacher:  Acc={teacher_test['accuracy']:.4f} | \"\n      f\"F1_macro={teacher_test['f1_macro']:.4f} | \"\n      f\"F1_weighted={teacher_test['f1_weighted']:.4f}\")\nprint(f\"üéì Student:  Acc={student_test['accuracy']:.4f} | \"\n      f\"F1_macro={student_test['f1_macro']:.4f} | \"\n      f\"F1_weighted={student_test['f1_weighted']:.4f}\")\n\n# --------------------------------------------------------------\n# üîó Alignment metrics (cosine, correlation, agreement)\n# --------------------------------------------------------------\n@torch.no_grad()\ndef evaluate_alignment(teacher, student, loader):\n    cosine_list, corr_list, agree_list = [], [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        t_out = teacher(b[\"t_input_ids\"], b[\"t_attention_mask\"])\n        s_out = student(b[\"s_input_ids\"], b[\"s_attention_mask\"])\n        t_logits = t_out.logits.detach().cpu().numpy()\n        s_logits = s_out[\"logits\"].detach().cpu().numpy()\n        t_probs = softmax(t_logits, axis=-1)\n        s_probs = softmax(s_logits, axis=-1)\n\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            cosine_list.append(1 - cosine(tl, sl))\n            corr_list.append(np.corrcoef(tp, sp)[0, 1])\n            agree_list.append(np.argmax(tp) == np.argmax(sp))\n\n    return {\n        \"logit_cosine\": float(np.nanmean(cosine_list)),\n        \"prob_corr\": float(np.nanmean(corr_list)),\n        \"pred_alignment\": float(np.mean(agree_list)),\n    }\n\nalignment = evaluate_alignment(teacher, student, test_loader)\n\nprint(f\"\"\"\n===== üîó Alignment Metrics =====\nüîπ Logit Cosine Similarity : {alignment['logit_cosine']:.4f}\nüîπ Probability Correlation : {alignment['prob_corr']:.4f}\nüîπ Prediction Agreement    : {alignment['pred_alignment']:.4f}\n\"\"\")\n\n# --------------------------------------------------------------\n# üíæ Save model + metrics\n# --------------------------------------------------------------\nSAVE_DIR = WORK_DIR / \"student_minilm_logitsKD\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\ntorch.save(student.state_dict(), SAVE_DIR / \"pytorch_model.bin\")\nstudent_tok.save_pretrained(SAVE_DIR)\n\nmetrics = {\n    \"teacher_test\": teacher_test,\n    \"student_test\": student_test,\n    \"alignment\": alignment,\n}\nwith open(WORK_DIR / \"metrics_minilm_logitsKD.json\", \"w\") as f:\n    json.dump(metrics, f, indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Saved model + metrics to\", SAVE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-02T13:02:54.491186Z","iopub.execute_input":"2025-11-02T13:02:54.491929Z","iopub.status.idle":"2025-11-02T13:03:15.197559Z","shell.execute_reply.started":"2025-11-02T13:02:54.491896Z","shell.execute_reply":"2025-11-02T13:03:15.196763Z"}},"outputs":[{"name":"stdout","text":"Evaluating Teacher (BanglaBERT) ...\nEvaluating Student (MiniLM) ...\n\n===== üìà Test Metrics =====\nüß† Teacher:  Acc=0.9687 | F1_macro=0.9611 | F1_weighted=0.9687\nüéì Student:  Acc=0.8984 | F1_macro=0.8759 | F1_weighted=0.8991\n\n===== üîó Alignment Metrics =====\nüîπ Logit Cosine Similarity : 0.8138\nüîπ Probability Correlation : 0.8188\nüîπ Prediction Agreement    : 0.9094\n\n‚úÖ Saved model + metrics to /kaggle/working/student_minilm_logitsKD\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}