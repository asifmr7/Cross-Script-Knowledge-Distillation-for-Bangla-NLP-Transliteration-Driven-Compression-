{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install -U transformers accelerate indic-transliteration scikit-learn\nimport torch, sys\nprint(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\nprint(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:11:19.686683Z","iopub.execute_input":"2025-11-01T21:11:19.686976Z","iopub.status.idle":"2025-11-01T21:12:58.454403Z","shell.execute_reply.started":"2025-11-01T21:11:19.686952Z","shell.execute_reply":"2025-11-01T21:12:58.453699Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m93.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m103.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mTorch: 2.6.0+cu124 | CUDA: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports, Config","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# ‚öôÔ∏è Configuration + Imports (Full KD: logits + hidden + attention)\n# ==============================================================\n\nimport os, json, random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nfrom torch.optim import AdamW   # ‚úÖ correct import (transformers >= 4.46)\n\n# ----------------------------\n# üîπ Reproducibility + Device\n# ----------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"‚úÖ Using device: {DEVICE}\")\n\n# ----------------------------\n# üîπ Paths\n# ----------------------------\nINPUT_DIR = Path(\"/kaggle/input\")\nWORK_DIR  = Path(\"/kaggle/working\")\nWORK_DIR.mkdir(exist_ok=True)\n\n# ----------------------------\n# üîπ Model IDs\n# ----------------------------\nTEACHER_MODEL_ID = \"csebuetnlp/banglabert\"       # Teacher: BanglaBERT\nSTUDENT_MODEL_ID = \"distilroberta-base\"     # Student: DistilBERT (shared tokenizer)\n\n# ----------------------------\n# üîπ Training parameters\n# ----------------------------\nMAX_LEN = 128\nBATCH_SIZE = 16\nEPOCHS_TEACHER = 3\nEPOCHS_STUDENT = 7\nLR_TEACHER = 2e-5\nLR_STUDENT = 3e-5\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\nPATIENCE = 2\n\n# ----------------------------\n# üîπ Knowledge Distillation hyperparameters\n# ----------------------------\nKD_T = 3.0          # temperature for softening logits\nKD_ALPHA = 0.5      # weight for soft vs hard supervision\nGAMMA_HIDDEN = 1.0  # layer-wise hidden-state distillation weight\nGAMMA_ATT   = 1.0   # üî• new: attention-map distillation weight\n\nprint(f\"\"\"\nKD configuration:\n  Temperature (T) .......... {KD_T}\n  Alpha (KL vs CE) ......... {KD_ALPHA}\n  Hidden-state MSE weight .. {GAMMA_HIDDEN}\n  Attention-map MSE weight . {GAMMA_ATT}\n\"\"\")\n\n# ----------------------------\n# üîπ Utility: metrics\n# ----------------------------\ndef compute_metrics(preds, labels):\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:38:31.359088Z","iopub.execute_input":"2025-11-01T21:38:31.359762Z","iopub.status.idle":"2025-11-01T21:38:31.369727Z","shell.execute_reply.started":"2025-11-01T21:38:31.359739Z","shell.execute_reply":"2025-11-01T21:38:31.369119Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: cuda\n\nKD configuration:\n  Temperature (T) .......... 3.0\n  Alpha (KL vs CE) ......... 0.5\n  Hidden-state MSE weight .. 1.0\n  Attention-map MSE weight . 1.0\n\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# üîç Inspect Kaggle input directories\nimport os\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    print(root)\n    for f in files:\n        print(\"   \", f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:14:55.165655Z","iopub.execute_input":"2025-11-01T21:14:55.166140Z","iopub.status.idle":"2025-11-01T21:14:55.174181Z","shell.execute_reply.started":"2025-11-01T21:14:55.166117Z","shell.execute_reply":"2025-11-01T21:14:55.173536Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input\n    all_negative_3307.txt\n    all_positive_8500.txt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Load pos.txt / neg.txt from /kaggle/input and split","metadata":{}},{"cell_type":"code","source":"def try_paths():\n    if (INPUT_DIR / \"all_positive_8500.txt\").exists() and (INPUT_DIR / \"all_negative_3307.txt\").exists():\n        return INPUT_DIR\n    # fallback: scan /kaggle/input for the files\n    for root, dirs, files in os.walk(\"/kaggle/input\"):\n        if \"all_positive_8500.txt\" in files and \"all_negative_3307.txt\" in files:\n            return Path(root)\n    raise FileNotFoundError(\"Could not find all_positive_8500.txt and all_negative_3307.txt under /kaggle/input\")\n\nDATA_DIR = try_paths()\nprint(\"Using data dir:\", DATA_DIR)\n\npos_path = DATA_DIR / \"all_positive_8500.txt\"\nneg_path = DATA_DIR / \"all_negative_3307.txt\"\n\ndef read_lines(p: Path):\n    with open(p, \"r\", encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos = read_lines(pos_path)\nneg = read_lines(neg_path)\nprint(f\"Loaded {len(pos)} positive, {len(neg)} negative\")\n\ndf = pd.concat([\n    pd.DataFrame({\"text\": pos, \"label\": 1}),\n    pd.DataFrame({\"text\": neg, \"label\": 0})\n], ignore_index=True).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n\ntrain_df, tmp_df = train_test_split(df, test_size=0.2, stratify=df.label, random_state=SEED)\nval_df,   test_df = train_test_split(tmp_df, test_size=0.5, stratify=tmp_df.label, random_state=SEED)\nprint(f\"Train/Val/Test: {len(train_df)}/{len(val_df)}/{len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:14:58.074097Z","iopub.execute_input":"2025-11-01T21:14:58.074353Z","iopub.status.idle":"2025-11-01T21:14:58.156170Z","shell.execute_reply.started":"2025-11-01T21:14:58.074335Z","shell.execute_reply":"2025-11-01T21:14:58.155548Z"}},"outputs":[{"name":"stdout","text":"Using data dir: /kaggle/input\nLoaded 8500 positive, 3307 negative\nTrain/Val/Test: 9445/1181/1181\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# üîÅ Shared BanglaBERT tokenizer for BOTH teacher & student\nshared_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\n\nclass SimpleDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.texts = df.text.tolist()\n        self.labels = df.label.tolist()\n        self.tk = tokenizer; self.max_len = max_len\n    def __len__(self): return len(self.labels)\n    def __getitem__(self, i):\n        enc = self.tk(self.texts[i], truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n        item = {k:v.squeeze(0) for k,v in enc.items()}\n        item[\"labels\"] = torch.tensor(self.labels[i], dtype=torch.long)\n        return item\n\ndef pad_collate(batch, pad_id):\n    keys = batch[0].keys()\n    out = {}\n    for k in keys:\n        if k == \"labels\":\n            out[k] = torch.stack([b[k] for b in batch])\n        else:\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True,\n                                               padding_value=(pad_id if k!=\"attention_mask\" else 0))\n    return out\n\ntrain_loader = DataLoader(SimpleDataset(train_df, shared_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=lambda b: pad_collate(b, shared_tok.pad_token_id))\nval_loader   = DataLoader(SimpleDataset(val_df, shared_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False,\n                          collate_fn=lambda b: pad_collate(b, shared_tok.pad_token_id))\ntest_loader  = DataLoader(SimpleDataset(test_df, shared_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False,\n                          collate_fn=lambda b: pad_collate(b, shared_tok.pad_token_id))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:15:01.409182Z","iopub.execute_input":"2025-11-01T21:15:01.410099Z","iopub.status.idle":"2025-11-01T21:15:02.309243Z","shell.execute_reply.started":"2025-11-01T21:15:01.410062Z","shell.execute_reply":"2025-11-01T21:15:02.308489Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9a45e1aa1b4e47ed8e7c61c07bbb6340"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2dc8d1acfd4cc28e828ca18054b35e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac7bae50ec444454a3d0b7874208a342"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39051cb9529b413eb73afdd82f9687dc"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Train teacher (BanglaBERT) and save best","metadata":{}},{"cell_type":"code","source":"teacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=2).to(DEVICE)\n\ndef evaluate_cls(model, loader):\n    model.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k:v.to(DEVICE) for k,v in batch.items()}\n            out = model(**{k:v for k,v in batch.items() if k!=\"labels\"})\n            preds += out.logits.argmax(-1).detach().cpu().tolist()\n            gold  += batch[\"labels\"].detach().cpu().tolist()\n    return compute_metrics(np.array(preds), np.array(gold))\n\nnum_steps = EPOCHS_TEACHER * len(train_loader)\nwarm_steps = int(WARMUP_RATIO * num_steps)\nopt_t = AdamW(teacher.parameters(), lr=LR_TEACHER, weight_decay=WEIGHT_DECAY)\nsch_t = get_linear_schedule_with_warmup(opt_t, warm_steps, num_steps)\n\nbest_f1, wait = -1, 0\nfor ep in range(1, EPOCHS_TEACHER+1):\n    teacher.train(); total = 0.0\n    for b in tqdm(train_loader, desc=f\"[Teacher] Epoch {ep}\", leave=False):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward()\n        nn.utils.clip_grad_norm_(teacher.parameters(), 1.0)\n        opt_t.step(); sch_t.step(); opt_t.zero_grad()\n        total += loss.item()\n    val = evaluate_cls(teacher, val_loader)\n    print(f\"[Teacher] loss={total/len(train_loader):.4f} | Val F1m={val['f1_macro']:.4f}\")\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        teacher.save_pretrained(WORK_DIR / \"teacher_model_sharedtok\")\n        shared_tok.save_pretrained(WORK_DIR / \"teacher_model_sharedtok\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"Early stop teacher.\")\n            break\n\n# reload best & test\nteacher = AutoModelForSequenceClassification.from_pretrained(WORK_DIR / \"teacher_model_sharedtok\").to(DEVICE)\nteacher_test = evaluate_cls(teacher, test_loader)\nprint(\"[Teacher][Test]:\", teacher_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:15:06.743174Z","iopub.execute_input":"2025-11-01T21:15:06.743922Z","iopub.status.idle":"2025-11-01T21:22:04.681582Z","shell.execute_reply.started":"2025-11-01T21:15:06.743896Z","shell.execute_reply":"2025-11-01T21:22:04.680903Z"}},"outputs":[{"name":"stderr","text":"2025-11-01 21:15:08.782786: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1762031708.980101      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1762031709.036392      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f3e2e6609d748bba961bc10b474e712"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e78789cc6534ffd8063b4a6a7aa5774"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[Teacher] Epoch 1:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Teacher] loss=0.2389 | Val F1m=0.9559\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Teacher] Epoch 2:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Teacher] loss=0.0880 | Val F1m=0.9488\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Teacher] Epoch 3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Teacher] loss=0.0459 | Val F1m=0.9573\n[Teacher][Test]: {'accuracy': 0.9678238780694327, 'f1_macro': 0.9601959073041282, 'f1_weighted': 0.9678533866604009}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Student model + Logit + Hidden + Attention KD loss","metadata":{}},{"cell_type":"code","source":"# Student encoder with shared tokenizer: resize embeddings to shared vocab\nclass StudentClassifier(nn.Module):\n    def __init__(self, base_model_id, num_labels=2, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_id)\n        # resize token embeddings to match shared tokenizer vocab\n        self.encoder.resize_token_embeddings(len(shared_tok))\n        H = self.encoder.config.hidden_size\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(H, num_labels)\n        # optional: silence SDPA attention warning\n        if hasattr(self.encoder, \"config\"):\n            try:\n                self.encoder.config.attn_implementation = \"eager\"\n            except Exception:\n                pass\n\n    def forward(self, input_ids=None, attention_mask=None,\n                output_hidden_states=False, output_attentions=False):\n        out = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=output_hidden_states,\n            output_attentions=output_attentions,\n            return_dict=True\n        )\n        cls_vec = out.last_hidden_state[:, 0, :]\n        logits = self.classifier(self.dropout(cls_vec))\n        return {\n            \"logits\": logits,\n            \"hidden_states\": out.hidden_states if output_hidden_states else None,\n            \"attentions\": out.attentions if output_attentions else None\n        }\n\nstudent = StudentClassifier(STUDENT_MODEL_ID, num_labels=2).to(DEVICE)\n\n# KD Loss: CE + KL (logits) + layer-wise hidden-state MSE + attention-map MSE\nclass KDLossWithHiddenAtt(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5, gamma_h=1.0, gamma_a=1.0):\n        super().__init__()\n        self.T, self.alpha = T, alpha\n        self.gamma_h, self.gamma_a = gamma_h, gamma_a\n        self.ce = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n        self.mse = nn.MSELoss()\n\n    @staticmethod\n    def map_layers(n_s, n_t, for_att=False):\n        # hidden_states: indices 1..n (ignore embedding 0)\n        # attentions: indices 0..(n-1)\n        if for_att:\n            s_idx = list(range(n_s))\n            t_pos = torch.linspace(0, n_t-1, steps=len(s_idx)).round().long().tolist()\n        else:\n            s_idx = list(range(1, n_s+1))\n            t_pos = torch.linspace(1, n_t, steps=len(s_idx)).round().long().tolist()\n        return list(zip(s_idx, t_pos))\n\n    def logits_loss(self, s_logits, t_logits, labels):\n        hard = self.ce(s_logits, labels)\n        log_p_s = torch.log_softmax(s_logits / self.T, dim=-1)\n        p_t     = torch.softmax(t_logits / self.T, dim=-1)\n        soft = self.kld(log_p_s, p_t) * (self.T ** 2)\n        return (1 - self.alpha) * hard + self.alpha * soft, hard.item(), soft.item()\n\n    def hidden_loss(self, hs_s, hs_t):\n        # hs_* include embedding at 0\n        n_s = len(hs_s) - 1; n_t = len(hs_t) - 1\n        pairs = self.map_layers(n_s, n_t, for_att=False)\n        losses = []\n        for s_i, t_i in pairs:\n            s = hs_s[s_i]  # (B, Ls, Hs)\n            t = hs_t[t_i]  # (B, Lt, Ht)\n            # align sequence length to min\n            L = min(s.size(1), t.size(1))\n            s = s[:, :L, :]\n            t = t[:, :L, :]\n            # align hidden dim by pad/trunc (student -> teacher)\n            Hs, Ht = s.size(-1), t.size(-1)\n            if Hs != Ht:\n                if Hs > Ht: s = s[..., :Ht]\n                else:\n                    pad = torch.zeros(s.size(0), s.size(1), Ht-Hs, device=s.device, dtype=s.dtype)\n                    s = torch.cat([s, pad], dim=-1)\n            losses.append(self.mse(s, t))\n        return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=hs_s[0].device)\n\n    def attention_loss(self, at_s, at_t):\n        # at_*: tuples of length n_layers, each (B, H, L, L)\n        n_s = len(at_s); n_t = len(at_t)\n        pairs = self.map_layers(n_s, n_t, for_att=True)\n        losses = []\n        for s_i, t_i in pairs:\n            s = at_s[s_i]  # (B, Hs, Ls, Ls)\n            t = at_t[t_i]  # (B, Ht, Lt, Lt)\n            # average heads to avoid head-count mismatch\n            s = s.mean(dim=1)  # (B, Ls, Ls)\n            t = t.mean(dim=1)  # (B, Lt, Lt)\n            # normalize rows (softmax) to stabilize\n            s = torch.softmax(s, dim=-1)\n            t = torch.softmax(t, dim=-1)\n            # align L\n            L = min(s.size(-1), t.size(-1))\n            s = s[:, :L, :L]\n            t = t[:, :L, :L]\n            losses.append(self.mse(s, t))\n        return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=at_s[0].device)\n\n    def forward(self, s_pack, t_pack, labels):\n        # logits loss\n        total, hard, soft = self.logits_loss(s_pack[\"logits\"], t_pack[\"logits\"], labels)\n\n        # hidden loss\n        h_loss = self.hidden_loss(s_pack[\"hidden_states\"], t_pack[\"hidden_states\"]) if self.gamma_h > 0 else 0.0\n        a_loss = self.attention_loss(s_pack[\"attentions\"],    t_pack[\"attentions\"])    if self.gamma_a > 0 else 0.0\n\n        total = total + self.gamma_h * h_loss + self.gamma_a * a_loss\n        parts = {\n            \"hard_ce\": hard,\n            \"soft_kl\": soft,\n            \"hidden_mse\": float(h_loss) if isinstance(h_loss, torch.Tensor) else h_loss,\n            \"attn_mse\":   float(a_loss) if isinstance(a_loss, torch.Tensor) else a_loss\n        }\n        return total, parts\n\ncriterion = KDLossWithHiddenAtt(T=KD_T, alpha=KD_ALPHA, gamma_h=GAMMA_HIDDEN, gamma_a=GAMMA_ATT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:38:45.680230Z","iopub.execute_input":"2025-11-01T21:38:45.680475Z","iopub.status.idle":"2025-11-01T21:38:46.225104Z","shell.execute_reply.started":"2025-11-01T21:38:45.680458Z","shell.execute_reply":"2025-11-01T21:38:46.224486Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# KD training loop enabling attentions","metadata":{}},{"cell_type":"code","source":"# Freeze teacher\nteacher.eval()\nfor p in teacher.parameters(): p.requires_grad = False\n\nopt_s = AdamW(student.parameters(), lr=LR_STUDENT, weight_decay=WEIGHT_DECAY)\nnum_steps_s = EPOCHS_STUDENT * len(train_loader)\nwarm_steps_s = int(WARMUP_RATIO * num_steps_s)\nsch_s = get_linear_schedule_with_warmup(opt_s, warm_steps_s, num_steps_s)\n\nbest_f1, wait = -1, 0\n\ndef eval_student(loader):\n    student.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = student(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n            preds += out[\"logits\"].argmax(-1).detach().cpu().tolist()\n            gold  += b[\"labels\"].detach().cpu().tolist()\n    return compute_metrics(np.array(preds), np.array(gold))\n\nfor ep in range(1, EPOCHS_STUDENT+1):\n    student.train(); run_loss = 0.0\n    for b in tqdm(train_loader, desc=f\"[Student KD] Epoch {ep}\", leave=False):\n        labels = b[\"labels\"].to(DEVICE)\n\n        # Student forward (need logits + hidden + attentions)\n        s_out = student(\n            input_ids=b[\"input_ids\"].to(DEVICE),\n            attention_mask=b[\"attention_mask\"].to(DEVICE),\n            output_hidden_states=True,\n            output_attentions=True\n        )\n\n        # Teacher forward (same batch)\n        with torch.no_grad():\n            t_raw = teacher(\n                input_ids=b[\"input_ids\"].to(DEVICE),\n                attention_mask=b[\"attention_mask\"].to(DEVICE),\n                output_hidden_states=True,\n                output_attentions=True,\n                return_dict=True\n            )\n            t_out = {\"logits\": t_raw.logits, \"hidden_states\": t_raw.hidden_states, \"attentions\": t_raw.attentions}\n\n        # KD losses\n        loss, parts = criterion(s_out, t_out, labels)\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n        opt_s.step(); sch_s.step(); opt_s.zero_grad()\n        run_loss += loss.item()\n\n    val = eval_student(val_loader)\n    print(f\"[Student KD] loss={run_loss/len(train_loader):.4f} | Val F1m={val['f1_macro']:.4f} Acc={val['accuracy']:.4f}\")\n\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save(student.state_dict(), WORK_DIR / \"student_best_sharedtok_attKD.pt\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"Early stop student.\")\n            break\n\n# reload best\nstudent.load_state_dict(torch.load(WORK_DIR / \"student_best_sharedtok_attKD.pt\", map_location=DEVICE))\nstudent.eval()\nprint(\"‚úÖ Student KD complete (logits + hidden + attention).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:39:20.365092Z","iopub.execute_input":"2025-11-01T21:39:20.365385Z","iopub.status.idle":"2025-11-01T21:52:54.065413Z","shell.execute_reply.started":"2025-11-01T21:39:20.365363Z","shell.execute_reply":"2025-11-01T21:52:54.064563Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 1:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=1.9833 | Val F1m=0.8687 Acc=0.8882\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 2:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=1.1103 | Val F1m=0.8863 Acc=0.9145\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=0.7978 | Val F1m=0.9040 Acc=0.9196\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 4:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=0.6136 | Val F1m=0.9193 Acc=0.9356\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=0.5141 | Val F1m=0.9223 Acc=0.9382\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 6:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=0.4538 | Val F1m=0.9219 Acc=0.9365\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 7:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=0.4042 | Val F1m=0.9239 Acc=0.9390\n‚úÖ Student KD complete (logits + hidden + attention).\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# Test metrics\nteacher_test = evaluate_cls(teacher, test_loader)\nstudent_test = eval_student(test_loader)\nprint(\"[Teacher][Test]:\", teacher_test)\nprint(\"[Student][Test]:\", student_test)\n\n# -------- Alignment summary (logit cosine, prob corr, prediction agreement) --------\nfrom scipy.special import softmax\nfrom scipy.spatial.distance import cosine\n\n@torch.no_grad()\ndef evaluate_alignment(teacher, student, loader):\n    teacher.eval(); student.eval()\n    logits_cos, prob_corr = [], []\n    t_preds_all, s_preds_all = [], []\n\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        t = teacher(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n        s = student(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n\n        t_logits = t.logits.detach().cpu().numpy()\n        s_logits = s[\"logits\"].detach().cpu().numpy()\n        t_probs  = softmax(t_logits, axis=-1)\n        s_probs  = softmax(s_logits, axis=-1)\n\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            logits_cos.append(1 - cosine(tl, sl))\n            prob_corr.append(np.corrcoef(tp, sp)[0, 1])\n            t_preds_all.append(np.argmax(tp)); s_preds_all.append(np.argmax(sp))\n\n    t_preds_all = np.array(t_preds_all)\n    s_preds_all = np.array(s_preds_all)\n    return {\n        \"logit_cosine\": float(np.nanmean(logits_cos)),\n        \"prob_corr\": float(np.nanmean(prob_corr)),\n        \"pred_alignment\": float((t_preds_all == s_preds_all).mean())\n    }\n\nalignment = evaluate_alignment(teacher, student, test_loader)\n\nprint(f\"\"\"\nüß© Alignment Results (Test):\n  üîπ Logit Cosine Similarity : {alignment['logit_cosine']:.4f}\n  üîπ Probability Correlation : {alignment['prob_corr']:.4f}\n  üîπ Prediction Agreement    : {alignment['pred_alignment']:.4f}\n\"\"\")\n\n# save artifacts\nsave_dir = WORK_DIR / \"student_model_sharedtok_hiddenKD\"\nsave_dir.mkdir(parents=True, exist_ok=True)\ntorch.save(student.state_dict(), save_dir / \"pytorch_model.bin\")\nshared_tok.save_pretrained(save_dir)\nwith open(save_dir / \"student_config.json\", \"w\") as f:\n    json.dump({\n        \"base_model\": STUDENT_MODEL_ID,\n        \"num_labels\": 2,\n        \"shared_tokenizer\": TEACHER_MODEL_ID,\n        \"kd_temperature\": KD_T,\n        \"alpha\": KD_ALPHA,\n        \"gamma_hidden\": GAMMA_HIDDEN\n    }, f, indent=2, ensure_ascii=False)\n\nwith open(WORK_DIR / \"metrics_sharedtok_hiddenKD.json\", \"w\") as f:\n    json.dump({\n        \"teacher_test\": teacher_test,\n        \"student_test\": student_test,\n        \"alignment\": alignment\n    }, f, indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Saved model + metrics to /kaggle/working\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T21:56:56.685252Z","iopub.execute_input":"2025-11-01T21:56:56.685515Z","iopub.status.idle":"2025-11-01T21:57:12.574644Z","shell.execute_reply.started":"2025-11-01T21:56:56.685498Z","shell.execute_reply":"2025-11-01T21:57:12.573775Z"}},"outputs":[{"name":"stdout","text":"[Teacher][Test]: {'accuracy': 0.9678238780694327, 'f1_macro': 0.9601959073041282, 'f1_weighted': 0.9678533866604009}\n[Student][Test]: {'accuracy': 0.9331075359864521, 'f1_macro': 0.9162382020736879, 'f1_weighted': 0.9327574177354325}\n\nüß© Alignment Results (Test):\n  üîπ Logit Cosine Similarity : 0.8767\n  üîπ Probability Correlation : 0.8764\n  üîπ Prediction Agreement    : 0.9382\n\n‚úÖ Saved model + metrics to /kaggle/working\n","output_type":"stream"}],"execution_count":14}]}