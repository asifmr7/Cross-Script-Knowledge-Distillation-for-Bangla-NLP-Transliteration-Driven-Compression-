{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip -q install -U transformers accelerate indic-transliteration scikit-learn\nimport torch, sys\nprint(\"Torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\nprint(\"GPU:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:20:01.851797Z","iopub.execute_input":"2025-11-01T09:20:01.852455Z","iopub.status.idle":"2025-11-01T09:21:41.015976Z","shell.execute_reply.started":"2025-11-01T09:20:01.852429Z","shell.execute_reply":"2025-11-01T09:21:41.015170Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m375.8/375.8 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m105.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m99.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.2 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.2 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mTorch: 2.6.0+cu124 | CUDA: True\nGPU: Tesla T4\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Imports, Config","metadata":{}},{"cell_type":"code","source":"# ==============================================================\n# ‚öôÔ∏è Configuration + Imports (Full KD: logits + hidden + attention)\n# ==============================================================\n\nimport os, json, random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset, DataLoader\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModel,\n    AutoModelForSequenceClassification,\n    get_linear_schedule_with_warmup\n)\nfrom torch.optim import AdamW   # ‚úÖ correct import (transformers >= 4.46)\n\n# ----------------------------\n# üîπ Reproducibility + Device\n# ----------------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"‚úÖ Using device: {DEVICE}\")\n\n# ----------------------------\n# üîπ Paths\n# ----------------------------\nINPUT_DIR = Path(\"/kaggle/input\")\nWORK_DIR  = Path(\"/kaggle/working\")\nWORK_DIR.mkdir(exist_ok=True)\n\n# ----------------------------\n# üîπ Model IDs\n# ----------------------------\nTEACHER_MODEL_ID = \"csebuetnlp/banglabert\"       # Teacher: BanglaBERT\nSTUDENT_MODEL_ID = \"distilbert-base-multilingual-cased\"     # Student: DistilBERT (shared tokenizer)\n\n# ----------------------------\n# üîπ Training parameters\n# ----------------------------\nMAX_LEN = 128\nBATCH_SIZE = 16\nEPOCHS_TEACHER = 3\nEPOCHS_STUDENT = 4\nLR_TEACHER = 2e-5\nLR_STUDENT = 3e-5\nWARMUP_RATIO = 0.1\nWEIGHT_DECAY = 0.01\nPATIENCE = 2\n\n# ----------------------------\n# üîπ Knowledge Distillation hyperparameters\n# ----------------------------\nKD_T = 3.0          # temperature for softening logits\nKD_ALPHA = 0.5      # weight for soft vs hard supervision\nGAMMA_HIDDEN = 1.0  # layer-wise hidden-state distillation weight\nGAMMA_ATT   = 1.0   # üî• new: attention-map distillation weight\n\nprint(f\"\"\"\nKD configuration:\n  Temperature (T) .......... {KD_T}\n  Alpha (KL vs CE) ......... {KD_ALPHA}\n  Hidden-state MSE weight .. {GAMMA_HIDDEN}\n  Attention-map MSE weight . {GAMMA_ATT}\n\"\"\")\n\n# ----------------------------\n# üîπ Utility: metrics\n# ----------------------------\ndef compute_metrics(preds, labels):\n    return {\n        \"accuracy\": accuracy_score(labels, preds),\n        \"f1_macro\": f1_score(labels, preds, average=\"macro\", zero_division=0),\n        \"f1_weighted\": f1_score(labels, preds, average=\"weighted\", zero_division=0),\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:21:41.017522Z","iopub.execute_input":"2025-11-01T09:21:41.017937Z","iopub.status.idle":"2025-11-01T09:21:46.991638Z","shell.execute_reply.started":"2025-11-01T09:21:41.017916Z","shell.execute_reply":"2025-11-01T09:21:46.990970Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: cuda\n\nKD configuration:\n  Temperature (T) .......... 3.0\n  Alpha (KL vs CE) ......... 0.5\n  Hidden-state MSE weight .. 1.0\n  Attention-map MSE weight . 1.0\n\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# üîç Inspect Kaggle input directories\nimport os\nfor root, dirs, files in os.walk(\"/kaggle/input\"):\n    print(root)\n    for f in files:\n        print(\"   \", f)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:21:46.992404Z","iopub.execute_input":"2025-11-01T09:21:46.992791Z","iopub.status.idle":"2025-11-01T09:21:47.000477Z","shell.execute_reply.started":"2025-11-01T09:21:46.992745Z","shell.execute_reply":"2025-11-01T09:21:46.999801Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input\n    all_negative_3307.txt\n    all_positive_8500.txt\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# Load pos.txt / neg.txt from /kaggle/input and split","metadata":{}},{"cell_type":"code","source":"def try_paths():\n    if (INPUT_DIR / \"all_positive_8500.txt\").exists() and (INPUT_DIR / \"all_negative_3307.txt\").exists():\n        return INPUT_DIR\n    # fallback: scan /kaggle/input for the files\n    for root, dirs, files in os.walk(\"/kaggle/input\"):\n        if \"all_positive_8500.txt\" in files and \"all_negative_3307.txt\" in files:\n            return Path(root)\n    raise FileNotFoundError(\"Could not find all_positive_8500.txt and all_negative_3307.txt under /kaggle/input\")\n\nDATA_DIR = try_paths()\nprint(\"Using data dir:\", DATA_DIR)\n\npos_path = DATA_DIR / \"all_positive_8500.txt\"\nneg_path = DATA_DIR / \"all_negative_3307.txt\"\n\ndef read_lines(p: Path):\n    with open(p, \"r\", encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos = read_lines(pos_path)\nneg = read_lines(neg_path)\nprint(f\"Loaded {len(pos)} positive, {len(neg)} negative\")\n\ndf = pd.concat([\n    pd.DataFrame({\"text\": pos, \"label\": 1}),\n    pd.DataFrame({\"text\": neg, \"label\": 0})\n], ignore_index=True).sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n\ntrain_df, tmp_df = train_test_split(df, test_size=0.2, stratify=df.label, random_state=SEED)\nval_df,   test_df = train_test_split(tmp_df, test_size=0.5, stratify=tmp_df.label, random_state=SEED)\nprint(f\"Train/Val/Test: {len(train_df)}/{len(val_df)}/{len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:21:47.002026Z","iopub.execute_input":"2025-11-01T09:21:47.002250Z","iopub.status.idle":"2025-11-01T09:21:47.083804Z","shell.execute_reply.started":"2025-11-01T09:21:47.002233Z","shell.execute_reply":"2025-11-01T09:21:47.083003Z"}},"outputs":[{"name":"stdout","text":"Using data dir: /kaggle/input\nLoaded 8500 positive, 3307 negative\nTrain/Val/Test: 9445/1181/1181\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# üîÅ Shared BanglaBERT tokenizer for BOTH teacher & student\nshared_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\n\nclass SimpleDataset(Dataset):\n    def __init__(self, df, tokenizer, max_len):\n        self.texts = df.text.tolist()\n        self.labels = df.label.tolist()\n        self.tk = tokenizer; self.max_len = max_len\n    def __len__(self): return len(self.labels)\n    def __getitem__(self, i):\n        enc = self.tk(self.texts[i], truncation=True, max_length=self.max_len, return_tensors=\"pt\")\n        item = {k:v.squeeze(0) for k,v in enc.items()}\n        item[\"labels\"] = torch.tensor(self.labels[i], dtype=torch.long)\n        return item\n\ndef pad_collate(batch, pad_id):\n    keys = batch[0].keys()\n    out = {}\n    for k in keys:\n        if k == \"labels\":\n            out[k] = torch.stack([b[k] for b in batch])\n        else:\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True,\n                                               padding_value=(pad_id if k!=\"attention_mask\" else 0))\n    return out\n\ntrain_loader = DataLoader(SimpleDataset(train_df, shared_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=lambda b: pad_collate(b, shared_tok.pad_token_id))\nval_loader   = DataLoader(SimpleDataset(val_df, shared_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False,\n                          collate_fn=lambda b: pad_collate(b, shared_tok.pad_token_id))\ntest_loader  = DataLoader(SimpleDataset(test_df, shared_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=False,\n                          collate_fn=lambda b: pad_collate(b, shared_tok.pad_token_id))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:21:47.084603Z","iopub.execute_input":"2025-11-01T09:21:47.084877Z","iopub.status.idle":"2025-11-01T09:21:48.116787Z","shell.execute_reply.started":"2025-11-01T09:21:47.084854Z","shell.execute_reply":"2025-11-01T09:21:48.116255Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db527686d2e48bcb2a78317b0144014"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afcf8980d0734302bd233d42ab7cb55d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a99cd27a00b343899cbaf86b1c723b8a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5b893110d631427689338433658400ab"}},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# Datasets & Tokenizers (Teacher, Student, Paired)","metadata":{}},{"cell_type":"markdown","source":"# Train teacher (BanglaBERT) and save best","metadata":{}},{"cell_type":"code","source":"teacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=2).to(DEVICE)\n\ndef evaluate_cls(model, loader):\n    model.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k:v.to(DEVICE) for k,v in batch.items()}\n            out = model(**{k:v for k,v in batch.items() if k!=\"labels\"})\n            preds += out.logits.argmax(-1).detach().cpu().tolist()\n            gold  += batch[\"labels\"].detach().cpu().tolist()\n    return compute_metrics(np.array(preds), np.array(gold))\n\nnum_steps = EPOCHS_TEACHER * len(train_loader)\nwarm_steps = int(WARMUP_RATIO * num_steps)\nopt_t = AdamW(teacher.parameters(), lr=LR_TEACHER, weight_decay=WEIGHT_DECAY)\nsch_t = get_linear_schedule_with_warmup(opt_t, warm_steps, num_steps)\n\nbest_f1, wait = -1, 0\nfor ep in range(1, EPOCHS_TEACHER+1):\n    teacher.train(); total = 0.0\n    for b in tqdm(train_loader, desc=f\"[Teacher] Epoch {ep}\", leave=False):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward()\n        nn.utils.clip_grad_norm_(teacher.parameters(), 1.0)\n        opt_t.step(); sch_t.step(); opt_t.zero_grad()\n        total += loss.item()\n    val = evaluate_cls(teacher, val_loader)\n    print(f\"[Teacher] loss={total/len(train_loader):.4f} | Val F1m={val['f1_macro']:.4f}\")\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        teacher.save_pretrained(WORK_DIR / \"teacher_model_sharedtok\")\n        shared_tok.save_pretrained(WORK_DIR / \"teacher_model_sharedtok\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"Early stop teacher.\")\n            break\n\n# reload best & test\nteacher = AutoModelForSequenceClassification.from_pretrained(WORK_DIR / \"teacher_model_sharedtok\").to(DEVICE)\nteacher_test = evaluate_cls(teacher, test_loader)\nprint(\"[Teacher][Test]:\", teacher_test)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:21:48.117563Z","iopub.execute_input":"2025-11-01T09:21:48.117809Z","iopub.status.idle":"2025-11-01T09:28:34.836455Z","shell.execute_reply.started":"2025-11-01T09:21:48.117781Z","shell.execute_reply":"2025-11-01T09:28:34.835805Z"}},"outputs":[{"name":"stderr","text":"2025-11-01 09:21:50.468119: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761988910.691986      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761988910.754245      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a17a9357aff147b2827b9a1b7774ced1"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e6999e5599445f4971f93cb46c195ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"[Teacher] Epoch 1:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Teacher] loss=0.2389 | Val F1m=0.9559\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Teacher] Epoch 2:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Teacher] loss=0.0880 | Val F1m=0.9488\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Teacher] Epoch 3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Teacher] loss=0.0459 | Val F1m=0.9573\n[Teacher][Test]: {'accuracy': 0.9678238780694327, 'f1_macro': 0.9601959073041282, 'f1_weighted': 0.9678533866604009}\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# Student model + Logit + Hidden + Attention KD loss","metadata":{}},{"cell_type":"code","source":"# Student encoder with shared tokenizer: resize embeddings to shared vocab\nclass StudentClassifier(nn.Module):\n    def __init__(self, base_model_id, num_labels=2, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_id)\n        # resize token embeddings to match shared tokenizer vocab\n        self.encoder.resize_token_embeddings(len(shared_tok))\n        H = self.encoder.config.hidden_size\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(H, num_labels)\n        # optional: silence SDPA attention warning\n        if hasattr(self.encoder, \"config\"):\n            try:\n                self.encoder.config.attn_implementation = \"eager\"\n            except Exception:\n                pass\n\n    def forward(self, input_ids=None, attention_mask=None,\n                output_hidden_states=False, output_attentions=False):\n        out = self.encoder(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            output_hidden_states=output_hidden_states,\n            output_attentions=output_attentions,\n            return_dict=True\n        )\n        cls_vec = out.last_hidden_state[:, 0, :]\n        logits = self.classifier(self.dropout(cls_vec))\n        return {\n            \"logits\": logits,\n            \"hidden_states\": out.hidden_states if output_hidden_states else None,\n            \"attentions\": out.attentions if output_attentions else None\n        }\n\nstudent = StudentClassifier(STUDENT_MODEL_ID, num_labels=2).to(DEVICE)\n\n# KD Loss: CE + KL (logits) + layer-wise hidden-state MSE + attention-map MSE\nclass KDLossWithHiddenAtt(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5, gamma_h=1.0, gamma_a=1.0):\n        super().__init__()\n        self.T, self.alpha = T, alpha\n        self.gamma_h, self.gamma_a = gamma_h, gamma_a\n        self.ce = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n        self.mse = nn.MSELoss()\n\n    @staticmethod\n    def map_layers(n_s, n_t, for_att=False):\n        # hidden_states: indices 1..n (ignore embedding 0)\n        # attentions: indices 0..(n-1)\n        if for_att:\n            s_idx = list(range(n_s))\n            t_pos = torch.linspace(0, n_t-1, steps=len(s_idx)).round().long().tolist()\n        else:\n            s_idx = list(range(1, n_s+1))\n            t_pos = torch.linspace(1, n_t, steps=len(s_idx)).round().long().tolist()\n        return list(zip(s_idx, t_pos))\n\n    def logits_loss(self, s_logits, t_logits, labels):\n        hard = self.ce(s_logits, labels)\n        log_p_s = torch.log_softmax(s_logits / self.T, dim=-1)\n        p_t     = torch.softmax(t_logits / self.T, dim=-1)\n        soft = self.kld(log_p_s, p_t) * (self.T ** 2)\n        return (1 - self.alpha) * hard + self.alpha * soft, hard.item(), soft.item()\n\n    def hidden_loss(self, hs_s, hs_t):\n        # hs_* include embedding at 0\n        n_s = len(hs_s) - 1; n_t = len(hs_t) - 1\n        pairs = self.map_layers(n_s, n_t, for_att=False)\n        losses = []\n        for s_i, t_i in pairs:\n            s = hs_s[s_i]  # (B, Ls, Hs)\n            t = hs_t[t_i]  # (B, Lt, Ht)\n            # align sequence length to min\n            L = min(s.size(1), t.size(1))\n            s = s[:, :L, :]\n            t = t[:, :L, :]\n            # align hidden dim by pad/trunc (student -> teacher)\n            Hs, Ht = s.size(-1), t.size(-1)\n            if Hs != Ht:\n                if Hs > Ht: s = s[..., :Ht]\n                else:\n                    pad = torch.zeros(s.size(0), s.size(1), Ht-Hs, device=s.device, dtype=s.dtype)\n                    s = torch.cat([s, pad], dim=-1)\n            losses.append(self.mse(s, t))\n        return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=hs_s[0].device)\n\n    def attention_loss(self, at_s, at_t):\n        # at_*: tuples of length n_layers, each (B, H, L, L)\n        n_s = len(at_s); n_t = len(at_t)\n        pairs = self.map_layers(n_s, n_t, for_att=True)\n        losses = []\n        for s_i, t_i in pairs:\n            s = at_s[s_i]  # (B, Hs, Ls, Ls)\n            t = at_t[t_i]  # (B, Ht, Lt, Lt)\n            # average heads to avoid head-count mismatch\n            s = s.mean(dim=1)  # (B, Ls, Ls)\n            t = t.mean(dim=1)  # (B, Lt, Lt)\n            # normalize rows (softmax) to stabilize\n            s = torch.softmax(s, dim=-1)\n            t = torch.softmax(t, dim=-1)\n            # align L\n            L = min(s.size(-1), t.size(-1))\n            s = s[:, :L, :L]\n            t = t[:, :L, :L]\n            losses.append(self.mse(s, t))\n        return torch.stack(losses).mean() if losses else torch.tensor(0.0, device=at_s[0].device)\n\n    def forward(self, s_pack, t_pack, labels):\n        # logits loss\n        total, hard, soft = self.logits_loss(s_pack[\"logits\"], t_pack[\"logits\"], labels)\n\n        # hidden loss\n        h_loss = self.hidden_loss(s_pack[\"hidden_states\"], t_pack[\"hidden_states\"]) if self.gamma_h > 0 else 0.0\n        a_loss = self.attention_loss(s_pack[\"attentions\"],    t_pack[\"attentions\"])    if self.gamma_a > 0 else 0.0\n\n        total = total + self.gamma_h * h_loss + self.gamma_a * a_loss\n        parts = {\n            \"hard_ce\": hard,\n            \"soft_kl\": soft,\n            \"hidden_mse\": float(h_loss) if isinstance(h_loss, torch.Tensor) else h_loss,\n            \"attn_mse\":   float(a_loss) if isinstance(a_loss, torch.Tensor) else a_loss\n        }\n        return total, parts\n\ncriterion = KDLossWithHiddenAtt(T=KD_T, alpha=KD_ALPHA, gamma_h=GAMMA_HIDDEN, gamma_a=GAMMA_ATT)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:28:34.837176Z","iopub.execute_input":"2025-11-01T09:28:34.837727Z","iopub.status.idle":"2025-11-01T09:28:37.743109Z","shell.execute_reply.started":"2025-11-01T09:28:34.837709Z","shell.execute_reply":"2025-11-01T09:28:37.742510Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/466 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7477127bb3c04a39a937b4e93bcb9aa9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/542M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d349242d6ce94f5aae31b6fa6286ac58"}},"metadata":{}}],"execution_count":7},{"cell_type":"markdown","source":"# KD training loop enabling attentions","metadata":{}},{"cell_type":"code","source":"# Freeze teacher\nteacher.eval()\nfor p in teacher.parameters(): p.requires_grad = False\n\nopt_s = AdamW(student.parameters(), lr=LR_STUDENT, weight_decay=WEIGHT_DECAY)\nnum_steps_s = EPOCHS_STUDENT * len(train_loader)\nwarm_steps_s = int(WARMUP_RATIO * num_steps_s)\nsch_s = get_linear_schedule_with_warmup(opt_s, warm_steps_s, num_steps_s)\n\nbest_f1, wait = -1, 0\n\ndef eval_student(loader):\n    student.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = student(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n            preds += out[\"logits\"].argmax(-1).detach().cpu().tolist()\n            gold  += b[\"labels\"].detach().cpu().tolist()\n    return compute_metrics(np.array(preds), np.array(gold))\n\nfor ep in range(1, EPOCHS_STUDENT+1):\n    student.train(); run_loss = 0.0\n    for b in tqdm(train_loader, desc=f\"[Student KD] Epoch {ep}\", leave=False):\n        labels = b[\"labels\"].to(DEVICE)\n\n        # Student forward (need logits + hidden + attentions)\n        s_out = student(\n            input_ids=b[\"input_ids\"].to(DEVICE),\n            attention_mask=b[\"attention_mask\"].to(DEVICE),\n            output_hidden_states=True,\n            output_attentions=True\n        )\n\n        # Teacher forward (same batch)\n        with torch.no_grad():\n            t_raw = teacher(\n                input_ids=b[\"input_ids\"].to(DEVICE),\n                attention_mask=b[\"attention_mask\"].to(DEVICE),\n                output_hidden_states=True,\n                output_attentions=True,\n                return_dict=True\n            )\n            t_out = {\"logits\": t_raw.logits, \"hidden_states\": t_raw.hidden_states, \"attentions\": t_raw.attentions}\n\n        # KD losses\n        loss, parts = criterion(s_out, t_out, labels)\n\n        loss.backward()\n        nn.utils.clip_grad_norm_(student.parameters(), 1.0)\n        opt_s.step(); sch_s.step(); opt_s.zero_grad()\n        run_loss += loss.item()\n\n    val = eval_student(val_loader)\n    print(f\"[Student KD] loss={run_loss/len(train_loader):.4f} | Val F1m={val['f1_macro']:.4f} Acc={val['accuracy']:.4f}\")\n\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save(student.state_dict(), WORK_DIR / \"student_best_sharedtok_attKD.pt\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"Early stop student.\")\n            break\n\n# reload best\nstudent.load_state_dict(torch.load(WORK_DIR / \"student_best_sharedtok_attKD.pt\", map_location=DEVICE))\nstudent.eval()\nprint(\"‚úÖ Student KD complete (logits + hidden + attention).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:28:37.743823Z","iopub.execute_input":"2025-11-01T09:28:37.744018Z","iopub.status.idle":"2025-11-01T09:36:18.732823Z","shell.execute_reply.started":"2025-11-01T09:28:37.744001Z","shell.execute_reply":"2025-11-01T09:36:18.731961Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 1:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"DistilBertSdpaAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n","output_type":"stream"},{"name":"stdout","text":"[Student KD] loss=1.6385 | Val F1m=0.8982 Acc=0.9162\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 2:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=0.8814 | Val F1m=0.9103 Acc=0.9306\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=0.6375 | Val F1m=0.9172 Acc=0.9340\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[Student KD] Epoch 4:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[Student KD] loss=0.5206 | Val F1m=0.9104 Acc=0.9280\n‚úÖ Student KD complete (logits + hidden + attention).\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# Test metrics\nteacher_test = evaluate_cls(teacher, test_loader)\nstudent_test = eval_student(test_loader)\nprint(\"[Teacher][Test]:\", teacher_test)\nprint(\"[Student][Test]:\", student_test)\n\n# -------- Alignment summary (logit cosine, prob corr, prediction agreement) --------\nfrom scipy.special import softmax\nfrom scipy.spatial.distance import cosine\n\n@torch.no_grad()\ndef evaluate_alignment(teacher, student, loader):\n    teacher.eval(); student.eval()\n    logits_cos, prob_corr = [], []\n    t_preds_all, s_preds_all = [], []\n\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        t = teacher(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n        s = student(input_ids=b[\"input_ids\"], attention_mask=b[\"attention_mask\"])\n\n        t_logits = t.logits.detach().cpu().numpy()\n        s_logits = s[\"logits\"].detach().cpu().numpy()\n        t_probs  = softmax(t_logits, axis=-1)\n        s_probs  = softmax(s_logits, axis=-1)\n\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            logits_cos.append(1 - cosine(tl, sl))\n            prob_corr.append(np.corrcoef(tp, sp)[0, 1])\n            t_preds_all.append(np.argmax(tp)); s_preds_all.append(np.argmax(sp))\n\n    t_preds_all = np.array(t_preds_all)\n    s_preds_all = np.array(s_preds_all)\n    return {\n        \"logit_cosine\": float(np.nanmean(logits_cos)),\n        \"prob_corr\": float(np.nanmean(prob_corr)),\n        \"pred_alignment\": float((t_preds_all == s_preds_all).mean())\n    }\n\nalignment = evaluate_alignment(teacher, student, test_loader)\n\nprint(f\"\"\"\nüß© Alignment Results (Test):\n  üîπ Logit Cosine Similarity : {alignment['logit_cosine']:.4f}\n  üîπ Probability Correlation : {alignment['prob_corr']:.4f}\n  üîπ Prediction Agreement    : {alignment['pred_alignment']:.4f}\n\"\"\")\n\n# save artifacts\nsave_dir = WORK_DIR / \"student_model_sharedtok_hiddenKD\"\nsave_dir.mkdir(parents=True, exist_ok=True)\ntorch.save(student.state_dict(), save_dir / \"pytorch_model.bin\")\nshared_tok.save_pretrained(save_dir)\nwith open(save_dir / \"student_config.json\", \"w\") as f:\n    json.dump({\n        \"base_model\": STUDENT_MODEL_ID,\n        \"num_labels\": 2,\n        \"shared_tokenizer\": TEACHER_MODEL_ID,\n        \"kd_temperature\": KD_T,\n        \"alpha\": KD_ALPHA,\n        \"gamma_hidden\": GAMMA_HIDDEN\n    }, f, indent=2, ensure_ascii=False)\n\nwith open(WORK_DIR / \"metrics_sharedtok_hiddenKD.json\", \"w\") as f:\n    json.dump({\n        \"teacher_test\": teacher_test,\n        \"student_test\": student_test,\n        \"alignment\": alignment\n    }, f, indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Saved model + metrics to /kaggle/working\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:36:18.733849Z","iopub.execute_input":"2025-11-01T09:36:18.734141Z","iopub.status.idle":"2025-11-01T09:36:34.273105Z","shell.execute_reply.started":"2025-11-01T09:36:18.734117Z","shell.execute_reply":"2025-11-01T09:36:34.272406Z"}},"outputs":[{"name":"stdout","text":"[Teacher][Test]: {'accuracy': 0.9678238780694327, 'f1_macro': 0.9601959073041282, 'f1_weighted': 0.9678533866604009}\n[Student][Test]: {'accuracy': 0.930567315834039, 'f1_macro': 0.9142639101365517, 'f1_weighted': 0.9306939442278058}\n\nüß© Alignment Results (Test):\n  üîπ Logit Cosine Similarity : 0.8694\n  üîπ Probability Correlation : 0.8713\n  üîπ Prediction Agreement    : 0.9356\n\n‚úÖ Saved model + metrics to /kaggle/working\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from scipy.special import softmax\nimport inspect\n\nlabel_names = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n\ndef safe_forward_student(model, enc):\n    \"\"\"\n    Safely forwards inputs through student even if tokenizer outputs token_type_ids.\n    Automatically filters unsupported kwargs.\n    \"\"\"\n    sig = inspect.signature(model.encoder.forward)\n    allowed = set(sig.parameters.keys())\n    valid_enc = {k: v for k, v in enc.items() if k in allowed or k in [\"input_ids\", \"attention_mask\"]}\n    return model(**valid_enc)\n\ndef test_one(text_bn: str):\n    # tokenize and move to GPU\n    enc = shared_tok(text_bn, truncation=True, padding=True, max_length=MAX_LEN, return_tensors=\"pt\").to(DEVICE)\n\n    with torch.no_grad():\n        # teacher forward (BanglaBERT)\n        t_out = teacher(**enc)\n        # student forward (DistilBERT) ‚Äî safely handles extra fields\n        s_out = safe_forward_student(student, enc)\n\n    # compute softmax\n    t_prob = softmax(t_out.logits.detach().cpu().numpy(), axis=-1)[0]\n    s_prob = softmax(s_out[\"logits\"].detach().cpu().numpy(), axis=-1)[0]\n    t_pred = int(t_prob.argmax())\n    s_pred = int(s_prob.argmax())\n\n    print(\"üìù Text:\", text_bn)\n    print(f\"üß† Teacher ‚Üí {label_names[t_pred]} (NEG={t_prob[0]:.4f}, POS={t_prob[1]:.4f})\")\n    print(f\"üéì Student ‚Üí {label_names[s_pred]} (NEG={s_prob[0]:.4f}, POS={s_prob[1]:.4f})\")\n    print(\"üîÅ Alignment:\", \"‚úÖ MATCH\" if t_pred == s_pred else \"‚ö†Ô∏è DIFFERENT\")\n    print()\n\n# üîç Try a few examples\nexamples = [\n    \"‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ú ‡¶ñ‡ßÅ‡¶¨ ‡¶ñ‡ßÅ‡¶∂‡¶ø‡•§\",          # positive\n    \"‡¶è‡¶á ‡¶∏‡¶ø‡¶®‡ßá‡¶Æ‡¶æ‡¶ü‡¶æ ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶¨‡¶æ‡¶ú‡ßá‡•§\",     # negative\n    \"‡¶Ü‡¶ú‡¶ï‡ßá ‡¶¶‡¶ø‡¶®‡¶ü‡¶æ ‡¶∏‡¶§‡ßç‡¶Ø‡¶ø‡¶á ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶õ‡¶ø‡¶≤‡•§\",\n    \"‡¶è‡¶á ‡¶∞‡ßá‡¶∏‡ßç‡¶ü‡ßÅ‡¶∞‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∞ ‡¶ñ‡¶æ‡¶¨‡¶æ‡¶∞‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶®‡¶æ‡•§\"\n]\n\nfor tx in examples:\n    print(\"-\" * 60)\n    test_one(tx)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T09:36:34.274821Z","iopub.execute_input":"2025-11-01T09:36:34.275211Z","iopub.status.idle":"2025-11-01T09:36:34.351357Z","shell.execute_reply.started":"2025-11-01T09:36:34.275193Z","shell.execute_reply":"2025-11-01T09:36:34.350737Z"}},"outputs":[{"name":"stdout","text":"------------------------------------------------------------\nüìù Text: ‡¶Ü‡¶Æ‡¶ø ‡¶Ü‡¶ú ‡¶ñ‡ßÅ‡¶¨ ‡¶ñ‡ßÅ‡¶∂‡¶ø‡•§\nüß† Teacher ‚Üí POSITIVE (NEG=0.0007, POS=0.9993)\nüéì Student ‚Üí POSITIVE (NEG=0.0007, POS=0.9993)\nüîÅ Alignment: ‚úÖ MATCH\n\n------------------------------------------------------------\nüìù Text: ‡¶è‡¶á ‡¶∏‡¶ø‡¶®‡ßá‡¶Æ‡¶æ‡¶ü‡¶æ ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶¨‡¶æ‡¶ú‡ßá‡•§\nüß† Teacher ‚Üí NEGATIVE (NEG=0.9945, POS=0.0055)\nüéì Student ‚Üí NEGATIVE (NEG=0.9962, POS=0.0038)\nüîÅ Alignment: ‚úÖ MATCH\n\n------------------------------------------------------------\nüìù Text: ‡¶Ü‡¶ú‡¶ï‡ßá ‡¶¶‡¶ø‡¶®‡¶ü‡¶æ ‡¶∏‡¶§‡ßç‡¶Ø‡¶ø‡¶á ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞ ‡¶õ‡¶ø‡¶≤‡•§\nüß† Teacher ‚Üí POSITIVE (NEG=0.0005, POS=0.9995)\nüéì Student ‚Üí POSITIVE (NEG=0.0004, POS=0.9996)\nüîÅ Alignment: ‚úÖ MATCH\n\n------------------------------------------------------------\nüìù Text: ‡¶è‡¶á ‡¶∞‡ßá‡¶∏‡ßç‡¶ü‡ßÅ‡¶∞‡ßá‡¶®‡ßç‡¶ü‡ßá‡¶∞ ‡¶ñ‡¶æ‡¶¨‡¶æ‡¶∞‡¶ü‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶®‡¶æ‡•§\nüß† Teacher ‚Üí NEGATIVE (NEG=0.9940, POS=0.0060)\nüéì Student ‚Üí NEGATIVE (NEG=0.9885, POS=0.0115)\nüîÅ Alignment: ‚úÖ MATCH\n\n","output_type":"stream"}],"execution_count":10}]}