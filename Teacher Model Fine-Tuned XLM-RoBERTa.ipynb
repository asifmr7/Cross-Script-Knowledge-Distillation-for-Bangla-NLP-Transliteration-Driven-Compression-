{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13479604,"sourceType":"datasetVersion","datasetId":8557871}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets evaluate accelerate torch bitsandbytes pandas scikit-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:36:55.252397Z","iopub.execute_input":"2025-10-23T16:36:55.252661Z","iopub.status.idle":"2025-10-23T16:38:15.809375Z","shell.execute_reply.started":"2025-10-23T16:36:55.252642Z","shell.execute_reply":"2025-10-23T16:38:15.808518Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m94.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m69.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.1/60.1 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.8/42.8 MB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 21.0.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os, pandas as pd, numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nimport torch\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification, \n    TrainingArguments, Trainer, pipeline\n)\nimport evaluate\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"âœ… Device:\", device)\n\nteacher_model_name = \"xlm-roberta-base\"\nstudent_model_name = \"roberta-base\"\nnum_labels = 2  # positive / negative\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:40:39.486505Z","iopub.execute_input":"2025-10-23T16:40:39.487231Z","iopub.status.idle":"2025-10-23T16:41:05.589445Z","shell.execute_reply.started":"2025-10-23T16:40:39.487204Z","shell.execute_reply":"2025-10-23T16:41:05.588732Z"}},"outputs":[{"name":"stderr","text":"2025-10-23 16:40:53.116082: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761237653.296281      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761237653.349328      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"âœ… Device: cuda\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os, pandas as pd, numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom datasets import Dataset, DatasetDict\nimport torch\nfrom transformers import (\n    AutoTokenizer, AutoModelForSequenceClassification, \n    TrainingArguments, Trainer, pipeline\n)\nimport evaluate\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"âœ… Device:\", device)\n\nteacher_model_name = \"xlm-roberta-base\"\nstudent_model_name = \"roberta-base\"\nnum_labels = 2  # positive / negative\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:41:28.736025Z","iopub.execute_input":"2025-10-23T16:41:28.737088Z","iopub.status.idle":"2025-10-23T16:41:28.742576Z","shell.execute_reply.started":"2025-10-23T16:41:28.737063Z","shell.execute_reply":"2025-10-23T16:41:28.741753Z"}},"outputs":[{"name":"stdout","text":"âœ… Device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# Paths to your txt files\npos_path = \"/kaggle/input/dataaaaaa/all_positive_8500.txt\"\nneg_path = \"/kaggle/input/dataaaaaa/all_negative_3307.txt\"\n\n# Read files (one comment per line)\nwith open(pos_path, 'r', encoding='utf-8') as f:\n    pos_lines = [line.strip() for line in f if line.strip()]\n\nwith open(neg_path, 'r', encoding='utf-8') as f:\n    neg_lines = [line.strip() for line in f if line.strip()]\n\n# Create DataFrame\ndf_pos = pd.DataFrame({\"text\": pos_lines, \"label\": 1})\ndf_neg = pd.DataFrame({\"text\": neg_lines, \"label\": 0})\ndf = pd.concat([df_pos, df_neg], ignore_index=True)\nprint(\"âœ… Dataset shape:\", df.shape)\nprint(df.head())\n\n# Save merged CSV (optional)\ncsv_path = \"bengali_youtube_sentiment.csv\"\ndf.to_csv(csv_path, index=False)\nprint(f\"ğŸ’¾ Saved merged dataset to {csv_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:41:32.854108Z","iopub.execute_input":"2025-10-23T16:41:32.854393Z","iopub.status.idle":"2025-10-23T16:41:32.972202Z","shell.execute_reply.started":"2025-10-23T16:41:32.854369Z","shell.execute_reply":"2025-10-23T16:41:32.971206Z"}},"outputs":[{"name":"stdout","text":"âœ… Dataset shape: (11807, 2)\n                                                text  label\n0                    à¦…à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦¨à¦¿à¦¶à§‹ à¦¬à¦¸à§ à¦†à¦° à¦…à¦®à¦¿ à¦­à¦¾à¦‡à¦•à§‡à¦“à¥¤      1\n1                               à¦†à¦®à¦¾à¦° à¦¦à§‡à¦–à¦¾ à¦¬à§‡à¦¸à§à¦Ÿ à¦¨à¦¾à¦Ÿà¦•      1\n2  à¦¨à¦¾à¦Ÿà¦• à¦Ÿà¦¾ à¦…à¦¨à§‡à¦• à¦¸à§à¦¨à§à¦¦à¦° à¦¹à¦¯à¦¼à§‡à¦›à§‡,,,,à¦†à¦«à¦°à¦¾à¦¨ à¦¨à¦¿à¦¶à§‹ à¦­à¦¾à¦‡à¦¯à¦¼...      1\n3                          à¦¸à¦¤à§à¦¯à¦¿ à¦…à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦à¦•à¦Ÿà¦¿ à¦°à¦¿à¦²à§‡à¦¶à¦¨      1\n4                     à¦®à¦œà¦¾ à¦ªà¦¾à¦‡à¦›à¦¿ à¦­à¦¾à¦·à¦¾ à¦—à§à¦²à§‹ à¦•à§‡à¦®à¦¨ à¦²à¦¾à¦—à¦²à§‹      1\nğŸ’¾ Saved merged dataset to bengali_youtube_sentiment.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# Train 80%, Val 10%, Test 10%\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=42)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42)\n\nprint(\"Train:\", len(train_df), \"Val:\", len(val_df), \"Test:\", len(test_df))\n\n# Convert to Hugging Face DatasetDict\ntrain_ds = Dataset.from_pandas(train_df)\nval_ds = Dataset.from_pandas(val_df)\ntest_ds = Dataset.from_pandas(test_df)\n\ndataset = DatasetDict({\n    \"train\": train_ds,\n    \"validation\": val_ds,\n    \"test\": test_ds\n})\ndataset\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:42:57.278752Z","iopub.execute_input":"2025-10-23T16:42:57.279110Z","iopub.status.idle":"2025-10-23T16:42:57.359027Z","shell.execute_reply.started":"2025-10-23T16:42:57.279088Z","shell.execute_reply":"2025-10-23T16:42:57.358140Z"}},"outputs":[{"name":"stdout","text":"Train: 9445 Val: 1181 Test: 1181\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', '__index_level_0__'],\n        num_rows: 9445\n    })\n    validation: Dataset({\n        features: ['text', 'label', '__index_level_0__'],\n        num_rows: 1181\n    })\n    test: Dataset({\n        features: ['text', 'label', '__index_level_0__'],\n        num_rows: 1181\n    })\n})"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"teacher_tokenizer = AutoTokenizer.from_pretrained(teacher_model_name)\nstudent_tokenizer = AutoTokenizer.from_pretrained(student_model_name)\n\ndef preprocess(batch):\n    return student_tokenizer(batch[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n\ntokenized_ds = dataset.map(preprocess, batched=True)\ntokenized_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n\nprint(tokenized_ds)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:43:11.373296Z","iopub.execute_input":"2025-10-23T16:43:11.373844Z","iopub.status.idle":"2025-10-23T16:43:17.717176Z","shell.execute_reply.started":"2025-10-23T16:43:11.373820Z","shell.execute_reply":"2025-10-23T16:43:17.716291Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9d07ce8fb4164c40b384eb3ddb613cb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e915edbf26a4ca090f36044c03d60ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec8f86961cfe4b8ba923bcbb07f2e51c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6960d64609064c11877c7478cc701517"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ea297d0f41644c283be762418c4fbb1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/481 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"086e3cf01e574db2a0868bb6f4fee724"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ef87fb6f700248c4b6bbfeef885e043f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ed1e02636ae84591a628b11db580fcd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"194cbf8f0be44bdca026575ec5251afc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9445 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03374c0e00194d48ae39ef10358f1e67"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1181 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b3e0ac311dd41ea9d22ad1f4906305c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1181 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bdf65cacdf924c5f8ee0beb9fd296f25"}},"metadata":{}},{"name":"stdout","text":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 9445\n    })\n    validation: Dataset({\n        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 1181\n    })\n    test: Dataset({\n        features: ['text', 'label', '__index_level_0__', 'input_ids', 'attention_mask'],\n        num_rows: 1181\n    })\n})\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"metric = evaluate.load(\"accuracy\")\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    return metric.compute(predictions=preds, references=labels)\n\nteacher_model = AutoModelForSequenceClassification.from_pretrained(\n    teacher_model_name, num_labels=num_labels\n).to(device)\n\nteacher_args = TrainingArguments(\n    output_dir=\"./xlmr_teacher_youtube\",\n    eval_strategy=\"epoch\",\n    save_strategy=\"epoch\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs_teacher\",\n    report_to=\"none\"\n)\n\ntrainer_teacher = Trainer(\n    model=teacher_model,\n    args=teacher_args,\n    train_dataset=tokenized_ds[\"train\"],\n    eval_dataset=tokenized_ds[\"validation\"],\n    tokenizer=teacher_tokenizer,\n    compute_metrics=compute_metrics,\n)\n\ntrainer_teacher.train()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T16:43:42.136750Z","iopub.execute_input":"2025-10-23T16:43:42.137514Z","iopub.status.idle":"2025-10-23T17:01:54.503118Z","shell.execute_reply.started":"2025-10-23T16:43:42.137487Z","shell.execute_reply":"2025-10-23T17:01:54.502211Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c558714219e46a3a725f6cf19ac02b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63a80f6478f44770ad41a631fb6b74e0"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_37/2311260714.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer_teacher = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1480' max='1480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1480/1480 18:04, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>0.423243</td>\n      <td>0.802710</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.487700</td>\n      <td>0.394028</td>\n      <td>0.846740</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.487700</td>\n      <td>0.318604</td>\n      <td>0.854361</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.335300</td>\n      <td>0.330422</td>\n      <td>0.861981</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.335300</td>\n      <td>0.317992</td>\n      <td>0.865368</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=1480, training_loss=0.3699384843980944, metrics={'train_runtime': 1086.1938, 'train_samples_per_second': 43.478, 'train_steps_per_second': 1.363, 'total_flos': 3106354897344000.0, 'train_loss': 0.3699384843980944, 'epoch': 5.0})"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"teacher_results = trainer_teacher.evaluate(tokenized_ds[\"test\"])\nprint(\"ğŸ“Š Teacher Accuracy:\", teacher_results)\n\nteacher_path = \"./finetuned_xlmr_youtube\"\ntrainer_teacher.save_model(teacher_path)\nteacher_tokenizer.save_pretrained(teacher_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T17:02:52.540674Z","iopub.execute_input":"2025-10-23T17:02:52.541006Z","iopub.status.idle":"2025-10-23T17:03:03.508863Z","shell.execute_reply.started":"2025-10-23T17:02:52.540982Z","shell.execute_reply":"2025-10-23T17:03:03.507996Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='37' max='37' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [37/37 00:07]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"ğŸ“Š Teacher Accuracy: {'eval_loss': 0.305649995803833, 'eval_accuracy': 0.8831498729889924, 'eval_runtime': 8.2125, 'eval_samples_per_second': 143.806, 'eval_steps_per_second': 4.505, 'epoch': 5.0}\n","output_type":"stream"},{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"('./finetuned_xlmr_youtube/tokenizer_config.json',\n './finetuned_xlmr_youtube/special_tokens_map.json',\n './finetuned_xlmr_youtube/sentencepiece.bpe.model',\n './finetuned_xlmr_youtube/added_tokens.json',\n './finetuned_xlmr_youtube/tokenizer.json')"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import shutil\nshutil.make_archive(\"finetuned_xlmr_youtube\", 'zip', \"./finetuned_xlmr_youtube\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-23T17:07:49.821485Z","iopub.execute_input":"2025-10-23T17:07:49.821782Z","iopub.status.idle":"2025-10-23T17:10:21.778077Z","shell.execute_reply.started":"2025-10-23T17:07:49.821761Z","shell.execute_reply":"2025-10-23T17:10:21.777346Z"}},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/finetuned_xlmr_youtube.zip'"},"metadata":{}}],"execution_count":9}]}