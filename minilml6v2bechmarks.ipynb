{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceId":13581507,"sourceType":"datasetVersion","datasetId":8628620},{"sourceId":13581607,"sourceType":"datasetVersion","datasetId":8628682}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir transformers==4.45.2 indic-transliteration -q\n!pip install --no-cache-dir scikit-learn pandas tqdm matplotlib sentencepiece -q\n\nimport os, json, random, numpy as np, torch\nfrom pathlib import Path\n\n# ---- Repro / device / dirs\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nWORK_DIR = Path(\"/kaggle/working\"); WORK_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---- Config\nTEACHER_MODEL_ID = \"csebuetnlp/banglabert\"\nSTUDENT_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\nMAX_LEN = 128\nBATCH_SIZE = 16\n\n# Teacher FT\nEPOCHS_TEACHER = 3\nLR_TEACHER = 2e-5\nWARMUP_RATIO_T = 0.1\nWEIGHT_DECAY_T = 0.01\n\n# KD\nEPOCHS_STUDENT = 5\nLR_STUDENT = 3e-5\nWARMUP_RATIO_S = 0.1\nWEIGHT_DECAY_S = 0.01\nPATIENCE = 2\n\nKD_T = 3.0\nKD_ALPHA = 0.5\nGAMMA_HIDDEN = 1.0\n\nprint(\"‚úÖ Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:44:27.816877Z","iopub.execute_input":"2025-12-10T14:44:27.817524Z","iopub.status.idle":"2025-12-10T14:44:54.563692Z","shell.execute_reply.started":"2025-12-10T14:44:27.817495Z","shell.execute_reply":"2025-12-10T14:44:54.563006Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m360.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m342.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h‚úÖ Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\n\nDATA_DIR = Path(\"/kaggle/input/dataaaaaa\")\nPOS_FILE = DATA_DIR / \"all_positive_8500.txt\"\nNEG_FILE = DATA_DIR / \"all_negative_3307.txt\"\nassert POS_FILE.exists() and NEG_FILE.exists(), f\"Missing data: {POS_FILE}, {NEG_FILE}\"\n\ndef read_txt(p: Path):\n    with open(p, encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos, neg = read_txt(POS_FILE), read_txt(NEG_FILE)\ndf = pd.DataFrame({\"text\": pos + neg, \"label\": [1]*len(pos) + [0]*len(neg)}).sample(frac=1, random_state=SEED)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\nval_df,   test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)\nprint(f\"Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:44:54.564889Z","iopub.execute_input":"2025-12-10T14:44:54.565208Z","iopub.status.idle":"2025-12-10T14:44:55.598396Z","shell.execute_reply.started":"2025-12-10T14:44:54.565187Z","shell.execute_reply":"2025-12-10T14:44:55.597743Z"}},"outputs":[{"name":"stdout","text":"Train=9445 | Val=1181 | Test=1181\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# train teacher","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm.auto import tqdm\n\nclass TxtClsDataset(Dataset):\n    def __init__(self, df, tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.tok, self.max_len = tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\",\n                       max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\nteacher_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\nteacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=2).to(DEVICE)\n\ntr_loader = DataLoader(TxtClsDataset(train_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nva_loader = DataLoader(TxtClsDataset(val_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\nte_loader = DataLoader(TxtClsDataset(test_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\nopt = AdamW(teacher.parameters(), lr=LR_TEACHER, weight_decay=WEIGHT_DECAY_T)\nsteps = len(tr_loader) * EPOCHS_TEACHER\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO_T*steps), steps)\ncriterion = torch.nn.CrossEntropyLoss()\n\nbest_f1 = -1\nfor ep in range(1, EPOCHS_TEACHER+1):\n    teacher.train(); total = 0\n    for b in tqdm(tr_loader, desc=f\"Teacher Epoch {ep}/{EPOCHS_TEACHER}\"):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward()\n        opt.step(); sch.step(); opt.zero_grad()\n        total += loss.item()\n\n    teacher.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in va_loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = teacher(**b)\n            preds += out.logits.argmax(-1).cpu().tolist()\n            gold  += b[\"labels\"].cpu().tolist()\n    acc = accuracy_score(gold, preds)\n    f1m = f1_score(gold, preds, average=\"macro\")\n    print(f\"Val: Acc={acc:.4f} | F1_macro={f1m:.4f}\")\n    if f1m > best_f1:\n        best_f1 = f1m\n        save_dir = WORK_DIR / \"finetuned_banglabert\"\n        save_dir.mkdir(parents=True, exist_ok=True)\n        teacher.save_pretrained(save_dir)\n        teacher_tok.save_pretrained(save_dir)\n        print(\"üíæ Saved best BanglaBERT teacher.\")\n\n# quick test\nteacher.eval(); preds, gold = [], []\nwith torch.no_grad():\n    for b in te_loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        preds += out.logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\nprint(\"‚úÖ BanglaBERT Teacher [Test]: Acc={:.4f} | F1_macro={:.4f}\".format(\n    accuracy_score(gold, preds), f1_score(gold, preds, average=\"macro\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:44:55.599090Z","iopub.execute_input":"2025-12-10T14:44:55.599456Z","iopub.status.idle":"2025-12-10T14:56:14.139667Z","shell.execute_reply.started":"2025-12-10T14:44:55.599437Z","shell.execute_reply":"2025-12-10T14:56:14.138964Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27bf61ff1a6e4d43819987c01edf29cc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee7bdf9db1164362a68fea3e698e9f62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20fb2966f8754f48a741ee270ca053e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32b535e1c1eb453ea5715adea0ed424e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f69ef2c9313f416f8c055b6d0133b1b8"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 1/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"531ea505012a46d68266bc25e91d3c57"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9577 | F1_macro=0.9489\nüíæ Saved best BanglaBERT teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 2/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"471ad6a2f4ea4ae4ab01e8fc78285309"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9670 | F1_macro=0.9586\nüíæ Saved best BanglaBERT teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 3/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44bc26744ba841eaa3f6a17adbf70930"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9678 | F1_macro=0.9601\nüíæ Saved best BanglaBERT teacher.\n‚úÖ BanglaBERT Teacher [Test]: Acc=0.9687 | F1_macro=0.9611\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# KD Data (Transliteration, 2 tokenizers)","metadata":{}},{"cell_type":"code","source":"from indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\ndef transliterate_bn_text(txt: str) -> str:\n    try:\n        return transliterate(txt, sanscript.BENGALI, sanscript.ITRANS)\n    except Exception:\n        return txt\n\nfrom transformers import AutoTokenizer\nstudent_tok = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\nclass KDDataset(Dataset):\n    def __init__(self, df, t_tok, s_tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.ttok, self.stok, self.max_len = t_tok, s_tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        txt_bn = self.texts[i]; txt_en = transliterate_bn_text(txt_bn)\n        t = self.ttok(txt_bn, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        s = self.stok(txt_en, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"t_input_ids\": t[\"input_ids\"].squeeze(0),\n            \"t_attention_mask\": t[\"attention_mask\"].squeeze(0),\n            \"s_input_ids\": s[\"input_ids\"].squeeze(0),\n            \"s_attention_mask\": s[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\ndef pad_collate(batch, t_pad, s_pad):\n    out = {}\n    for k in batch[0]:\n        if k == \"labels\": out[k] = torch.stack([b[k] for b in batch])\n        elif k.startswith(\"t_\"):\n            padv = 0 if \"attention\" in k else t_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n        elif k.startswith(\"s_\"):\n            padv = 0 if \"attention\" in k else s_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n    return out\n\ntrain_loader = DataLoader(KDDataset(train_df, teacher_tok, student_tok, MAX_LEN),\n                          batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nval_loader = DataLoader(KDDataset(val_df, teacher_tok, student_tok, MAX_LEN),\n                        batch_size=BATCH_SIZE,\n                        collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\ntest_loader = DataLoader(KDDataset(test_df, teacher_tok, student_tok, MAX_LEN),\n                         batch_size=BATCH_SIZE,\n                         collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nprint(\"‚úÖ KD dataloaders ready (BanglaBERT‚ÜíMiniLM).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:56:14.141228Z","iopub.execute_input":"2025-12-10T14:56:14.141628Z","iopub.status.idle":"2025-12-10T14:56:14.965413Z","shell.execute_reply.started":"2025-12-10T14:56:14.141608Z","shell.execute_reply":"2025-12-10T14:56:14.964662Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6dd74387ea544abda47a2a3c5a277aa8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2410f29a4ad44939ade16914c2759a00"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a219c030a2004d088ddca6996c214e98"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6bb0c4bb6bbc45ea857f35c5993ef91f"}},"metadata":{}},{"name":"stdout","text":"‚úÖ KD dataloaders ready (BanglaBERT‚ÜíMiniLM).\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Student (MiniLM-L6-v2) head (logits + hidden)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel\n\nclass StudentClassifier(nn.Module):\n    def __init__(self, base_model_id, num_labels=2, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_id)\n        s_H = self.encoder.config.hidden_size            # MiniLM hidden size (often 384)\n        self.s_hidden = s_H\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(s_H, num_labels)\n    def forward(self, input_ids=None, attention_mask=None, **_):\n        out = self.encoder(input_ids=input_ids,\n                           attention_mask=attention_mask,\n                           output_hidden_states=True,\n                           return_dict=True)\n        cls = out.last_hidden_state[:, 0, :]\n        logits = self.fc(self.dropout(cls))\n        return {\"logits\": logits, \"hidden_states\": out.hidden_states}\n\nstudent = StudentClassifier(STUDENT_MODEL_ID).to(DEVICE)\nprint(\"‚úÖ Student initialized. Hidden size =\", student.s_hidden)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:56:14.966219Z","iopub.execute_input":"2025-12-10T14:56:14.966564Z","iopub.status.idle":"2025-12-10T14:56:16.055755Z","shell.execute_reply.started":"2025-12-10T14:56:14.966536Z","shell.execute_reply":"2025-12-10T14:56:16.055130Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d93a69c1c7284e099eef1793685928ae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"43db77cfff9948bcb9c68b2ffd299881"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Student initialized. Hidden size = 384\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# KD Projection + Loss (CE + KL + HiddenProj)","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# discover sizes\nt_hidden = teacher.config.hidden_size    # XLM-R = 768\ns_hidden = student.s_hidden              # MiniLM-L6-v2 = 384\n\nclass KDProjectionHead(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.bridge = nn.Sequential(\n            nn.Linear(in_dim, out_dim),\n            nn.GELU(),\n            nn.LayerNorm(out_dim)\n        )\n    def forward(self, x):\n        return self.bridge(x)\n\nproj_head = KDProjectionHead(s_hidden, t_hidden).to(DEVICE)\n\nclass KDLossProj(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5, gamma_h=1.0):\n        super().__init__()\n        self.T, self.alpha, self.gamma_h = T, alpha, gamma_h\n        self.ce  = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n        self.mse = nn.MSELoss()\n\n    @staticmethod\n    def map_layers(n_s, n_t):\n        # skip embeddings index 0; map hidden layers 1..n\n        s_idx = list(range(1, n_s))  # student hidden_states length includes embeddings at 0\n        t_idx = torch.linspace(1, n_t-1, steps=len(s_idx)).round().long().tolist()\n        return list(zip(s_idx, t_idx))\n\n    def forward(self, s_pack, t_pack, labels):\n        # logits: CE + KL\n        logits_s, logits_t = s_pack[\"logits\"], t_pack[\"logits\"]\n        hard = self.ce(logits_s, labels)\n        soft = self.kld(F.log_softmax(logits_s/self.T, dim=-1),\n                        F.softmax(logits_t/self.T,  dim=-1)) * (self.T**2)\n        loss = (1 - self.alpha)*hard + self.alpha*soft\n\n        # hidden: MSE(proj(student_h), teacher_h) with proportional mapping\n        hs, ht = s_pack.get(\"hidden_states\", []), t_pack.get(\"hidden_states\", [])\n        if hs and ht:\n            pairs = self.map_layers(len(hs), len(ht))\n            h_losses = []\n            for i_s, i_t in pairs:\n                s_h = proj_head(hs[i_s])           # [B, L, t_hidden]\n                t_h = ht[i_t]\n                L = min(s_h.size(1), t_h.size(1))\n                h_losses.append(self.mse(s_h[:, :L, :], t_h[:, :L, :]))\n            if h_losses:\n                loss = loss + GAMMA_HIDDEN * torch.stack(h_losses).mean()\n\n        return loss\n\ncriterion = KDLossProj(T=KD_T, alpha=KD_ALPHA, gamma_h=GAMMA_HIDDEN)\nprint(\"‚úÖ KD loss & projection ready. (student‚Üíteacher dims: {}‚Üí{})\".format(s_hidden, t_hidden))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:56:16.056519Z","iopub.execute_input":"2025-12-10T14:56:16.056799Z","iopub.status.idle":"2025-12-10T14:56:16.070611Z","shell.execute_reply.started":"2025-12-10T14:56:16.056772Z","shell.execute_reply":"2025-12-10T14:56:16.070047Z"}},"outputs":[{"name":"stdout","text":"‚úÖ KD loss & projection ready. (student‚Üíteacher dims: 384‚Üí768)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# KD Training (teacher frozen)","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# freeze teacher for KD\nteacher.eval()\nfor p in teacher.parameters(): p.requires_grad = False\n\nopt = AdamW(list(student.parameters()) + list(proj_head.parameters()),\n            lr=LR_STUDENT, weight_decay=WEIGHT_DECAY_S)\nnum_steps = EPOCHS_STUDENT * len(train_loader)\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO_S * num_steps), num_steps)\n\ndef metrics(preds, gold):\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\"),\n    }\n\n@torch.no_grad()\ndef eval_student(loader):\n    student.eval(); proj_head.eval()\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = student(input_ids=b[\"s_input_ids\"], attention_mask=b[\"s_attention_mask\"])\n        preds += out[\"logits\"].argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return metrics(np.array(preds), np.array(gold))\n\nbest_f1, wait = -1.0, 0\n\nfor ep in range(1, EPOCHS_STUDENT+1):\n    student.train(); proj_head.train()\n    run = 0.0\n\n    for b in tqdm(train_loader, desc=f\"[KD Epoch {ep}/{EPOCHS_STUDENT}]\"):\n        labels = b[\"labels\"].to(DEVICE)\n\n        s_out = student(input_ids=b[\"s_input_ids\"].to(DEVICE),\n                        attention_mask=b[\"s_attention_mask\"].to(DEVICE),\n                        )\n\n        with torch.no_grad():\n            t_raw = teacher(input_ids=b[\"t_input_ids\"].to(DEVICE),\n                            attention_mask=b[\"t_attention_mask\"].to(DEVICE),\n                            output_hidden_states=True,\n                            return_dict=True)\n            t_out = {\"logits\": t_raw.logits, \"hidden_states\": t_raw.hidden_states}\n\n        loss = criterion(s_out, t_out, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(list(student.parameters()) + list(proj_head.parameters()), 1.0)\n        opt.step(); sch.step(); opt.zero_grad()\n        run += loss.item()\n\n    val = eval_student(val_loader)\n    print(f\"[KD] loss={run/len(train_loader):.4f} | Val Acc={val['accuracy']:.4f} | \"\n          f\"F1m={val['f1_macro']:.4f} | F1w={val['f1_weighted']:.4f}\")\n\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save({\"student\": student.state_dict(), \"proj\": proj_head.state_dict()},\n                   WORK_DIR / \"student_minilm_kd_best.pt\")\n        print(\"üíæ Saved best student.\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"‚è∏Ô∏è Early stopping.\")\n            break\n\n# reload best\nckpt = torch.load(WORK_DIR / \"student_minilm_kd_best.pt\", map_location=DEVICE)\nstudent.load_state_dict(ckpt[\"student\"]); proj_head.load_state_dict(ckpt[\"proj\"])\nstudent.eval(); proj_head.eval()\nprint(\"‚úÖ KD complete & best reloaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T14:56:16.071460Z","iopub.execute_input":"2025-12-10T14:56:16.071675Z","iopub.status.idle":"2025-12-10T15:06:36.062241Z","shell.execute_reply.started":"2025-12-10T14:56:16.071659Z","shell.execute_reply":"2025-12-10T15:06:36.061563Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 1/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ad88923366b84eafa2ba448c3c800840"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=1.8668 | Val Acc=0.9170 | F1m=0.8952 | F1w=0.9162\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 2/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f8fc4f580fb403f9a8ebdba794fdb83"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=1.0859 | Val Acc=0.9136 | F1m=0.8837 | F1w=0.9096\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 3/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74ebf91b1ab84632a8039dfd2321f85a"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.9249 | Val Acc=0.9399 | F1m=0.9247 | F1w=0.9396\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 4/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ed4cd2bbb324210a7036244035515a4"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.8058 | Val Acc=0.9458 | F1m=0.9331 | F1w=0.9459\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 5/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"859ea69100e9478db54e29bf44f076de"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.7441 | Val Acc=0.9450 | F1m=0.9322 | F1w=0.9451\n‚úÖ KD complete & best reloaded.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Test Metrics + Alignment + Save","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\nfrom scipy.spatial.distance import cosine\n\n@torch.no_grad()\ndef eval_model(model, loader, mode=\"student\"):\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        inp = {\"input_ids\": b[\"t_input_ids\"], \"attention_mask\": b[\"t_attention_mask\"]} if mode==\"teacher\" else \\\n              {\"input_ids\": b[\"s_input_ids\"], \"attention_mask\": b[\"s_attention_mask\"]}\n        out = model(**inp)\n        logits = out[\"logits\"] if isinstance(out, dict) else out.logits\n        preds += logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\")\n    }\n\nprint(\"üß™ Evaluating on test‚Ä¶\")\nteacher_test = eval_model(teacher, test_loader, mode=\"teacher\")\nstudent_test = eval_model(student, test_loader, mode=\"student\")\nprint(\"[Teacher][Test]:\", teacher_test)\nprint(\"[Student][Test]:\", student_test)\n\n@torch.no_grad()\ndef alignment_metrics(teacher, student, loader):\n    cos_list, corr_list, agree = [], [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        t = teacher(b[\"t_input_ids\"], b[\"t_attention_mask\"])\n        s = student(b[\"s_input_ids\"], b[\"s_attention_mask\"])\n        t_logits = t.logits.detach().cpu().numpy()\n        s_logits = s[\"logits\"].detach().cpu().numpy()\n        t_probs  = softmax(t_logits, axis=-1)\n        s_probs  = softmax(s_logits, axis=-1)\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            cos_list.append(1 - cosine(tl, sl))\n            corr_list.append(np.corrcoef(tp, sp)[0, 1])\n            agree.append(np.argmax(tp) == np.argmax(sp))\n    return {\n        \"logit_cosine\": float(np.nanmean(cos_list)),\n        \"prob_corr\": float(np.nanmean(corr_list)),\n        \"pred_alignment\": float(np.mean(agree))\n    }\n\nalign = alignment_metrics(teacher, student, test_loader)\nprint(f\"\"\"\nüß© Alignment (Test)\n  ‚Ä¢ Logit cosine : {align['logit_cosine']:.4f}\n  ‚Ä¢ Prob corr    : {align['prob_corr']:.4f}\n  ‚Ä¢ Agreement    : {align['pred_alignment']:.4f}\n\"\"\")\n\n# ---- Save artifacts\nSAVE_DIR = WORK_DIR / \"student_minilm_translit_kd_proj\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\ntorch.save(student.state_dict(), SAVE_DIR / \"pytorch_model.bin\")\nfrom transformers import AutoTokenizer\n# student tokenizer saving\nAutoTokenizer.from_pretrained(STUDENT_MODEL_ID).save_pretrained(SAVE_DIR)\n\nmeta = {\n    \"teacher_model\": TEACHER_MODEL_ID,\n    \"student_model\": STUDENT_MODEL_ID,\n    \"kd_temperature\": KD_T,\n    \"alpha\": KD_ALPHA,\n    \"gamma_hidden\": GAMMA_HIDDEN,\n    \"max_len\": MAX_LEN,\n    \"lr_student\": LR_STUDENT,\n    \"epochs_student\": EPOCHS_STUDENT\n}\njson.dump(meta, open(SAVE_DIR / \"student_config.json\", \"w\"), indent=2, ensure_ascii=False)\n\ndef to_py(o):\n    if isinstance(o, dict): return {k: to_py(v) for k,v in o.items()}\n    if hasattr(o, \"item\"): return o.item()\n    return o\n\njson.dump({\"teacher_test\": to_py(teacher_test),\n           \"student_test\": to_py(student_test),\n           \"alignment\": to_py(align)},\n          open(WORK_DIR / \"metrics_minilm_kd_proj.json\", \"w\"), indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Saved student + metrics to:\", SAVE_DIR, \"and\", WORK_DIR / \"metrics_minilm_kd_proj.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:06:36.063076Z","iopub.execute_input":"2025-12-10T15:06:36.063325Z","iopub.status.idle":"2025-12-10T15:06:58.562636Z","shell.execute_reply.started":"2025-12-10T15:06:36.063304Z","shell.execute_reply":"2025-12-10T15:06:58.561840Z"}},"outputs":[{"name":"stdout","text":"üß™ Evaluating on test‚Ä¶\n[Teacher][Test]: {'accuracy': 0.9686706181202371, 'f1_macro': 0.961136147554033, 'f1_weighted': 0.9686561287537637}\n[Student][Test]: {'accuracy': 0.9458086367485182, 'f1_macro': 0.9329615280911632, 'f1_weighted': 0.9458583354280438}\n\nüß© Alignment (Test)\n  ‚Ä¢ Logit cosine : 0.9036\n  ‚Ä¢ Prob corr    : 0.9069\n  ‚Ä¢ Agreement    : 0.9534\n\n‚úÖ Saved student + metrics to: /kaggle/working/student_minilm_translit_kd_proj and /kaggle/working/metrics_minilm_kd_proj.json\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Load the Student Model ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom pathlib import Path\n\nckpt_path = Path(\"/kaggle/working/student_minilm_kd_best.pt\")\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\n\n# find actual state_dict\nfor key in [\"student\", \"model\", \"state_dict\"]:\n    if key in ckpt:\n        state_dict = ckpt[key]\n        print(f\"Found state_dict under '{key}'\")\n        break\nelse:\n    state_dict = ckpt\n    print(\"No wrapper key found; using raw checkpoint\")\n\n# show first 40 parameter names\nprint(\"\\n\".join(list(state_dict.keys())[:40]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:06:58.563417Z","iopub.execute_input":"2025-12-10T15:06:58.563743Z","iopub.status.idle":"2025-12-10T15:06:58.636098Z","shell.execute_reply.started":"2025-12-10T15:06:58.563723Z","shell.execute_reply":"2025-12-10T15:06:58.635330Z"}},"outputs":[{"name":"stdout","text":"Found state_dict under 'student'\nencoder.embeddings.word_embeddings.weight\nencoder.embeddings.position_embeddings.weight\nencoder.embeddings.token_type_embeddings.weight\nencoder.embeddings.LayerNorm.weight\nencoder.embeddings.LayerNorm.bias\nencoder.encoder.layer.0.attention.self.query.weight\nencoder.encoder.layer.0.attention.self.query.bias\nencoder.encoder.layer.0.attention.self.key.weight\nencoder.encoder.layer.0.attention.self.key.bias\nencoder.encoder.layer.0.attention.self.value.weight\nencoder.encoder.layer.0.attention.self.value.bias\nencoder.encoder.layer.0.attention.output.dense.weight\nencoder.encoder.layer.0.attention.output.dense.bias\nencoder.encoder.layer.0.attention.output.LayerNorm.weight\nencoder.encoder.layer.0.attention.output.LayerNorm.bias\nencoder.encoder.layer.0.intermediate.dense.weight\nencoder.encoder.layer.0.intermediate.dense.bias\nencoder.encoder.layer.0.output.dense.weight\nencoder.encoder.layer.0.output.dense.bias\nencoder.encoder.layer.0.output.LayerNorm.weight\nencoder.encoder.layer.0.output.LayerNorm.bias\nencoder.encoder.layer.1.attention.self.query.weight\nencoder.encoder.layer.1.attention.self.query.bias\nencoder.encoder.layer.1.attention.self.key.weight\nencoder.encoder.layer.1.attention.self.key.bias\nencoder.encoder.layer.1.attention.self.value.weight\nencoder.encoder.layer.1.attention.self.value.bias\nencoder.encoder.layer.1.attention.output.dense.weight\nencoder.encoder.layer.1.attention.output.dense.bias\nencoder.encoder.layer.1.attention.output.LayerNorm.weight\nencoder.encoder.layer.1.attention.output.LayerNorm.bias\nencoder.encoder.layer.1.intermediate.dense.weight\nencoder.encoder.layer.1.intermediate.dense.bias\nencoder.encoder.layer.1.output.dense.weight\nencoder.encoder.layer.1.output.dense.bias\nencoder.encoder.layer.1.output.LayerNorm.weight\nencoder.encoder.layer.1.output.LayerNorm.bias\nencoder.encoder.layer.2.attention.self.query.weight\nencoder.encoder.layer.2.attention.self.query.bias\nencoder.encoder.layer.2.attention.self.key.weight\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Load the Student Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom pathlib import Path\n\nWORK_DIR = Path(\"/kaggle/working\")\nDEVICE = \"cpu\"\n\nckpt_path = WORK_DIR / \"student_minilm_kd_best.pt\"\nbase_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\nstate_dict = ckpt[\"student\"] if \"student\" in ckpt else ckpt.get(\"model\", ckpt.get(\"state_dict\", ckpt))\n\ndef remap_key(k: str) -> str | None:\n    # drop KD projection heads entirely\n    if k.startswith(\"proj.\") or \"projection\" in k:\n        return None\n    # classifier\n    if k.startswith(\"fc.\"):\n        return k.replace(\"fc.\", \"classifier.\")\n    # core renames from your checkpoint structure ‚Üí HF structure\n    k = k.replace(\"encoder.encoder.\", \"bert.encoder.\")      # encoder blocks\n    k = k.replace(\"encoder.embeddings.\", \"bert.embeddings.\")# embeddings\n    k = k.replace(\"encoder.pooler.\", \"bert.pooler.\")        # <-- FIXED: pooler\n    # handle odd double nesting we saw earlier\n    k = k.replace(\"bert.bert.\", \"bert.encoder.\")\n    k = k.replace(\"bert.encoder.encoder.\", \"bert.encoder.\")\n    return k\n\nfixed_state = {}\nfor k, v in state_dict.items():\n    nk = remap_key(k)\n    if nk is not None:\n        fixed_state[nk] = v\n\n# build 2-class MiniLM and load\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n\nmissing, unexpected = model.load_state_dict(fixed_state, strict=False)\n\nprint(\"‚úÖ MiniLM KD (2-class) loaded with correct mapping.\")\nprint(\"Missing keys:\", missing)\nprint(\"Unexpected keys:\", unexpected)\n\nmodel.to(DEVICE).eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:06:58.638770Z","iopub.execute_input":"2025-12-10T15:06:58.639061Z","iopub.status.idle":"2025-12-10T15:06:58.988199Z","shell.execute_reply.started":"2025-12-10T15:06:58.639043Z","shell.execute_reply":"2025-12-10T15:06:58.987555Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ MiniLM KD (2-class) loaded with correct mapping.\nMissing keys: []\nUnexpected keys: []\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n      (position_embeddings): Embedding(512, 384)\n      (token_type_embeddings): Embedding(2, 384)\n      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=384, out_features=384, bias=True)\n              (key): Linear(in_features=384, out_features=384, bias=True)\n              (value): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=384, out_features=384, bias=True)\n              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=384, out_features=1536, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=1536, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=384, out_features=384, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=384, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":10},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"CLEAN_DIR = WORK_DIR / \"student_minilm_kd_clean\"\nCLEAN_DIR.mkdir(parents=True, exist_ok=True)\nmodel.save_pretrained(CLEAN_DIR)\ntokenizer.save_pretrained(CLEAN_DIR)\nprint(\"Saved to:\", CLEAN_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:06:58.988951Z","iopub.execute_input":"2025-12-10T15:06:58.989239Z","iopub.status.idle":"2025-12-10T15:06:59.123727Z","shell.execute_reply.started":"2025-12-10T15:06:58.989213Z","shell.execute_reply":"2025-12-10T15:06:59.123139Z"}},"outputs":[{"name":"stdout","text":"Saved to: /kaggle/working/student_minilm_kd_clean\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# prune ","metadata":{}},{"cell_type":"code","source":"import torch.nn.utils.prune as prune\n\namount = 0.30  # 30% unstructured weight pruning\nfor m in model.modules():\n    if isinstance(m, torch.nn.Linear):\n        prune.l1_unstructured(m, name=\"weight\", amount=amount)\n\n# Remove reparam so weights are real tensors\nfor m in model.modules():\n    if isinstance(m, torch.nn.Linear) and hasattr(m, \"weight_orig\"):\n        prune.remove(m, \"weight\")\n\nPRUNED_DIR = WORK_DIR / \"student_minilm_pruned\"\nPRUNED_DIR.mkdir(exist_ok=True, parents=True)\nmodel.save_pretrained(PRUNED_DIR)\ntokenizer.save_pretrained(PRUNED_DIR)\nprint(\"Pruned model saved to:\", PRUNED_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:06:59.124436Z","iopub.execute_input":"2025-12-10T15:06:59.124693Z","iopub.status.idle":"2025-12-10T15:06:59.762164Z","shell.execute_reply.started":"2025-12-10T15:06:59.124672Z","shell.execute_reply":"2025-12-10T15:06:59.761484Z"}},"outputs":[{"name":"stdout","text":"Pruned model saved to: /kaggle/working/student_minilm_pruned\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nteacher_path = \"/kaggle/working/finetuned_banglabert\"  # your real teacher ckpt path\nteacher = AutoModelForSequenceClassification.from_pretrained(teacher_path)\nteacher.to(DEVICE).eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:06:59.762870Z","iopub.execute_input":"2025-12-10T15:06:59.763129Z","iopub.status.idle":"2025-12-10T15:06:59.821497Z","shell.execute_reply.started":"2025-12-10T15:06:59.763100Z","shell.execute_reply":"2025-12-10T15:06:59.820776Z"}},"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"ElectraForSequenceClassification(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): ElectraClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): GELUActivation()\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"batch = next(iter(train_loader))\nprint(\"Batch keys:\", batch.keys())\nfor k, v in batch.items():\n    print(f\"{k}: shape {v.shape if torch.is_tensor(v) else type(v)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:06:59.822360Z","iopub.execute_input":"2025-12-10T15:06:59.822705Z","iopub.status.idle":"2025-12-10T15:06:59.839987Z","shell.execute_reply.started":"2025-12-10T15:06:59.822681Z","shell.execute_reply":"2025-12-10T15:06:59.839441Z"}},"outputs":[{"name":"stdout","text":"Batch keys: dict_keys(['t_input_ids', 't_attention_mask', 's_input_ids', 's_attention_mask', 'labels'])\nt_input_ids: shape torch.Size([16, 128])\nt_attention_mask: shape torch.Size([16, 128])\ns_input_ids: shape torch.Size([16, 128])\ns_attention_mask: shape torch.Size([16, 128])\nlabels: shape torch.Size([16])\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# finetune for Accuracy Recovery","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# ====== DEVICE & CONFIG ======\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nEPOCHS = 5\nLR = 2e-5\nALPHA = 0.7        # CE vs KD (logits)\nBETA = 0.3         # hidden-state KD weight\nTEMPERATURE = 3.0\n\nprint(\"‚úÖ Using device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE.type==\"cuda\" else \"CPU\")\n\n# ====== SETUP ======\nteacher.to(DEVICE).eval()\nmodel.to(DEVICE).train()\n\n# Projection layer: ELECTRA(768) ‚Üí MiniLM(384)\nproj = nn.Linear(768, 384).to(DEVICE)\nmse = nn.MSELoss()\n\nopt = torch.optim.AdamW(\n    list(model.parameters()) + list(proj.parameters()),\n    lr=LR\n)\nscaler = torch.cuda.amp.GradScaler()   # ‚úÖ Mixed precision scaler\n\n# ====== TRAINING LOOP ======\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    proj.train()\n    tot = 0\n\n    for batch in tqdm(train_loader, desc=f\"KD Recovery Epoch {epoch}/{EPOCHS}\"):\n        # Move batch to GPU\n        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n\n        with torch.cuda.amp.autocast():  # ‚úÖ FP16 mixed precision\n            # --- Forward (Student) ---\n            out_s = model(\n                input_ids=batch[\"s_input_ids\"],\n                attention_mask=batch[\"s_attention_mask\"],\n                output_hidden_states=True\n            )\n            logits_s = out_s.logits\n            s_hidden = out_s.hidden_states[-1]  # [B, T, 384]\n\n            # --- Forward (Teacher) ---\n            with torch.no_grad():\n                out_t = teacher(\n                    input_ids=batch[\"t_input_ids\"],\n                    attention_mask=batch[\"t_attention_mask\"],\n                    output_hidden_states=True\n                )\n                logits_t = out_t.logits\n                t_hidden = out_t.hidden_states[-1]  # [B, T, 768]\n\n            # --- Compute Losses ---\n            feat_loss = mse(s_hidden, proj(t_hidden))\n            kd_loss = F.kl_div(\n                F.log_softmax(logits_s / TEMPERATURE, dim=-1),\n                F.softmax(logits_t / TEMPERATURE, dim=-1),\n                reduction=\"batchmean\"\n            ) * (TEMPERATURE ** 2)\n            ce_loss = F.cross_entropy(logits_s, batch[\"labels\"])\n            loss = ALPHA * ce_loss + (1 - ALPHA) * kd_loss + BETA * feat_loss\n\n        opt.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n\n        tot += loss.item()\n\n    print(f\"Epoch {epoch}: avg_loss = {tot / len(train_loader):.4f}\")\n    torch.cuda.empty_cache()  # ‚úÖ free VRAM between epochs\n\n# ====== SAVE MODEL ======\nREC_DIR = WORK_DIR / \"student_minilm_proj_kd_recovered_gpu\"\nREC_DIR.mkdir(parents=True, exist_ok=True)\n\nmodel.cpu().save_pretrained(REC_DIR)\ntokenizer.save_pretrained(REC_DIR)\ntorch.save(proj.state_dict(), REC_DIR / \"proj_layer.pt\")\n\nprint(\"‚úÖ Projection-KD MiniLM student saved to:\", REC_DIR)\n\n# ====== EVALUATION ======\nmodel.eval().to(DEVICE)\nproj.eval().to(DEVICE)\ny_true, y_pred = [], []\n\nfor batch in tqdm(test_loader, desc=\"Evaluating\"):\n    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        logits = model(\n            input_ids=batch[\"s_input_ids\"],\n            attention_mask=batch[\"s_attention_mask\"]\n        ).logits\n\n    preds = logits.argmax(-1).cpu().numpy()\n    y_pred.extend(preds)\n    y_true.extend(batch[\"labels\"].cpu().numpy())\n\nacc = accuracy_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred, average=\"macro\")\nprint(f\"‚úÖ Test Accuracy: {acc:.4f} | Macro F1: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:06:59.840774Z","iopub.execute_input":"2025-12-10T15:06:59.840929Z","iopub.status.idle":"2025-12-10T15:10:34.744324Z","shell.execute_reply.started":"2025-12-10T15:06:59.840917Z","shell.execute_reply":"2025-12-10T15:10:34.743634Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: cuda | GPU: Tesla T4\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_47/2362264538.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()   # ‚úÖ Mixed precision scaler\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 1/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d8fbdd8b7394be1a23ccd43291e14a1"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_47/2362264538.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():  # ‚úÖ FP16 mixed precision\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: avg_loss = 0.2656\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 2/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d6f2be9c7e124933bfb64cbe88fbd266"}},"metadata":{}},{"name":"stdout","text":"Epoch 2: avg_loss = 0.1508\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 3/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c6fb194fc8142c296bf999a4bbe6ef9"}},"metadata":{}},{"name":"stdout","text":"Epoch 3: avg_loss = 0.1125\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 4/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bccd1cd64b5442558ae5f9e12cbb807f"}},"metadata":{}},{"name":"stdout","text":"Epoch 4: avg_loss = 0.0993\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 5/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91548a51734e40a5adb3c347a12f3b03"}},"metadata":{}},{"name":"stdout","text":"Epoch 5: avg_loss = 0.0864\n‚úÖ Projection-KD MiniLM student saved to: /kaggle/working/student_minilm_proj_kd_recovered_gpu\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48edbebaa9e342eeba1bd993e7c56fc6"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_47/2362264538.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Test Accuracy: 0.9500 | Macro F1: 0.9383\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# Load Fine-Tuned Recovered Student","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch, os\nfrom pathlib import Path\n\nWORK_DIR = Path(\"/kaggle/working\")\nDEPLOY_DIR = WORK_DIR / \"student_minilm_proj_kd_recovered_gpu\"\n\n# Load fine-tuned MiniLM (post-KD recovery)\nmodel = AutoModelForSequenceClassification.from_pretrained(DEPLOY_DIR)\ntokenizer = AutoTokenizer.from_pretrained(DEPLOY_DIR)\n\nprint(\"‚úÖ Loaded fine-tuned MiniLM student for deployment\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:10:34.745169Z","iopub.execute_input":"2025-12-10T15:10:34.745449Z","iopub.status.idle":"2025-12-10T15:10:34.809742Z","shell.execute_reply.started":"2025-12-10T15:10:34.745428Z","shell.execute_reply":"2025-12-10T15:10:34.808983Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded fine-tuned MiniLM student for deployment\nDevice: cuda\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Apply Dynamic Quantization (int8)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.quantization\n\nmodel.eval().cpu()  # move to CPU for quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {nn.Linear}, dtype=torch.qint8\n)\n\ntorch.save(quantized_model.state_dict(), DEPLOY_DIR / \"minilm_quantized.pt\")\nprint(\"‚úÖ Quantized (int8) model ready ‚Äî much smaller & faster on CPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:10:34.810785Z","iopub.execute_input":"2025-12-10T15:10:34.811566Z","iopub.status.idle":"2025-12-10T15:10:35.098815Z","shell.execute_reply.started":"2025-12-10T15:10:34.811547Z","shell.execute_reply":"2025-12-10T15:10:35.098172Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Quantized (int8) model ready ‚Äî much smaller & faster on CPU\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Export to ONNX (for edge / mobile inference)","metadata":{}},{"cell_type":"code","source":"import torch\n\n# use the fine-tuned (FP32) model, not quantized one\nexport_model = model.eval().cpu()\n\n# Example input\nsample = tokenizer(\n    \"‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ!\", \n    padding=\"max_length\", \n    truncation=True, \n    max_length=128, \n    return_tensors=\"pt\"\n)\n\nonnx_path = DEPLOY_DIR / \"student_minilm_fp32.onnx\"\n\ntorch.onnx.export(\n    export_model,\n    (sample[\"input_ids\"], sample[\"attention_mask\"]),\n    onnx_path,\n    input_names=[\"input_ids\", \"attention_mask\"],\n    output_names=[\"logits\"],\n    dynamic_axes={\"input_ids\": {0: \"batch\"}, \"attention_mask\": {0: \"batch\"}},\n    opset_version=17,\n    do_constant_folding=True\n)\n\nprint(\"‚úÖ Exported MiniLM (FP32) ONNX model:\", onnx_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:10:35.099577Z","iopub.execute_input":"2025-12-10T15:10:35.099821Z","iopub.status.idle":"2025-12-10T15:10:36.707269Z","shell.execute_reply.started":"2025-12-10T15:10:35.099794Z","shell.execute_reply":"2025-12-10T15:10:36.706623Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Exported MiniLM (FP32) ONNX model: /kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_fp32.onnx\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Load & Run ONNX Model with ONNXRuntime","metadata":{}},{"cell_type":"code","source":"!pip install -q onnxruntime\n\nimport onnxruntime as ort\nimport numpy as np\n\n# Load ONNX model\nonnx_sess = ort.InferenceSession(str(onnx_path), providers=[\"CPUExecutionProvider\"])\n\n# Prepare input for ONNX\ninputs_onnx = {\n    \"input_ids\": sample[\"input_ids\"].numpy(),\n    \"attention_mask\": sample[\"attention_mask\"].numpy()\n}\n\n# Run inference\noutputs = onnx_sess.run([\"logits\"], inputs_onnx)\npred = np.argmax(outputs[0], axis=-1)\nprint(\"‚úÖ ONNX inference output logits:\", outputs[0])\nprint(\"‚úÖ Predicted class:\", pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:10:36.708197Z","iopub.execute_input":"2025-12-10T15:10:36.708562Z","iopub.status.idle":"2025-12-10T15:10:42.005930Z","shell.execute_reply.started":"2025-12-10T15:10:36.708545Z","shell.execute_reply":"2025-12-10T15:10:42.005055Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h‚úÖ ONNX inference output logits: [[-2.5146873  2.3884728]]\n‚úÖ Predicted class: [1]\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from onnxruntime.quantization import quantize_dynamic, QuantType\nfrom pathlib import Path\n\nfp32_path = DEPLOY_DIR / \"student_minilm_fp32.onnx\"\nint8_path = DEPLOY_DIR / \"student_minilm_int8.onnx\"\n\nquantize_dynamic(\n    model_input=str(fp32_path),\n    model_output=str(int8_path),\n    weight_type=QuantType.QInt8\n)\n\nprint(\"‚úÖ Quantized ONNX model saved at:\", int8_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:10:42.007252Z","iopub.execute_input":"2025-12-10T15:10:42.007549Z","iopub.status.idle":"2025-12-10T15:10:43.485854Z","shell.execute_reply.started":"2025-12-10T15:10:42.007518Z","shell.execute_reply":"2025-12-10T15:10:43.485134Z"}},"outputs":[{"name":"stderr","text":"WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Quantized ONNX model saved at: /kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_int8.onnx\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Compare Model File Sizes","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n# Adjust to your actual directory\nREC_DIR = Path(\"/kaggle/working/student_minilm_proj_kd_recovered_gpu\")\nDEPLOY_DIR = REC_DIR  # since all your exports are inside same folder\n\n# Common possible file names\n\nonnx_fp32_path    = DEPLOY_DIR / \"student_minilm_fp32.onnx\"  # ONNX FP32 export\nonnx_int8_path    = DEPLOY_DIR / \"student_minilm_int8.onnx\"  # Quantized ONNX\n\ndef get_size(path):\n    try:\n        size_mb = os.path.getsize(path) / (1024 * 1024)\n        return f\"{size_mb:.2f} MB\"\n    except FileNotFoundError:\n        return \"‚ùå Not found\"\n\nprint(\"üì¶ Model Size Comparison\")\n\nprint(f\"ONNX FP32:   {get_size(onnx_fp32_path)}\")\nprint(f\"ONNX INT8:   {get_size(onnx_int8_path)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:10:43.486633Z","iopub.execute_input":"2025-12-10T15:10:43.486865Z","iopub.status.idle":"2025-12-10T15:10:43.493753Z","shell.execute_reply.started":"2025-12-10T15:10:43.486837Z","shell.execute_reply":"2025-12-10T15:10:43.492974Z"}},"outputs":[{"name":"stdout","text":"üì¶ Model Size Comparison\nONNX FP32:   86.78 MB\nONNX INT8:   21.98 MB\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Benchmark ONNX FP32 vs INT8 Models","metadata":{}},{"cell_type":"code","source":"import time, onnxruntime as ort, numpy as np, pandas as pd\nfrom tqdm.auto import tqdm\n\n# ---------------------------------------------------------------------\n#  ‚úÖ CONFIG\n# ---------------------------------------------------------------------\nDEVICE = \"cpu\"\n\n# Example Bangla sentences for inference\ntexts = [\n    \"‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ!\",        \n    \"‡¶Ö‡¶≠‡¶ø‡¶®‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶õ‡¶ø‡¶≤, ‡¶™‡ßÅ‡¶∞‡ßã ‡¶®‡¶æ‡¶ü‡¶ï‡¶ü‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶â‡¶™‡¶≠‡ßã‡¶ó ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\"     \n]\n\nonnx_fp32_path = DEPLOY_DIR / \"student_minilm_fp32.onnx\"\nonnx_int8_path = DEPLOY_DIR / \"student_minilm_int8.onnx\"\n\n# ---------------------------------------------------------------------\n#  üîπ Benchmark helper\n# ---------------------------------------------------------------------\ndef benchmark_onnx(model_path, text_list, runs=20):\n    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n    inputs = tokenizer(text_list, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n    start = time.time()\n    for _ in range(runs):\n        _ = sess.run([\"logits\"], {\n            \"input_ids\": inputs[\"input_ids\"],\n            \"attention_mask\": inputs[\"attention_mask\"]\n        })\n    end = time.time()\n    return (end - start) / runs * 1000  # average latency (ms)\n\n# ---------------------------------------------------------------------\n#  üîπ Run latency benchmarks\n# ---------------------------------------------------------------------\nlat_onnx_fp32 = benchmark_onnx(onnx_fp32_path, texts)\nlat_onnx_int8 = benchmark_onnx(onnx_int8_path, texts)\n\n# ---------------------------------------------------------------------\n#  üîπ Inference results (for demonstration)\n# ---------------------------------------------------------------------\ndef predict_onnx(model_path, text):\n    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n    inputs = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n    logits = sess.run([\"logits\"], {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"]\n    })[0]\n    pred = np.argmax(logits, axis=-1).item()\n    return pred, logits\n\nfor t in texts:\n    p_fp32, l_fp32 = predict_onnx(onnx_fp32_path, t)\n    p_int8, l_int8 = predict_onnx(onnx_int8_path, t)\n    print(f\"\\nüìù Text: {t}\")\n    print(f\"ONNX FP32 ‚Üí Pred: {p_fp32}, Logits: {np.round(l_fp32, 3)}\")\n    print(f\"ONNX INT8 ‚Üí Pred: {p_int8}, Logits: {np.round(l_int8, 3)}\")\n\n# ---------------------------------------------------------------------\n#  üîπ Summary Table\n# ---------------------------------------------------------------------\ndata = {\n    \"Model\": [\"ONNX FP32\", \"ONNX INT8\"],\n    \"Size (MB)\": [86.78, 21.98],      # from your previous results\n    \"Latency (ms)\": [lat_onnx_fp32, lat_onnx_int8],\n    \"Notes\": [\n        \"Baseline full-precision export\",\n        \"Quantized deployment (fast + compact)\"\n    ]\n}\n\nsummary = pd.DataFrame(data)\nprint(\"\\nüèÅ Deployment Performance Summary:\")\ndisplay(summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:31:45.859092Z","iopub.execute_input":"2025-12-10T15:31:45.859370Z","iopub.status.idle":"2025-12-10T15:31:48.218397Z","shell.execute_reply.started":"2025-12-10T15:31:45.859349Z","shell.execute_reply":"2025-12-10T15:31:48.217463Z"}},"outputs":[{"name":"stdout","text":"\nüìù Text: ‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ!\nONNX FP32 ‚Üí Pred: 1, Logits: [[-2.515  2.388]]\nONNX INT8 ‚Üí Pred: 1, Logits: [[-1.985  1.908]]\n\nüìù Text: ‡¶Ö‡¶≠‡¶ø‡¶®‡¶Ø‡¶º‡¶ó‡ßÅ‡¶≤‡ßã ‡¶Ö‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶õ‡¶ø‡¶≤, ‡¶™‡ßÅ‡¶∞‡ßã ‡¶®‡¶æ‡¶ü‡¶ï‡¶ü‡¶ø ‡¶ñ‡ßÅ‡¶¨ ‡¶â‡¶™‡¶≠‡ßã‡¶ó ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\nONNX FP32 ‚Üí Pred: 1, Logits: [[-1.857  1.807]]\nONNX INT8 ‚Üí Pred: 1, Logits: [[-1.521  1.508]]\n\nüèÅ Deployment Performance Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       Model  Size (MB)  Latency (ms)                                  Notes\n0  ONNX FP32      86.78     38.958073         Baseline full-precision export\n1  ONNX INT8      21.98     32.221401  Quantized deployment (fast + compact)","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Size (MB)</th>\n      <th>Latency (ms)</th>\n      <th>Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ONNX FP32</td>\n      <td>86.78</td>\n      <td>38.958073</td>\n      <td>Baseline full-precision export</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ONNX INT8</td>\n      <td>21.98</td>\n      <td>32.221401</td>\n      <td>Quantized deployment (fast + compact)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":30},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# Evaluate ONNX FP32 vs INT8 Model Accuracy","metadata":{}},{"cell_type":"code","source":"import onnxruntime as ort\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm.auto import tqdm\n\n# ------------------------------\n#  Setup ONNX sessions\n# ------------------------------\nsess_fp32 = ort.InferenceSession(str(onnx_fp32_path), providers=[\"CPUExecutionProvider\"])\nsess_int8 = ort.InferenceSession(str(onnx_int8_path), providers=[\"CPUExecutionProvider\"])\n\ny_true, y_fp32, y_int8 = [], [], []\n\n# ------------------------------\n#  Loop over test set\n# ------------------------------\nfor batch in tqdm(test_loader, desc=\"Evaluating ONNX Models\"):\n    # move tensors to CPU and convert to numpy\n    input_ids = batch[\"s_input_ids\"].cpu().numpy()\n    attention_mask = batch[\"s_attention_mask\"].cpu().numpy()\n    labels = batch[\"labels\"].cpu().numpy()\n\n    y_true.extend(labels)\n\n    # FP32 inference\n    logits_fp32 = sess_fp32.run([\"logits\"], {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask\n    })[0]\n    preds_fp32 = np.argmax(logits_fp32, axis=-1)\n    y_fp32.extend(preds_fp32)\n\n    # INT8 inference\n    logits_int8 = sess_int8.run([\"logits\"], {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask\n    })[0]\n    preds_int8 = np.argmax(logits_int8, axis=-1)\n    y_int8.extend(preds_int8)\n\n# ------------------------------\n#  Compute accuracy & F1\n# ------------------------------\nacc_fp32 = accuracy_score(y_true, y_fp32)\nf1_fp32 = f1_score(y_true, y_fp32, average=\"macro\")\n\nacc_int8 = accuracy_score(y_true, y_int8)\nf1_int8 = f1_score(y_true, y_int8, average=\"macro\")\n\nprint(\"‚úÖ ONNX Model Accuracy Comparison\")\nprint(f\"ONNX FP32 ‚Üí Accuracy: {acc_fp32:.4f} | Macro F1: {f1_fp32:.4f}\")\nprint(f\"ONNX INT8 ‚Üí Accuracy: {acc_int8:.4f} | Macro F1: {f1_int8:.4f}\")\nprint(f\"Œî Accuracy: {(acc_int8 - acc_fp32)*100:.2f}% | Œî F1: {(f1_int8 - f1_fp32)*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:32:28.603628Z","iopub.execute_input":"2025-12-10T15:32:28.603908Z","iopub.status.idle":"2025-12-10T15:33:10.514387Z","shell.execute_reply.started":"2025-12-10T15:32:28.603885Z","shell.execute_reply":"2025-12-10T15:33:10.513602Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating ONNX Models:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddd3ae8abf68443cb7dc9501a5e94f73"}},"metadata":{}},{"name":"stdout","text":"‚úÖ ONNX Model Accuracy Comparison\nONNX FP32 ‚Üí Accuracy: 0.9500 | Macro F1: 0.9383\nONNX INT8 ‚Üí Accuracy: 0.9492 | Macro F1: 0.9367\nŒî Accuracy: -0.08% | Œî F1: -0.16%\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom pathlib import Path\n\n# Paths\nREC_DIR    = Path(\"/kaggle/working/student_minilm_proj_kd_recovered_gpu\")  # adjust if needed\nDEPLOY_DIR = REC_DIR\nFP32_STATIC = DEPLOY_DIR / \"student_minilm_fp32_static.onnx\"\n\nbase_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n# Load FP32 PyTorch model (the recovered KD student you saved with save_pretrained)\nmodel = AutoModelForSequenceClassification.from_pretrained(REC_DIR, num_labels=2).eval().cpu()\n\n# Create a STATIC dummy (batch_size=8, seq_len=128) for best fusion\nBATCH, SEQ = 8, 128\ndummy = tokenizer([\"‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ!\"] * BATCH,\n                  return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=SEQ)\n\n# Export ONNX with *static* shapes (no dynamic_axes)\ntorch.onnx.export(\n    model,\n    (dummy[\"input_ids\"], dummy[\"attention_mask\"]),\n    FP32_STATIC,\n    input_names=[\"input_ids\", \"attention_mask\"],\n    output_names=[\"logits\"],\n    opset_version=17,\n    dynamic_axes=None,          # << no dynamics ‚Üí better graph planning\n    do_constant_folding=True\n)\n\nprint(\"‚úÖ Static-shape ONNX exported:\", FP32_STATIC)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:11:28.154815Z","iopub.execute_input":"2025-12-10T15:11:28.155102Z","iopub.status.idle":"2025-12-10T15:11:29.573761Z","shell.execute_reply.started":"2025-12-10T15:11:28.155076Z","shell.execute_reply":"2025-12-10T15:11:29.573133Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Static-shape ONNX exported: /kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_fp32_static.onnx\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"# ‚úÖ Robust dynamic INT8 quantization for your optimized ONNX (handles dtype inference issues)\nimport onnx\nfrom onnx import shape_inference, TensorProto\nfrom onnxruntime.quantization import quantize_dynamic, QuantType\nfrom pathlib import Path\n\nFP32_STATIC     = DEPLOY_DIR / \"student_minilm_fp32_static.onnx\"      # from your static export\nOPTIMIZED_FP32  = DEPLOY_DIR / \"student_minilm_fp32_optimized.onnx\"   # from optimizer step\nSHAPED_FP32     = DEPLOY_DIR / \"student_minilm_fp32_shaped.onnx\"\nINT8_QDQ        = DEPLOY_DIR / \"student_minilm_int8_qdq.onnx\"\n\ndef has_quant_nodes(onnx_path: Path) -> bool:\n    m = onnx.load(str(onnx_path))\n    q_ops = {\"QuantizeLinear\",\"DequantizeLinear\",\"QLinearMatMul\",\"MatMulInteger\",\"DynamicQuantizeLinear\"}\n    return any(n.op_type in q_ops for n in m.graph.node)\n\ndef run_shape_inference(src: Path, dst: Path):\n    m = onnx.load(str(src))\n    m = shape_inference.infer_shapes(m)  # fill missing types/shapes\n    onnx.checker.check_model(m)\n    onnx.save(m, str(dst))\n\n# 1) Prefer the optimized model; if not present, fall back to static\nsrc_model = OPTIMIZED_FP32 if OPTIMIZED_FP32.exists() else FP32_STATIC\nprint(\"Using source ONNX:\", src_model)\n\n# 2) If already quantized, skip\nif has_quant_nodes(src_model):\n    print(\"‚ö†Ô∏è Model appears already quantized. Skipping quantization.\")\nelse:\n    # 3) Ensure shapes/types are present\n    run_shape_inference(src_model, SHAPED_FP32)\n    print(\"‚úÖ Shape inference done:\", SHAPED_FP32)\n\n    # 4) Try quantization with safe options; fall back to non-optimized graph if needed\n    def try_quantize(src, dst):\n        quantize_dynamic(\n            model_input=str(src),\n            model_output=str(dst),\n            weight_type=QuantType.QInt8,\n            op_types_to_quantize=[\"MatMul\",\"Gemm\"],  # safe set\n            extra_options={\"DefaultTensorType\": TensorProto.FLOAT},  # ‚Üê fixes your error\n        )\n\n    try:\n        try_quantize(SHAPED_FP32, INT8_QDQ)\n        print(\"‚úÖ Quantized ONNX (INT8) saved:\", INT8_QDQ)\n    except Exception as e1:\n        print(\"First attempt failed:\", e1)\n        # last resort: quantize the original static model directly after shape inference\n        run_shape_inference(FP32_STATIC, SHAPED_FP32)\n        try:\n            try_quantize(SHAPED_FP32, INT8_QDQ)\n            print(\"‚úÖ Quantized ONNX (INT8) saved (fallback path):\", INT8_QDQ)\n        except Exception as e2:\n            print(\"‚ùå Quantization failed again:\", e2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:11:29.574511Z","iopub.execute_input":"2025-12-10T15:11:29.574763Z","iopub.status.idle":"2025-12-10T15:11:31.865669Z","shell.execute_reply.started":"2025-12-10T15:11:29.574744Z","shell.execute_reply":"2025-12-10T15:11:31.865053Z"}},"outputs":[{"name":"stdout","text":"Using source ONNX: /kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_fp32_static.onnx\n","output_type":"stream"},{"name":"stderr","text":"WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Shape inference done: /kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_fp32_shaped.onnx\n‚úÖ Quantized ONNX (INT8) saved: /kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_int8_qdq.onnx\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import onnxruntime as ort\nimport numpy as np, time\nfrom tqdm.auto import tqdm\n\n# --- Session options ---\nsess_opts = ort.SessionOptions()\nsess_opts.intra_op_num_threads = 4\nsess_opts.inter_op_num_threads = 1\nsess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n\nsess = ort.InferenceSession(\n    str(INT8_QDQ),\n    sess_options=sess_opts,\n    providers=[\"CPUExecutionProvider\"]\n)\n\n# --- Prepare input ---\ntext = \"‡¶è‡¶á ‡¶®‡¶æ‡¶ü‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∏‡¶Æ‡ßü‡ßá‡¶∞ ‡¶Ö‡¶™‡¶ö‡ßü ‡¶õ‡¶ø‡¶≤, ‡¶Ö‡¶≠‡¶ø‡¶®‡ßü‡¶ì ‡¶ñ‡ßÅ‡¶¨‡¶á ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡•§\"\ntokens = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n\n# Drop token_type_ids if present\nif \"token_type_ids\" in tokens:\n    del tokens[\"token_type_ids\"]\n\n# Convert to int64 (required by ONNX)\ninputs = {\n    \"input_ids\": tokens[\"input_ids\"].astype(np.int64),\n    \"attention_mask\": tokens[\"attention_mask\"].astype(np.int64)\n}\n\n# Repeat for batch=8 (static shape)\ninputs[\"input_ids\"] = np.repeat(inputs[\"input_ids\"], 8, axis=0)\ninputs[\"attention_mask\"] = np.repeat(inputs[\"attention_mask\"], 8, axis=0)\n\n# --- Warm-up ---\nfor _ in range(5):\n    _ = sess.run([\"logits\"], inputs)\n\n# --- Benchmark ---\nruns, total = 50, 0\nfor _ in tqdm(range(runs), desc=\"Benchmarking INT8 QDQ (CPU optimized)\"):\n    start = time.time()\n    logits = sess.run([\"logits\"], inputs)[0]\n    total += time.time() - start\n\navg_ms = (total / runs) * 1000 / 8  # divide by batch size\npred = np.argmax(logits[0])\nprint(f\"\\nüß† Predicted label: {pred} | ‚ö° Avg Latency: {avg_ms:.2f} ms/sample | Throughput: {1000/avg_ms:.1f} samples/s\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:11:31.866554Z","iopub.execute_input":"2025-12-10T15:11:31.867038Z","iopub.status.idle":"2025-12-10T15:11:37.680138Z","shell.execute_reply.started":"2025-12-10T15:11:31.867009Z","shell.execute_reply":"2025-12-10T15:11:37.679570Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Benchmarking INT8 QDQ (CPU optimized):   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c03690a061b34fecaaa1e5b132acbe17"}},"metadata":{}},{"name":"stdout","text":"\nüß† Predicted label: 1 | ‚ö° Avg Latency: 12.78 ms/sample | Throughput: 78.3 samples/s\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"import onnxruntime as ort\nimport numpy as np, time, pandas as pd\nfrom tqdm.auto import tqdm\n\n# ======================================================\n# ‚öôÔ∏è CONFIG\n# ======================================================\nTEXT = \"‡¶è‡¶á ‡¶®‡¶æ‡¶ü‡¶ï‡¶ü‡¶ø ‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∏‡¶Æ‡ßü‡ßá‡¶∞ ‡¶Ö‡¶™‡¶ö‡ßü ‡¶õ‡¶ø‡¶≤, ‡¶Ö‡¶≠‡¶ø‡¶®‡ßü‡¶ì ‡¶ñ‡ßÅ‡¶¨‡¶á ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤‡•§\"\nBATCH = 8\nRUNS = 50\n\n# ------------------------------------------------------\n# üß© Tokenize once\n# ------------------------------------------------------\ntokens = tokenizer(TEXT, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\nif \"token_type_ids\" in tokens:\n    del tokens[\"token_type_ids\"]\n\n# Ensure int64 dtype and static batch\ninputs = {\n    \"input_ids\": np.repeat(tokens[\"input_ids\"].astype(np.int64), BATCH, axis=0),\n    \"attention_mask\": np.repeat(tokens[\"attention_mask\"].astype(np.int64), BATCH, axis=0)\n}\n\n# ======================================================\n# üß† Helper: benchmark any ONNX model path\n# ======================================================\ndef benchmark_onnx_model(model_path, label, runs=RUNS, batch=BATCH):\n    sess_opts = ort.SessionOptions()\n    sess_opts.intra_op_num_threads = 4\n    sess_opts.inter_op_num_threads = 1\n    sess_opts.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n\n    sess = ort.InferenceSession(str(model_path), sess_options=sess_opts, providers=[\"CPUExecutionProvider\"])\n\n    # Warm-up\n    for _ in range(5):\n        _ = sess.run([\"logits\"], inputs)\n\n    # Measure latency\n    total = 0\n    for _ in tqdm(range(runs), desc=f\"Benchmarking {label}\"):\n        start = time.time()\n        logits = sess.run([\"logits\"], inputs)[0]\n        total += time.time() - start\n\n    avg_ms = (total / runs) * 1000 / batch\n    pred = np.argmax(logits[0])\n    throughput = 1000 / avg_ms\n\n    print(f\"\\nüß† {label}: Pred {pred} | ‚ö° {avg_ms:.2f} ms/sample | üöÄ {throughput:.1f} samples/s\")\n    return {\n        \"Model Variant\": label,\n        \"Size (MB)\": round(os.path.getsize(model_path) / (1024 * 1024), 2),\n        \"Latency (ms/sample)\": round(avg_ms, 2),\n        \"Throughput (samples/s)\": round(throughput, 2),\n        \"Notes\": \"‚úì OK\"\n    }\n\n# ======================================================\n# üöÄ Run Benchmarks\n# ======================================================\nresults = []\nresults.append(benchmark_onnx_model(onnx_fp32_path, \"ONNX FP32\"))\nresults.append(benchmark_onnx_model(INT8_QDQ, \"ONNX INT8 (QDQ)\"))\n\n# ======================================================\n# üìä Display Results\n# ======================================================\nsummary = pd.DataFrame(results)\ndisplay(summary)\n\nprint(\"\\n‚úÖ Benchmark completed successfully!\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-10T15:11:37.681612Z","iopub.execute_input":"2025-12-10T15:11:37.681852Z","iopub.status.idle":"2025-12-10T15:11:50.983734Z","shell.execute_reply.started":"2025-12-10T15:11:37.681834Z","shell.execute_reply":"2025-12-10T15:11:50.982924Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Benchmarking ONNX FP32:   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"76a819333d1f42c28da82784f655801f"}},"metadata":{}},{"name":"stdout","text":"\nüß† ONNX FP32: Pred 1 | ‚ö° 15.87 ms/sample | üöÄ 63.0 samples/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Benchmarking ONNX INT8 (QDQ):   0%|          | 0/50 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3672eb60b694075a78a70da6ef98b43"}},"metadata":{}},{"name":"stdout","text":"\nüß† ONNX INT8 (QDQ): Pred 1 | ‚ö° 13.00 ms/sample | üöÄ 76.9 samples/s\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"     Model Variant  Size (MB)  Latency (ms/sample)  Throughput (samples/s)  \\\n0        ONNX FP32      86.78                15.87                   62.99   \n1  ONNX INT8 (QDQ)      56.00                13.00                   76.90   \n\n  Notes  \n0  ‚úì OK  \n1  ‚úì OK  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model Variant</th>\n      <th>Size (MB)</th>\n      <th>Latency (ms/sample)</th>\n      <th>Throughput (samples/s)</th>\n      <th>Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ONNX FP32</td>\n      <td>86.78</td>\n      <td>15.87</td>\n      <td>62.99</td>\n      <td>‚úì OK</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ONNX INT8 (QDQ)</td>\n      <td>56.00</td>\n      <td>13.00</td>\n      <td>76.90</td>\n      <td>‚úì OK</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"name":"stdout","text":"\n‚úÖ Benchmark completed successfully!\n","output_type":"stream"}],"execution_count":27}]}