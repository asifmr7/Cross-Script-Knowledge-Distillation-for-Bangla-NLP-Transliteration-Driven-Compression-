{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13124885,"sourceType":"datasetVersion","datasetId":8314285},{"sourceId":13124968,"sourceType":"datasetVersion","datasetId":8314344},{"sourceId":13139397,"sourceType":"datasetVersion","datasetId":8324407},{"sourceId":13139409,"sourceType":"datasetVersion","datasetId":8324416}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"5208868e-0e07-4b11-811b-92f2c725b13c","cell_type":"code","source":"# =========================================================\n# âœ… Cell 0 â€” Full Environment Reset & Dependency Fix\n# =========================================================\n!pip uninstall -y transformers accelerate peft bitsandbytes > /dev/null 2>&1\n\n# Install perfectly matched versions\n!pip install -U transformers==4.41.2 accelerate==0.31.0 torch datasets seqeval conllu --quiet\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:23:15.895075Z","iopub.execute_input":"2025-10-29T19:23:15.895886Z","iopub.status.idle":"2025-10-29T19:28:08.489396Z","shell.execute_reply.started":"2025-10-29T19:23:15.895850Z","shell.execute_reply":"2025-10-29T19:28:08.488462Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.4/309.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m899.8/899.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m88.0/88.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m954.8/954.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.1/193.1 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.6/63.6 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m267.5/267.5 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m288.2/288.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m39.3/39.3 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.7/124.7 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m170.4/170.4 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m506.8/506.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nfastai 2.8.4 requires torch<2.9,>=1.10, but you have torch 2.9.0 which is incompatible.\nbigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ntorchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.9.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntorchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.9.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"id":"8917ca81-164e-46da-892b-3db98cf26463","cell_type":"code","source":"# =========================================================\n# âœ… Cell 0 â€” Final Environment Fix (add PEFT)\n# =========================================================\n!pip install -U peft==0.10.0 --quiet","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:33:19.640517Z","iopub.execute_input":"2025-10-29T19:33:19.641058Z","iopub.status.idle":"2025-10-29T19:33:23.478000Z","shell.execute_reply.started":"2025-10-29T19:33:19.641028Z","shell.execute_reply":"2025-10-29T19:33:23.477031Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":2},{"id":"16517627-7b87-4011-873e-56af89c63311","cell_type":"code","source":"import transformers, accelerate, torch\nprint(\"Transformers:\", transformers.__version__)\nprint(\"Accelerate:\", accelerate.__version__)\nprint(\"Torch:\", torch.__version__)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:33:25.726345Z","iopub.execute_input":"2025-10-29T19:33:25.726958Z","iopub.status.idle":"2025-10-29T19:33:29.551731Z","shell.execute_reply.started":"2025-10-29T19:33:25.726932Z","shell.execute_reply":"2025-10-29T19:33:29.551088Z"}},"outputs":[{"name":"stdout","text":"Transformers: 4.41.2\nAccelerate: 0.31.0\nTorch: 2.9.0+cu128\n","output_type":"stream"}],"execution_count":3},{"id":"eddf1e0f-830b-4874-8bfe-312101d75ae7","cell_type":"code","source":"# =========================================================\n# âœ… Cell 2 â€” Copy UD files (EN, HI, UR from /universal-dependencies, BN from /ud-bengali)\n# =========================================================\nimport shutil\nfrom pathlib import Path\n\n# Kaggle dataset locations\nKAGGLE_INPUTS = Path(\"/kaggle/input/universaldependencies\")\nKAGGLE_BENGALI_INPUTS = Path(\"/kaggle/input/ud-bengali\")\n\nNEEDED = [\n    # English\n    \"en_ewt-ud-train.conllu\", \"en_ewt-ud-dev.conllu\", \"en_ewt-ud-test.conllu\",\n    # Hindi\n    \"hi_hdtb-ud-train.conllu\", \"hi_hdtb-ud-dev.conllu\", \"hi_hdtb-ud-test.conllu\",\n    # Urdu\n    \"ur_udtb-ud-train.conllu\", \"ur_udtb-ud-dev.conllu\", \"ur_udtb-ud-test.conllu\",\n    # Bengali (test only)\n    \"bn_bru-ud-test.conllu\"\n]\n\nBASE_DIR = Path(\"./ud_data\")\nBASE_DIR.mkdir(parents=True, exist_ok=True)\n\ndef copy_if_found(fname, src_path):\n    dest = BASE_DIR / fname\n    if dest.exists():\n        return True\n    src_file = src_path / fname\n    if src_file.exists():\n        shutil.copy2(src_file, dest)\n        print(f\"Copied: {src_file} â†’ {dest}\")\n        return True\n    return False\n\nmissing = []\nfor fname in NEEDED:\n    if \"bn_bru\" in fname:\n        ok = copy_if_found(fname, KAGGLE_BENGALI_INPUTS)\n    else:\n        ok = copy_if_found(fname, KAGGLE_INPUTS)\n    if not ok:\n        missing.append(fname)\n\nif missing:\n    print(\"âŒ Missing files:\")\n    for m in missing: print(\" -\", m)\nelse:\n    print(\"âœ… All UD files copied into:\", BASE_DIR.resolve())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:33:32.504393Z","iopub.execute_input":"2025-10-29T19:33:32.505070Z","iopub.status.idle":"2025-10-29T19:33:33.904206Z","shell.execute_reply.started":"2025-10-29T19:33:32.505045Z","shell.execute_reply":"2025-10-29T19:33:33.903158Z"}},"outputs":[{"name":"stdout","text":"Copied: /kaggle/input/universaldependencies/en_ewt-ud-train.conllu â†’ ud_data/en_ewt-ud-train.conllu\nCopied: /kaggle/input/universaldependencies/en_ewt-ud-dev.conllu â†’ ud_data/en_ewt-ud-dev.conllu\nCopied: /kaggle/input/universaldependencies/en_ewt-ud-test.conllu â†’ ud_data/en_ewt-ud-test.conllu\nCopied: /kaggle/input/universaldependencies/hi_hdtb-ud-train.conllu â†’ ud_data/hi_hdtb-ud-train.conllu\nCopied: /kaggle/input/universaldependencies/hi_hdtb-ud-dev.conllu â†’ ud_data/hi_hdtb-ud-dev.conllu\nCopied: /kaggle/input/universaldependencies/hi_hdtb-ud-test.conllu â†’ ud_data/hi_hdtb-ud-test.conllu\nCopied: /kaggle/input/universaldependencies/ur_udtb-ud-train.conllu â†’ ud_data/ur_udtb-ud-train.conllu\nCopied: /kaggle/input/universaldependencies/ur_udtb-ud-dev.conllu â†’ ud_data/ur_udtb-ud-dev.conllu\nCopied: /kaggle/input/universaldependencies/ur_udtb-ud-test.conllu â†’ ud_data/ur_udtb-ud-test.conllu\nCopied: /kaggle/input/ud-bengali/bn_bru-ud-test.conllu â†’ ud_data/bn_bru-ud-test.conllu\nâœ… All UD files copied into: /kaggle/working/ud_data\n","output_type":"stream"}],"execution_count":4},{"id":"861cd642-f87d-4117-8aac-8cac2d6dcd39","cell_type":"code","source":"# =========================================================\n# âœ… Cell 3 â€” Load and parse CoNLL-U files (UPOS tags only)\n# =========================================================\nfrom typing import List, Tuple, Dict\nfrom pathlib import Path\n\nTRAIN_LANGS = [\"en_ewt\", \"hi_hdtb\", \"ur_udtb\"]\nZERO_SHOT_TEST_LANG = \"bn_bcc\"  # internal key for BN\nBASE_DIR = Path(\"./ud_data\")\n\nUD_LOCAL = {\n    \"en_ewt\": {\"train\": \"en_ewt-ud-train.conllu\", \"dev\": \"en_ewt-ud-dev.conllu\", \"test\": \"en_ewt-ud-test.conllu\"},\n    \"hi_hdtb\": {\"train\": \"hi_hdtb-ud-train.conllu\", \"dev\": \"hi_hdtb-ud-dev.conllu\", \"test\": \"hi_hdtb-ud-test.conllu\"},\n    \"ur_udtb\": {\"train\": \"ur_udtb-ud-train.conllu\", \"dev\": \"ur_udtb-ud-dev.conllu\", \"test\": \"ur_udtb-ud-test.conllu\"},\n    \"bn_bcc\": {\"test\": \"bn_bru-ud-test.conllu\"},\n}\n\ndef read_conllu_upos(path: Path) -> List[Tuple[List[str], List[str]]]:\n    sents, words, upos = [], [], []\n    with path.open(\"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            line = line.strip()\n            if not line:\n                if words:\n                    sents.append((words, upos))\n                    words, upos = [], []\n                continue\n            if line.startswith(\"#\"):\n                continue\n            parts = line.split(\"\\t\")\n            if len(parts) != 10: continue\n            idx = parts[0]\n            if \"-\" in idx or \".\" in idx:  # skip multiword/empty\n                continue\n            words.append(parts[1])\n            upos.append(parts[3])\n    if words:\n        sents.append((words, upos))\n    return sents\n\ndef load_split(lang: str, split: str):\n    assert lang in UD_LOCAL\n    path = BASE_DIR / UD_LOCAL[lang][split]\n    return read_conllu_upos(path)\n\ndef build_upos_vocab(lang_keys: List[str]):\n    tagset = set()\n    for lk in lang_keys:\n        for sp in UD_LOCAL[lk]:\n            for _, tags in load_split(lk, sp):\n                tagset.update(tags)\n    id2tag = sorted(tagset)\n    tag2id = {t: i for i, t in enumerate(id2tag)}\n    return tag2id, id2tag\n\ntag2id, id2tag = build_upos_vocab(TRAIN_LANGS + [ZERO_SHOT_TEST_LANG])\nNUM_TAGS = len(id2tag)\nprint(\"NUM_TAGS:\", NUM_TAGS)\nprint(\"UPOS tags:\", id2tag)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:08:08.280535Z","iopub.execute_input":"2025-10-29T17:08:08.280807Z","iopub.status.idle":"2025-10-29T17:08:09.642020Z","shell.execute_reply.started":"2025-10-29T17:08:08.280785Z","shell.execute_reply":"2025-10-29T17:08:09.641282Z"}},"outputs":[{"name":"stdout","text":"NUM_TAGS: 17\nUPOS tags: ['ADJ', 'ADP', 'ADV', 'AUX', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X']\n","output_type":"stream"}],"execution_count":4},{"id":"5853de93-014d-4960-a4e1-62671469b561","cell_type":"code","source":"# =========================================================\n# âœ… Cell 4 â€” Model Setup (XLM-R)\n# =========================================================\nimport torch, random, numpy as np\nfrom transformers import (\n    AutoTokenizer, AutoModelForTokenClassification,\n    get_linear_schedule_with_warmup, DataCollatorForTokenClassification\n)\n\nSEED = 42\ndef set_seed(seed=SEED):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed()\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", device)\n\nMODEL_NAME = \"xlm-roberta-base\"\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = AutoModelForTokenClassification.from_pretrained(MODEL_NAME, num_labels=NUM_TAGS).to(device)\nprint(\"âœ… Loaded\", MODEL_NAME, \"with\", NUM_TAGS, \"labels\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:08:26.143849Z","iopub.execute_input":"2025-10-29T17:08:26.144442Z","iopub.status.idle":"2025-10-29T17:08:51.217450Z","shell.execute_reply.started":"2025-10-29T17:08:26.144414Z","shell.execute_reply":"2025-10-29T17:08:51.216671Z"}},"outputs":[{"name":"stderr","text":"2025-10-29 17:08:31.020957: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761757711.243732      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761757711.300367      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"04d5d8a72ffd49ba9b352a890a897f99"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fdcaa7352a034cae9c808af5ae8c17df"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94e184a0ee8d4ccda615b2d282d2ecf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ecc433089da41d380fc8ff6e1736ab4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f690d773cb34d3bbfff2fb29862be79"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"âœ… Loaded xlm-roberta-base with 17 labels\n","output_type":"stream"}],"execution_count":5},{"id":"d49ee14f-eb03-4d6b-a4c9-dfb68111796c","cell_type":"code","source":"# =========================================================\n# âœ… Cell 5 â€” Tokenization + Alignment\n# =========================================================\nfrom torch.utils.data import Dataset, DataLoader\n\nMAX_LEN = 256\n\ndef tokenize_and_align_labels(sentences, upos_tags, tag2id):\n    enc = tokenizer(\n        sentences, is_split_into_words=True,\n        truncation=True, padding=False, max_length=MAX_LEN\n    )\n    labels = []\n    for i, tags in enumerate(upos_tags):\n        word_ids = enc.word_ids(i)\n        label_ids, prev = [], None\n        for w_id in word_ids:\n            if w_id is None:\n                label_ids.append(-100)\n            elif w_id != prev:\n                label_ids.append(tag2id[tags[w_id]])\n            else:\n                label_ids.append(-100)\n            prev = w_id\n        labels.append(label_ids)\n    enc[\"labels\"] = labels\n    return enc\n\nclass UPOSDataset(Dataset):\n    def __init__(self, sents_tags):\n        self.sents = [w for w, _ in sents_tags]\n        self.tags = [t for _, t in sents_tags]\n        self.enc = tokenize_and_align_labels(self.sents, self.tags, tag2id)\n    def __len__(self): return len(self.sents)\n    def __getitem__(self, i): return {k: torch.tensor(v[i]) for k,v in self.enc.items()}\n\ndata_collator = DataCollatorForTokenClassification(tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:10:18.387427Z","iopub.execute_input":"2025-10-29T17:10:18.387702Z","iopub.status.idle":"2025-10-29T17:10:18.394636Z","shell.execute_reply.started":"2025-10-29T17:10:18.387678Z","shell.execute_reply":"2025-10-29T17:10:18.394006Z"}},"outputs":[],"execution_count":7},{"id":"9c9504e9-fa9e-418c-a858-551ce4e5f88a","cell_type":"code","source":"# =========================================================\n# âœ… Cell 6 â€” Build datasets/loaders\n# =========================================================\nBATCH_SIZE = 16\n\ndef make_dataset(lang, split): return UPOSDataset(load_split(lang, split))\n\n# Train/dev\nen_train, hi_train, ur_train = make_dataset(\"en_ewt\",\"train\"), make_dataset(\"hi_hdtb\",\"train\"), make_dataset(\"ur_udtb\",\"train\")\nen_dev, hi_dev, ur_dev = make_dataset(\"en_ewt\",\"dev\"), make_dataset(\"hi_hdtb\",\"dev\"), make_dataset(\"ur_udtb\",\"dev\")\n# Test\nbn_test = make_dataset(\"bn_bcc\",\"test\")\n\ndef make_loader(ds, shuffle=False):\n    return DataLoader(ds, batch_size=BATCH_SIZE, shuffle=shuffle, collate_fn=data_collator)\n\nldr = {\n    \"en_train\": make_loader(en_train, True),\n    \"hi_train\": make_loader(hi_train, True),\n    \"ur_train\": make_loader(ur_train, True),\n    \"en_dev\": make_loader(en_dev), \"hi_dev\": make_loader(hi_dev), \"ur_dev\": make_loader(ur_dev),\n    \"bn_test\": make_loader(bn_test)\n}\nfor k,v in ldr.items():\n    print(f\"{k}: {len(v)} batches\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:10:31.097440Z","iopub.execute_input":"2025-10-29T17:10:31.098126Z","iopub.status.idle":"2025-10-29T17:10:35.551898Z","shell.execute_reply.started":"2025-10-29T17:10:31.098098Z","shell.execute_reply":"2025-10-29T17:10:35.551259Z"}},"outputs":[{"name":"stdout","text":"en_train: 784 batches\nhi_train: 832 batches\nur_train: 253 batches\nen_dev: 126 batches\nhi_dev: 104 batches\nur_dev: 35 batches\nbn_test: 4 batches\n","output_type":"stream"}],"execution_count":8},{"id":"8ced7121-de36-4120-b178-9cdfed019e03","cell_type":"code","source":"# =========================================================\n# âœ… Cell 7 â€” Metrics (seqeval)\n# =========================================================\nfrom seqeval.metrics import classification_report, f1_score, accuracy_score\n\ndef decode_predictions(logits, labels):\n    preds = logits.argmax(-1)\n    y_true, y_pred = [], []\n    for p, l in zip(preds, labels):\n        true_seq, pred_seq = [], []\n        for pi, li in zip(p.tolist(), l.tolist()):\n            if li == -100: continue\n            true_seq.append(id2tag[li])\n            pred_seq.append(id2tag[pi])\n        if true_seq:\n            y_true.append(true_seq); y_pred.append(pred_seq)\n    return y_true, y_pred\n\n@torch.no_grad()\ndef evaluate_loader(model, loader, desc=\"eval\"):\n    model.eval()\n    all_true, all_pred = [], []\n    for batch in loader:\n        batch = {k:v.to(device) for k,v in batch.items()}\n        out = model(**batch)\n        yt, yp = decode_predictions(out.logits.cpu(), batch[\"labels\"].cpu())\n        all_true += yt; all_pred += yp\n    acc = accuracy_score(all_true, all_pred)\n    f1 = f1_score(all_true, all_pred)\n    return {\"acc\": acc, \"f1\": f1, \"report\": classification_report(all_true, all_pred, digits=4)}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:10:56.418888Z","iopub.execute_input":"2025-10-29T17:10:56.419357Z","iopub.status.idle":"2025-10-29T17:10:56.430013Z","shell.execute_reply.started":"2025-10-29T17:10:56.419334Z","shell.execute_reply":"2025-10-29T17:10:56.429427Z"}},"outputs":[],"execution_count":9},{"id":"9dd01886-6263-42bd-aa79-b816a21d2779","cell_type":"code","source":"# =========================================================\n# âœ… Cell 8 â€” Round-robin fine-tuning (ENâ†’HIâ†’UR)\n# =========================================================\nfrom torch.optim import AdamW\nimport itertools\n\nEPOCHS = 6\nLR = 1e-5\nMAX_GRAD_NORM = 1.0\nWARMUP_RATIO = 0.06\n\ndef cycle_loader(dl):\n    while True:\n        for b in dl: yield b\n\ndef make_epoch_iters():\n    en_it, hi_it, ur_it = map(cycle_loader, [ldr[\"en_train\"], ldr[\"hi_train\"], ldr[\"ur_train\"]])\n    per_lang = min(len(ldr[\"en_train\"]), len(ldr[\"hi_train\"]), len(ldr[\"ur_train\"]))\n    return en_it, hi_it, ur_it, per_lang\n\nsteps_per_epoch = min(len(ldr[\"en_train\"]), len(ldr[\"hi_train\"]), len(ldr[\"ur_train\"])) * 3\ntotal_steps = steps_per_epoch * EPOCHS\nwarmup_steps = int(total_steps * WARMUP_RATIO)\n\noptimizer = AdamW(model.parameters(), lr=LR)\nscheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\nbest_f1, best_ckpt = -1, \"xlmr_pos_roundrobin_best.ckpt\"\n\nfor epoch in range(1, EPOCHS+1):\n    model.train()\n    en_it, hi_it, ur_it, per_lang = make_epoch_iters()\n    lang_cycle = itertools.cycle([\"en\",\"hi\",\"ur\"])\n    total_batches = per_lang * 3\n    losses = []\n\n    for step in range(total_batches):\n        lang = next(lang_cycle)\n        batch = next(en_it) if lang==\"en\" else next(hi_it) if lang==\"hi\" else next(ur_it)\n        batch = {k:v.to(device) for k,v in batch.items()}\n        out = model(**batch)\n        loss = out.loss\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n        optimizer.step(); scheduler.step(); optimizer.zero_grad(set_to_none=True)\n        losses.append(loss.item())\n        if (step+1)%100==0 or (step+1)==total_batches:\n            print(f\"Epoch {epoch} Step {step+1}/{total_batches} | Loss {np.mean(losses):.4f}\")\n\n    # Evaluate\n    en_m, hi_m, ur_m = evaluate_loader(model, ldr[\"en_dev\"]), evaluate_loader(model, ldr[\"hi_dev\"]), evaluate_loader(model, ldr[\"ur_dev\"])\n    avg_f1 = (en_m[\"f1\"] + hi_m[\"f1\"] + ur_m[\"f1\"]) / 3\n    print(f\"\\nEpoch {epoch} Dev F1s â†’ EN={en_m['f1']:.4f}, HI={hi_m['f1']:.4f}, UR={ur_m['f1']:.4f}, AVG={avg_f1:.4f}\")\n\n    if avg_f1 > best_f1:\n        best_f1 = avg_f1\n        torch.save(model.state_dict(), best_ckpt)\n        print(f\"âœ… Saved new best (AVG F1={best_f1:.4f}) â†’ {best_ckpt}\")\n\nprint(\"ğŸ¯ Training done. Best AVG F1 =\", best_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T17:35:49.214873Z","iopub.execute_input":"2025-10-29T17:35:49.215636Z","iopub.status.idle":"2025-10-29T17:58:20.132988Z","shell.execute_reply.started":"2025-10-29T17:35:49.215612Z","shell.execute_reply":"2025-10-29T17:58:20.132269Z"}},"outputs":[{"name":"stdout","text":"Epoch 1 Step 100/759 | Loss 0.0636\nEpoch 1 Step 200/759 | Loss 0.0627\nEpoch 1 Step 300/759 | Loss 0.0636\nEpoch 1 Step 400/759 | Loss 0.0647\nEpoch 1 Step 500/759 | Loss 0.0651\nEpoch 1 Step 600/759 | Loss 0.0662\nEpoch 1 Step 700/759 | Loss 0.0660\nEpoch 1 Step 759/759 | Loss 0.0659\n\nEpoch 1 Dev F1s â†’ EN=0.9723, HI=0.9750, UR=0.9360, AVG=0.9611\nâœ… Saved new best (AVG F1=0.9611) â†’ xlmr_pos_roundrobin_best.ckpt\nEpoch 2 Step 100/759 | Loss 0.0622\nEpoch 2 Step 200/759 | Loss 0.0632\nEpoch 2 Step 300/759 | Loss 0.0625\nEpoch 2 Step 400/759 | Loss 0.0631\nEpoch 2 Step 500/759 | Loss 0.0624\nEpoch 2 Step 600/759 | Loss 0.0627\nEpoch 2 Step 700/759 | Loss 0.0629\nEpoch 2 Step 759/759 | Loss 0.0628\n\nEpoch 2 Dev F1s â†’ EN=0.9740, HI=0.9735, UR=0.9392, AVG=0.9622\nâœ… Saved new best (AVG F1=0.9622) â†’ xlmr_pos_roundrobin_best.ckpt\nEpoch 3 Step 100/759 | Loss 0.0589\nEpoch 3 Step 200/759 | Loss 0.0559\nEpoch 3 Step 300/759 | Loss 0.0562\nEpoch 3 Step 400/759 | Loss 0.0553\nEpoch 3 Step 500/759 | Loss 0.0559\nEpoch 3 Step 600/759 | Loss 0.0559\nEpoch 3 Step 700/759 | Loss 0.0561\nEpoch 3 Step 759/759 | Loss 0.0559\n\nEpoch 3 Dev F1s â†’ EN=0.9738, HI=0.9743, UR=0.9347, AVG=0.9609\nEpoch 4 Step 100/759 | Loss 0.0531\nEpoch 4 Step 200/759 | Loss 0.0516\nEpoch 4 Step 300/759 | Loss 0.0503\nEpoch 4 Step 400/759 | Loss 0.0517\nEpoch 4 Step 500/759 | Loss 0.0512\nEpoch 4 Step 600/759 | Loss 0.0514\nEpoch 4 Step 700/759 | Loss 0.0512\nEpoch 4 Step 759/759 | Loss 0.0515\n\nEpoch 4 Dev F1s â†’ EN=0.9734, HI=0.9743, UR=0.9372, AVG=0.9616\nEpoch 5 Step 100/759 | Loss 0.0482\nEpoch 5 Step 200/759 | Loss 0.0495\nEpoch 5 Step 300/759 | Loss 0.0481\nEpoch 5 Step 400/759 | Loss 0.0468\nEpoch 5 Step 500/759 | Loss 0.0471\nEpoch 5 Step 600/759 | Loss 0.0476\nEpoch 5 Step 700/759 | Loss 0.0478\nEpoch 5 Step 759/759 | Loss 0.0478\n\nEpoch 5 Dev F1s â†’ EN=0.9736, HI=0.9748, UR=0.9360, AVG=0.9614\nEpoch 6 Step 100/759 | Loss 0.0461\nEpoch 6 Step 200/759 | Loss 0.0451\nEpoch 6 Step 300/759 | Loss 0.0448\nEpoch 6 Step 400/759 | Loss 0.0453\nEpoch 6 Step 500/759 | Loss 0.0448\nEpoch 6 Step 600/759 | Loss 0.0444\nEpoch 6 Step 700/759 | Loss 0.0436\nEpoch 6 Step 759/759 | Loss 0.0434\n\nEpoch 6 Dev F1s â†’ EN=0.9736, HI=0.9745, UR=0.9374, AVG=0.9618\nğŸ¯ Training done. Best AVG F1 = 0.9622388077520604\n","output_type":"stream"}],"execution_count":12},{"id":"064ef23c-8124-4fa5-8555-2e5e84a4b234","cell_type":"code","source":"# =========================================================\n# âœ… Cell 9 â€” evaluation on Bangla\n# =========================================================\nBEST_CKPT = \"xlmr_pos_roundrobin_best.ckpt\"\nif Path(BEST_CKPT).exists():\n    model.load_state_dict(torch.load(BEST_CKPT, map_location=device))\n    print(\"Loaded best model:\", BEST_CKPT)\nelse:\n    print(\"âš ï¸ No checkpoint found, using current model.\")\n\nbn_metrics = evaluate_loader(model, ldr[\"bn_test\"])\nprint(\"\\n===== Bangla Evaluation =====\")\nprint(f\"Accuracy: {bn_metrics['acc']:.4f}\")\nprint(f\"F1: {bn_metrics['f1']:.4f}\\n\")\nprint(\"Detailed Report:\\n\", bn_metrics[\"report\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T18:02:22.284287Z","iopub.execute_input":"2025-10-29T18:02:22.285092Z","iopub.status.idle":"2025-10-29T18:02:23.437625Z","shell.execute_reply.started":"2025-10-29T18:02:22.285066Z","shell.execute_reply":"2025-10-29T18:02:23.436794Z"}},"outputs":[{"name":"stdout","text":"Loaded best model: xlmr_pos_roundrobin_best.ckpt\n\n===== Bangla Evaluation =====\nAccuracy: 0.8594\nF1: 0.8182\n\nDetailed Report:\n               precision    recall  f1-score   support\n\n         ART     0.6364    0.7778    0.7000         9\n        CONJ     0.5000    0.5000    0.5000         2\n          DJ     0.6429    0.7500    0.6923        12\n          DP     1.0000    0.5000    0.6667         2\n          DV     0.6000    0.2500    0.3529        12\n         ERB     0.8571    0.8077    0.8317        52\n          ET     0.5000    0.2308    0.3158        13\n         NTJ     0.7500    1.0000    0.8571         6\n         OUN     0.8200    0.8200    0.8200        50\n         RON     0.7358    0.9070    0.8125        43\n        ROPN     0.5714    1.0000    0.7273         4\n          UM     0.6667    1.0000    0.8000         2\n        UNCT     1.0000    1.0000    1.0000        73\n          UX     0.7500    0.5000    0.6000         6\n\n   micro avg     0.8182    0.8182    0.8182       286\n   macro avg     0.7165    0.7174    0.6912       286\nweighted avg     0.8146    0.8182    0.8067       286\n\n","output_type":"stream"}],"execution_count":13},{"id":"0d70ce81-977d-400c-9334-63520d32ab55","cell_type":"code","source":"# ============================\n# ğŸ‡¬ğŸ‡§ğŸ‡®ğŸ‡³ğŸ‡µğŸ‡° â†’ ğŸ‡§ğŸ‡©  Multi-Source POS (XLM-R)\n# EN+HI+UR train/dev  â†’  BN test\n# Sweep: LR Ã— Epochs ; Multi-seed ; Reports + Confusion + Curves\n# ============================\nimport os\n# Quiet noisy libs BEFORE importing HF/Torch\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\nos.environ[\"XLA_PYTHON_CLIENT_PREALLOCATE\"] = \"false\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport random\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom conllu import parse_incr\nfrom datasets import Dataset, concatenate_datasets\nfrom transformers import (\n    AutoTokenizer, AutoModelForTokenClassification,\n    Trainer, TrainingArguments, DataCollatorForTokenClassification, set_seed\n)\nfrom transformers.trainer_utils import EvalPrediction\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix\nfrom itertools import product\nfrom pathlib import Path\n\n# Harmless warnings\nwarnings.filterwarnings(\"ignore\", message=\"Some weights of\")\nwarnings.filterwarnings(\"ignore\", message=\"checkpoint\")\nwarnings.filterwarnings(\"ignore\", message=\"AdamW is deprecated\")\n\n# ===== Universal POS labels (UD) =====\nLABEL_LIST = ['ADJ','ADP','ADV','AUX','CCONJ','DET','INTJ','NOUN','NUM',\n              'PART','PRON','PROPN','PUNCT','SCONJ','SYM','VERB','X']\nLABEL2ID = {t:i for i,t in enumerate(LABEL_LIST)}\nID2LABEL = {i:t for t,i in LABEL2ID.items()}\n\n# ===== Config =====\nBASE_CFG = dict(\n    model_name=\"xlm-roberta-base\",\n    max_len=128,\n    train_bs=8,\n    eval_bs=16,\n    # small sweep\n    sweep_lrs=[3e-5],#1e-5\n    sweep_epochs=[5],#3\n    # multi-seed\n    seeds=[42],#7,2024\n)\n\n# ------------------------- Utility Functions -------------------------\n\ndef _set_all_seeds(seed:int):\n    set_seed(seed)\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n\ndef _load_ud(file_path:str) -> Dataset:\n    \"\"\"Read a UD .conllu file â†’ HF Dataset with tokens and upos.\"\"\"\n    cache = file_path + \".cache\"\n    if os.path.exists(cache):\n        return Dataset.load_from_disk(cache)\n    rows = []\n    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n        for tokenlist in parse_incr(f):\n            rows.append({\"tokens\":[t[\"form\"] for t in tokenlist],\n                         \"upos\":[t[\"upos\"] for t in tokenlist]})\n    ds = Dataset.from_pandas(pd.DataFrame(rows))\n    ds.save_to_disk(cache)\n    return ds\n\ndef _to_ids(examples):\n    \"\"\"Map UPOS tags â†’ integer IDs.\"\"\"\n    return {\"upos\":[[LABEL2ID.get(tag, LABEL2ID['X']) for tag in tags] for tags in examples[\"upos\"]]}\n\ndef _tokenize_fn(tokenizer, max_len):\n    \"\"\"Tokenize & align UPOS tags (only first subword is labeled; others = -100).\"\"\"\n    def f(examples):\n        tok = tokenizer(\n            examples[\"tokens\"], truncation=True, padding=\"max_length\",\n            max_length=max_len, is_split_into_words=True\n        )\n        labels = []\n        for i, lab in enumerate(examples[\"upos\"]):\n            word_ids = tok.word_ids(batch_index=i)\n            prev = None; ids = []\n            for w in word_ids:\n                if w is None:\n                    ids.append(-100)\n                elif w != prev:\n                    ids.append(lab[w])\n                else:\n                    ids.append(-100)\n                prev = w\n            # hard pad to max_len (defensive; tokenizer already pads)\n            ids += [-100]*(max_len - len(ids))\n            labels.append(ids[:max_len])\n        tok[\"labels\"] = labels\n        return tok\n    return f\n\ndef _flatten_preds_labels(pred_logits, dataset):\n    \"\"\"Flatten predictions/labels across tokens, ignoring -100.\"\"\"\n    preds = np.argmax(pred_logits, axis=2)\n    y_true_ids, y_pred_ids = [], []\n    for i in range(len(dataset)):\n        labels = dataset[i][\"labels\"]\n        mask = np.array(labels) != -100\n        y_true_ids.extend(np.array(labels)[mask].tolist())\n        y_pred_ids.extend(np.array(preds[i])[mask].tolist())\n    return y_true_ids, y_pred_ids\n\ndef _metrics_from_ids(y_true_ids, y_pred_ids):\n    \"\"\"Compute accuracy, macro-F1, weighted-F1, full report, and confusion matrix.\"\"\"\n    y_true = [ID2LABEL[i] for i in y_true_ids]\n    y_pred = [ID2LABEL[i] for i in y_pred_ids]\n    acc = accuracy_score(y_true, y_pred)\n    macro_f1 = f1_score(y_true, y_pred, labels=LABEL_LIST, average=\"macro\", zero_division=0)\n    weighted_f1 = f1_score(y_true, y_pred, labels=LABEL_LIST, average=\"weighted\", zero_division=0)\n    report = classification_report(y_true, y_pred, labels=LABEL_LIST, digits=4, zero_division=0)\n    cm = confusion_matrix(y_true, y_pred, labels=LABEL_LIST)  # counts\n    return acc, macro_f1, weighted_f1, report, cm\n\ndef _plot_confusion(cm_norm, title, out_png):\n    plt.figure(figsize=(12,10))\n    sns.heatmap(cm_norm, annot=False, cmap=\"Blues\",\n                xticklabels=LABEL_LIST, yticklabels=LABEL_LIST)\n    plt.xlabel(\"Predicted\"); plt.ylabel(\"True\"); plt.title(title)\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\ndef _compute_metrics(eval_pred: EvalPrediction):\n    \"\"\"For Trainer: token accuracy (ignoring -100).\"\"\"\n    preds = np.argmax(eval_pred.predictions, axis=2)\n    labels = eval_pred.label_ids\n    mask = labels != -100\n    total = mask.sum()\n    correct = ((preds == labels) & mask).sum()\n    acc = float(correct) / float(total) if total > 0 else 0.0\n    return {\"accuracy\": acc}\n\ndef _plot_learning_curve_from_logs(log_history, out_png, title=\"Learning Curve\"):\n    epochs, eval_loss, eval_acc = [], [], []\n    train_epochs, train_loss = [], []\n    for rec in log_history:\n        if \"eval_loss\" in rec:\n            epochs.append(rec.get(\"epoch\"))\n            eval_loss.append(rec[\"eval_loss\"])\n            eval_acc.append(rec.get(\"eval_accuracy\"))\n        elif \"loss\" in rec and \"epoch\" in rec:\n            train_epochs.append(rec[\"epoch\"])\n            train_loss.append(rec[\"loss\"])\n    plt.figure(figsize=(10,6))\n    if train_epochs:\n        plt.plot(train_epochs, train_loss, label=\"train_loss\", marker=\"o\")\n    if epochs:\n        plt.plot(epochs, eval_loss, label=\"eval_loss\", marker=\"o\")\n    if epochs and any(a is not None for a in eval_acc):\n        ax = plt.gca()\n        ax2 = ax.twinx()\n        ax2.plot(epochs, eval_acc, label=\"eval_accuracy\", marker=\"s\", linestyle=\"--\", color=\"tab:green\")\n        ax2.set_ylabel(\"Eval Accuracy\")\n        ax2.legend(loc=\"lower right\")\n    plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(title)\n    plt.legend(loc=\"upper right\")\n    plt.tight_layout()\n    plt.savefig(out_png)\n    plt.close()\n\n# ------------------------- Main Experiment -------------------------\n\ndef run_pos_tagging_multisource():\n    \"\"\"\n    Train on EN+HI+UR (train) ; validate on EN+HI+UR (dev concatenated).\n    Zero-shot evaluate on BN test.\n    Multi-seed + hyperparameter sweep.\n    \"\"\"\n    print(\"\\n\" + \"=\"*50 + \"\\nMULTI-SOURCE POS TAGGING (EN+HI+UR â†’ BN)\\n\" + \"=\"*50)\n    out_root = Path(\"results_msweep_multisrc\"); out_root.mkdir(exist_ok=True, parents=True)\n\n    # ===== 1ï¸âƒ£ Load & preprocess datasets from ud_data =====\n    DATA_DIR = Path(\"./ud_data\")\n    print(\"ğŸ“‚ Loading datasets from:\", DATA_DIR.resolve())\n\n    en_train = _load_ud(str(DATA_DIR / \"en_ewt-ud-train.conllu\")).map(_to_ids, batched=True)\n    hi_train = _load_ud(str(DATA_DIR / \"hi_hdtb-ud-train.conllu\")).map(_to_ids, batched=True)\n    ur_train = _load_ud(str(DATA_DIR / \"ur_udtb-ud-train.conllu\")).map(_to_ids, batched=True)\n\n    en_dev = _load_ud(str(DATA_DIR / \"en_ewt-ud-dev.conllu\")).map(_to_ids, batched=True)\n    hi_dev = _load_ud(str(DATA_DIR / \"hi_hdtb-ud-dev.conllu\")).map(_to_ids, batched=True)\n    ur_dev = _load_ud(str(DATA_DIR / \"ur_udtb-ud-dev.conllu\")).map(_to_ids, batched=True)\n\n    # Bengali (zero-shot test)\n    bn_test = _load_ud(str(DATA_DIR / \"bn_bru-ud-test.conllu\")).map(_to_ids, batched=True)\n\n    # Concatenate multi-source train/dev\n    from datasets import concatenate_datasets\n    train_ds = concatenate_datasets([en_train, hi_train, ur_train])\n    dev_ds   = concatenate_datasets([en_dev, hi_dev, ur_dev])\n\n    print(f\"âœ… Loaded: Train={len(train_ds)}  Dev={len(dev_ds)}  BN-Test={len(bn_test)}\")\n\n    # ===== 2ï¸âƒ£ Tokenize =====\n    tokenizer = AutoTokenizer.from_pretrained(BASE_CFG[\"model_name\"])\n    tok_fn = _tokenize_fn(tokenizer, BASE_CFG[\"max_len\"])\n\n    print(\"ğŸ§© Tokenizing datasetsâ€¦\")\n    train_tok = train_ds.map(tok_fn, batched=True, remove_columns=train_ds.column_names)\n    dev_tok   = dev_ds.map(tok_fn,   batched=True, remove_columns=dev_ds.column_names)\n    bn_test_tok = bn_test.map(tok_fn, batched=True, remove_columns=bn_test.column_names)\n\n    # (rest of script remains unchanged â€” zero-shot, sweep, etc.)\n\n\n    # ===== 2) Zero-shot baseline on BN-test =====\n    print(\"\\n--- ZERO-SHOT (bn_test) ---\")\n    zs_model = AutoModelForTokenClassification.from_pretrained(\n        BASE_CFG[\"model_name\"], num_labels=len(LABEL_LIST),\n        id2label=ID2LABEL, label2id=LABEL2ID, ignore_mismatched_sizes=True\n    )\n    zs_trainer = Trainer(\n        model=zs_model,\n        args=TrainingArguments(\n            output_dir=str(out_root/\"zero_shot\"),\n            per_device_eval_batch_size=BASE_CFG[\"eval_bs\"],\n            report_to=\"none\",\n            dataloader_num_workers=2,\n            dataloader_pin_memory=True,\n            fp16=torch.cuda.is_available(),\n        ),\n        data_collator=DataCollatorForTokenClassification(tokenizer)\n    )\n    zs_pred = zs_trainer.predict(bn_test_tok)\n    zs_true, zs_pred_ids = _flatten_preds_labels(zs_pred.predictions, bn_test_tok)\n    zs_acc, zs_mf1, zs_wf1, zs_report, zs_cm = _metrics_from_ids(zs_true, zs_pred_ids)\n    (out_root/\"zero_shot\").mkdir(exist_ok=True, parents=True)\n    with open(out_root/\"zero_shot\"/\"bn_test_report.txt\",\"w\",encoding=\"utf-8\") as f:\n        f.write(f\"acc={zs_acc:.6f}  macroF1={zs_mf1:.6f}  weightedF1={zs_wf1:.6f}\\n\\n{zs_report}\")\n    zs_cm_norm = zs_cm / (zs_cm.sum(axis=1, keepdims=True) + 1e-9)\n    _plot_confusion(zs_cm_norm, \"Zero-shot Confusion (BN test)\", out_root/\"zero_shot\"/\"bn_confusion.png\")\n    del zs_model; torch.cuda.empty_cache()\n\n    # ===== 3) Multi-seed tiny sweep + learning curves =====\n    runs = []\n    aggregate_cm_counts = np.zeros((len(LABEL_LIST), len(LABEL_LIST)), dtype=np.int64)\n\n    for seed in BASE_CFG[\"seeds\"]:\n        print(f\"\\n===== SEED {seed} =====\")\n        _set_all_seeds(seed)\n        best = dict(macro_f1=-1, lr=None, epochs=None, trainer=None, model_dir=None)\n\n        for lr, epochs in product(BASE_CFG[\"sweep_lrs\"], BASE_CFG[\"sweep_epochs\"]):\n            print(f\"  >> Try LR={lr}, Epochs={epochs}\")\n            _set_all_seeds(seed)\n            model = AutoModelForTokenClassification.from_pretrained(\n                BASE_CFG[\"model_name\"], num_labels=len(LABEL_LIST),\n                id2label=ID2LABEL, label2id=LABEL2ID, ignore_mismatched_sizes=True\n            )\n            args = TrainingArguments(\n                output_dir=str(out_root/f\"tmp_s{seed}_lr{lr}_ep{epochs}\"),\n                learning_rate=lr,\n                per_device_train_batch_size=BASE_CFG[\"train_bs\"],\n                per_device_eval_batch_size=BASE_CFG[\"eval_bs\"],\n                num_train_epochs=epochs,\n                eval_strategy=\"epoch\",\n                save_strategy=\"no\",\n                logging_strategy=\"epoch\",\n                report_to=\"none\",\n                fp16=torch.cuda.is_available(),\n                remove_unused_columns=True,\n                dataloader_num_workers=2,\n                dataloader_pin_memory=True,\n                seed=seed,\n            )\n            trainer = Trainer(\n                model=model, args=args,\n                train_dataset=train_tok, eval_dataset=dev_tok,\n                data_collator=DataCollatorForTokenClassification(tokenizer),\n                compute_metrics=_compute_metrics\n            )\n            trainer.train()\n\n            # Dev evaluation for selection (macro-F1 on concatenated EN+HI+UR dev)\n            dev_pred = trainer.predict(dev_tok)\n            y_true_ids, y_pred_ids = _flatten_preds_labels(dev_pred.predictions, dev_tok)\n            dev_macro_f1 = f1_score([ID2LABEL[i] for i in y_true_ids],\n                                    [ID2LABEL[i] for i in y_pred_ids],\n                                    labels=LABEL_LIST, average=\"macro\", zero_division=0)\n            print(f\"     dev macro-F1 = {dev_macro_f1:.4f}\")\n\n            if dev_macro_f1 > best[\"macro_f1\"]:\n                best.update(macro_f1=dev_macro_f1, lr=lr, epochs=epochs, trainer=trainer, model_dir=args.output_dir)\n            else:\n                del trainer.model; del trainer; torch.cuda.empty_cache()\n\n        # Use best trainer/model for this seed â†’ Final EN+HI+UR-dev & BN-test\n        seed_dir = out_root/f\"seed_{seed}\"; Path(seed_dir).mkdir(exist_ok=True, parents=True)\n        print(f\"Best for seed {seed}: LR={best['lr']}  Epochs={best['epochs']}  (dev macro-F1={best['macro_f1']:.4f})\")\n\n        # Save best model + tokenizer\n        best_dir = Path(seed_dir) / \"best_model\"\n        best[\"trainer\"].save_model(str(best_dir))\n        tokenizer.save_pretrained(str(best_dir))\n        print(f\"Saved best model for seed {seed} to: {best_dir}\")\n\n        # Learning curve from best trainer logs (multi-source dev)\n        log_hist = best[\"trainer\"].state.log_history\n        pd.DataFrame(log_hist).to_csv(Path(seed_dir)/\"training_log_history.csv\", index=False)\n        _plot_learning_curve_from_logs(\n            log_hist,\n            Path(seed_dir)/\"learning_curve.png\",\n            title=f\"Learning Curve (EN+HI+UR dev) â€” seed {seed}, lr={best['lr']}, ep={best['epochs']}\"\n        )\n\n        # Multi-source dev final report\n        dev_pred = best[\"trainer\"].predict(dev_tok)\n        dev_true, dev_pred_ids = _flatten_preds_labels(dev_pred.predictions, dev_tok)\n        dev_acc, dev_mf1, dev_wf1, dev_report, dev_cm = _metrics_from_ids(dev_true, dev_pred_ids)\n        with open(Path(seed_dir)/\"multi_dev_report.txt\",\"w\",encoding=\"utf-8\") as f:\n            f.write(f\"acc={dev_acc:.6f}  macroF1={dev_mf1:.6f}  weightedF1={dev_wf1:.6f}\\n\\n{dev_report}\")\n        dev_cm_norm = dev_cm / (dev_cm.sum(axis=1, keepdims=True) + 1e-9)\n        _plot_confusion(dev_cm_norm, f\"EN+HI+UR-dev Confusion (seed {seed})\", Path(seed_dir)/\"multi_dev_confusion.png\")\n\n        # BN-test final report (held-out, zero-shot target)\n        test_pred = best[\"trainer\"].predict(bn_test_tok)\n        test_true, test_pred_ids = _flatten_preds_labels(test_pred.predictions, bn_test_tok)\n        test_acc, test_mf1, test_wf1, test_report, test_cm = _metrics_from_ids(test_true, test_pred_ids)\n        with open(Path(seed_dir)/\"bn_test_report.txt\",\"w\",encoding=\"utf-8\") as f:\n            f.write(f\"acc={test_acc:.6f}  macroF1={test_mf1:.6f}  weightedF1={test_wf1:.6f}\\n\\n{test_report}\")\n        test_cm_norm = test_cm / (test_cm.sum(axis=1, keepdims=True) + 1e-9)\n        _plot_confusion(test_cm_norm, f\"BN-test Confusion (seed {seed})\", Path(seed_dir)/\"bn_confusion.png\")\n\n        # Accumulate results\n        runs.append(dict(seed=seed,\n                         multisrc_dev=dict(acc=dev_acc, macro_f1=dev_mf1, weighted_f1=dev_wf1),\n                         bn=dict(acc=test_acc, macro_f1=test_mf1, weighted_f1=test_wf1)))\n        aggregate_cm_counts += test_cm\n\n        del best[\"trainer\"].model; del best[\"trainer\"]; torch.cuda.empty_cache()\n\n    # ===== 4) Aggregate across seeds =====\n    def _agg(metric):\n        dev_vals = [r[\"multisrc_dev\"][metric] for r in runs]\n        bn_vals  = [r[\"bn\"][metric] for r in runs]\n        return (np.mean(dev_vals), np.std(dev_vals), np.mean(bn_vals), np.std(bn_vals))\n\n    mean_dev_acc, std_dev_acc, mean_bn_acc, std_bn_acc = _agg(\"acc\")\n    mean_dev_mf1, std_dev_mf1, mean_bn_mf1, std_bn_mf1 = _agg(\"macro_f1\")\n    mean_dev_wf1, std_dev_wf1, mean_bn_wf1, std_bn_wf1 = _agg(\"weighted_f1\")\n\n    agg_cm_norm = aggregate_cm_counts / (aggregate_cm_counts.sum(axis=1, keepdims=True) + 1e-9)\n    _plot_confusion(agg_cm_norm, \"BN-test Confusion (Aggregate over seeds)\", out_root/\"bn_confusion_aggregate.png\")\n\n    summary = f\"\"\"FINAL SUMMARY over {len(runs)} seed(s)\nEN+HI+UR dev : acc={mean_dev_acc:.4f}Â±{std_dev_acc:.4f} | macroF1={mean_dev_mf1:.4f}Â±{std_dev_mf1:.4f} | weightedF1={mean_dev_wf1:.4f}Â±{std_dev_wf1:.4f}\nBN-test      : acc={mean_bn_acc:.4f}Â±{std_bn_acc:.4f} | macroF1={mean_bn_mf1:.4f}Â±{std_bn_mf1:.4f} | weightedF1={mean_bn_wf1:.4f}Â±{std_bn_wf1:.4f}\n\nZero-shot BN-test (single run): acc={zs_acc:.4f} | macroF1={zs_mf1:.4f} | weightedF1={zs_wf1:.4f}\n\"\"\"\n    print(\"\\n\" + summary)\n    out_root.joinpath(\"summary.txt\").write_text(summary, encoding=\"utf-8\")\n\n    return {\n        \"zero_shot_bn\": dict(acc=zs_acc, macro_f1=zs_mf1, weighted_f1=zs_wf1),\n        \"runs\": runs,\n        \"mean_std\": {\n            \"multisrc_dev\": {\"acc\": (mean_dev_acc, std_dev_acc), \"macro_f1\": (mean_dev_mf1, std_dev_mf1), \"weighted_f1\": (mean_dev_wf1, std_dev_wf1)},\n            \"bn_test\": {\"acc\": (mean_bn_acc, std_bn_acc), \"macro_f1\": (mean_bn_mf1, std_bn_mf1), \"weighted_f1\": (mean_bn_wf1, std_bn_wf1)},\n        }\n    }\n\nif __name__ == \"__main__\":\n    run_pos_tagging_multisource()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-29T19:33:48.078069Z","iopub.execute_input":"2025-10-29T19:33:48.078761Z","iopub.status.idle":"2025-10-29T20:53:36.942797Z","shell.execute_reply.started":"2025-10-29T19:33:48.078735Z","shell.execute_reply":"2025-10-29T20:53:36.941835Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761766434.562925      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761766434.690271      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"\n==================================================\nMULTI-SOURCE POS TAGGING (EN+HI+UR â†’ BN)\n==================================================\nğŸ“‚ Loading datasets from: /kaggle/working/ud_data\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/12544 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"012c3063e3cc4b9ca298e876148eaba3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/12544 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"322cd74ec549444597a6826b939cd20b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/13306 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91811c535c0049ebbf983fd541ea53cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/13306 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d609871c2ef84f39bc3aa8a17d60f145"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/4043 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0461258802784ea2aa3b473dc9e33aae"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4043 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf0e878165cf4c6ea92f5a716c602eaf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/2001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0bda02ab5a04583934af79de048411b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2001 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e2b6ac590ff457884d71a0d0ec9d250"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/1659 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d94496bb246d40889eacd7e4a3d2b32b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1659 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e95e7d989ef46558cdf0b118355fc3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2ce2f5b541046b785917727c636f1e3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/552 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b9f7bec0f6492ead8779977068fc08"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/56 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cb7dfe74fe694b67a3470965c42eb1d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/56 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4a9310db5d764208a5c1114fe413b607"}},"metadata":{}},{"name":"stdout","text":"âœ… Loaded: Train=29893  Dev=4212  BN-Test=56\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35b86b74ad0d4b248aefcf703009ebb2"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a63427ff375450eb4f4afcc5057d170"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"64bf3a23ac6c4a4399d69b061d44253b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"138d58a12b4b4943ae77d508bb916ba7"}},"metadata":{}},{"name":"stdout","text":"ğŸ§© Tokenizing datasetsâ€¦\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/29893 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fea0a38596ee435d8a8f40f3238519fd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4212 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96b5bfcc4e134a73a3a401ee874e0a0c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/56 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35f38e245fe540f089163e1f033a8ce5"}},"metadata":{}},{"name":"stdout","text":"\n--- ZERO-SHOT (bn_test) ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.12G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d3cb74c64e49cd9321149a0a1f686f"}},"metadata":{}},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\n===== SEED 42 =====\n  >> Try LR=3e-05, Epochs=5\n","output_type":"stream"},{"name":"stderr","text":"Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:477: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='9345' max='9345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [9345/9345 1:17:38, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.182500</td>\n      <td>0.093423</td>\n      <td>0.969870</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.075800</td>\n      <td>0.087061</td>\n      <td>0.972792</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.054100</td>\n      <td>0.088025</td>\n      <td>0.974545</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.040200</td>\n      <td>0.092146</td>\n      <td>0.974624</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.030300</td>\n      <td>0.097849</td>\n      <td>0.975408</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"     dev macro-F1 = 0.9687\nBest for seed 42: LR=3e-05  Epochs=5  (dev macro-F1=0.9687)\nSaved best model for seed 42 to: results_msweep_multisrc/seed_42/best_model\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"name":"stdout","text":"\nFINAL SUMMARY over 1 seed(s)\nEN+HI+UR dev : acc=0.9754Â±0.0000 | macroF1=0.9687Â±0.0000 | weightedF1=0.9754Â±0.0000\nBN-test      : acc=0.8562Â±0.0000 | macroF1=0.5617Â±0.0000 | weightedF1=0.8428Â±0.0000\n\nZero-shot BN-test (single run): acc=0.0187 | macroF1=0.0022 | weightedF1=0.0007\n\n","output_type":"stream"}],"execution_count":5}]}