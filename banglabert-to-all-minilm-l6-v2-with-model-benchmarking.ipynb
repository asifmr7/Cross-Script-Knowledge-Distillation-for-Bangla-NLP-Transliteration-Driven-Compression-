{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceId":13581507,"sourceType":"datasetVersion","datasetId":8628620},{"sourceId":13581607,"sourceType":"datasetVersion","datasetId":8628682}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade --no-cache-dir transformers==4.45.2 indic-transliteration -q\n!pip install --no-cache-dir scikit-learn pandas tqdm matplotlib sentencepiece -q\n\nimport os, json, random, numpy as np, torch\nfrom pathlib import Path\n\n# ---- Repro / device / dirs\nSEED = 42\nrandom.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nWORK_DIR = Path(\"/kaggle/working\"); WORK_DIR.mkdir(parents=True, exist_ok=True)\n\n# ---- Config\nTEACHER_MODEL_ID = \"csebuetnlp/banglabert\"\nSTUDENT_MODEL_ID = \"sentence-transformers/all-MiniLM-L6-v2\"\nMAX_LEN = 128\nBATCH_SIZE = 16\n\n# Teacher FT\nEPOCHS_TEACHER = 3\nLR_TEACHER = 2e-5\nWARMUP_RATIO_T = 0.1\nWEIGHT_DECAY_T = 0.01\n\n# KD\nEPOCHS_STUDENT = 5\nLR_STUDENT = 3e-5\nWARMUP_RATIO_S = 0.1\nWEIGHT_DECAY_S = 0.01\nPATIENCE = 2\n\nKD_T = 3.0\nKD_ALPHA = 0.5\nGAMMA_HIDDEN = 1.0\n\nprint(\"‚úÖ Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:43:12.239210Z","iopub.execute_input":"2025-11-04T19:43:12.239528Z","iopub.status.idle":"2025-11-04T19:43:33.713607Z","shell.execute_reply.started":"2025-11-04T19:43:12.239502Z","shell.execute_reply":"2025-11-04T19:43:33.712779Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m113.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.6/159.6 kB\u001b[0m \u001b[31m330.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m351.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m289.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndatasets 4.1.1 requires pyarrow>=21.0.0, but you have pyarrow 19.0.1 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.0a1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m‚úÖ Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom pathlib import Path\n\nDATA_DIR = Path(\"/kaggle/input/dataaaaaaa\")\nPOS_FILE = DATA_DIR / \"all_positive_8500.txt\"\nNEG_FILE = DATA_DIR / \"all_negative_3307.txt\"\nassert POS_FILE.exists() and NEG_FILE.exists(), f\"Missing data: {POS_FILE}, {NEG_FILE}\"\n\ndef read_txt(p: Path):\n    with open(p, encoding=\"utf-8\") as f:\n        return [ln.strip() for ln in f if ln.strip()]\n\npos, neg = read_txt(POS_FILE), read_txt(NEG_FILE)\ndf = pd.DataFrame({\"text\": pos + neg, \"label\": [1]*len(pos) + [0]*len(neg)}).sample(frac=1, random_state=SEED)\n\ntrain_df, temp_df = train_test_split(df, test_size=0.2, stratify=df[\"label\"], random_state=SEED)\nval_df,   test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=SEED)\nprint(f\"Train={len(train_df)} | Val={len(val_df)} | Test={len(test_df)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:45:59.660222Z","iopub.execute_input":"2025-11-04T19:45:59.661214Z","iopub.status.idle":"2025-11-04T19:46:00.681397Z","shell.execute_reply.started":"2025-11-04T19:45:59.661187Z","shell.execute_reply":"2025-11-04T19:46:00.680619Z"}},"outputs":[{"name":"stdout","text":"Train=9445 | Val=1181 | Test=1181\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# train teacher","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm.auto import tqdm\n\nclass TxtClsDataset(Dataset):\n    def __init__(self, df, tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.tok, self.max_len = tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        enc = self.tok(self.texts[i], truncation=True, padding=\"max_length\",\n                       max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\nteacher_tok = AutoTokenizer.from_pretrained(TEACHER_MODEL_ID)\nteacher = AutoModelForSequenceClassification.from_pretrained(TEACHER_MODEL_ID, num_labels=2).to(DEVICE)\n\ntr_loader = DataLoader(TxtClsDataset(train_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE, shuffle=True)\nva_loader = DataLoader(TxtClsDataset(val_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\nte_loader = DataLoader(TxtClsDataset(test_df, teacher_tok, MAX_LEN), batch_size=BATCH_SIZE)\n\nopt = AdamW(teacher.parameters(), lr=LR_TEACHER, weight_decay=WEIGHT_DECAY_T)\nsteps = len(tr_loader) * EPOCHS_TEACHER\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO_T*steps), steps)\ncriterion = torch.nn.CrossEntropyLoss()\n\nbest_f1 = -1\nfor ep in range(1, EPOCHS_TEACHER+1):\n    teacher.train(); total = 0\n    for b in tqdm(tr_loader, desc=f\"Teacher Epoch {ep}/{EPOCHS_TEACHER}\"):\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        loss = out.loss\n        loss.backward()\n        opt.step(); sch.step(); opt.zero_grad()\n        total += loss.item()\n\n    teacher.eval(); preds, gold = [], []\n    with torch.no_grad():\n        for b in va_loader:\n            b = {k:v.to(DEVICE) for k,v in b.items()}\n            out = teacher(**b)\n            preds += out.logits.argmax(-1).cpu().tolist()\n            gold  += b[\"labels\"].cpu().tolist()\n    acc = accuracy_score(gold, preds)\n    f1m = f1_score(gold, preds, average=\"macro\")\n    print(f\"Val: Acc={acc:.4f} | F1_macro={f1m:.4f}\")\n    if f1m > best_f1:\n        best_f1 = f1m\n        save_dir = WORK_DIR / \"finetuned_banglabert\"\n        save_dir.mkdir(parents=True, exist_ok=True)\n        teacher.save_pretrained(save_dir)\n        teacher_tok.save_pretrained(save_dir)\n        print(\"üíæ Saved best BanglaBERT teacher.\")\n\n# quick test\nteacher.eval(); preds, gold = [], []\nwith torch.no_grad():\n    for b in te_loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = teacher(**b)\n        preds += out.logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\nprint(\"‚úÖ BanglaBERT Teacher [Test]: Acc={:.4f} | F1_macro={:.4f}\".format(\n    accuracy_score(gold, preds), f1_score(gold, preds, average=\"macro\")))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:46:03.488108Z","iopub.execute_input":"2025-11-04T19:46:03.488948Z","iopub.status.idle":"2025-11-04T19:57:39.764495Z","shell.execute_reply.started":"2025-11-04T19:46:03.488922Z","shell.execute_reply":"2025-11-04T19:57:39.763653Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/119 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f36a791be0e49749b0dc7b252ff607a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/586 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b7b1f65266d42ea9595fd95fa2b567c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3fe6cee38bfc435f872e788abd81799b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97b1d38e5851465fa38ccc505cfd9497"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/443M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bce3617f878946188829d032c9ca0c5f"}},"metadata":{}},{"name":"stderr","text":"Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at csebuetnlp/banglabert and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 1/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1ebedbf558b49f4bedccc46f4b12b8a"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9577 | F1_macro=0.9489\nüíæ Saved best BanglaBERT teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 2/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73b311b29aa0441e84e1e0965582753a"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9670 | F1_macro=0.9586\nüíæ Saved best BanglaBERT teacher.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Teacher Epoch 3/3:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d0b3401e89f4fc39d698f41f41b33cb"}},"metadata":{}},{"name":"stdout","text":"Val: Acc=0.9678 | F1_macro=0.9601\nüíæ Saved best BanglaBERT teacher.\n‚úÖ BanglaBERT Teacher [Test]: Acc=0.9687 | F1_macro=0.9611\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# KD Data (Transliteration, 2 tokenizers)","metadata":{}},{"cell_type":"code","source":"from indic_transliteration import sanscript\nfrom indic_transliteration.sanscript import transliterate\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\n\ndef transliterate_bn_text(txt: str) -> str:\n    try:\n        return transliterate(txt, sanscript.BENGALI, sanscript.ITRANS)\n    except Exception:\n        return txt\n\nfrom transformers import AutoTokenizer\nstudent_tok = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n\nclass KDDataset(Dataset):\n    def __init__(self, df, t_tok, s_tok, max_len):\n        self.texts = df.text.tolist(); self.labels = df.label.tolist()\n        self.ttok, self.stok, self.max_len = t_tok, s_tok, max_len\n    def __len__(self): return len(self.texts)\n    def __getitem__(self, i):\n        txt_bn = self.texts[i]; txt_en = transliterate_bn_text(txt_bn)\n        t = self.ttok(txt_bn, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        s = self.stok(txt_en, truncation=True, padding=\"max_length\", max_length=self.max_len, return_tensors=\"pt\")\n        return {\n            \"t_input_ids\": t[\"input_ids\"].squeeze(0),\n            \"t_attention_mask\": t[\"attention_mask\"].squeeze(0),\n            \"s_input_ids\": s[\"input_ids\"].squeeze(0),\n            \"s_attention_mask\": s[\"attention_mask\"].squeeze(0),\n            \"labels\": torch.tensor(self.labels[i], dtype=torch.long)\n        }\n\ndef pad_collate(batch, t_pad, s_pad):\n    out = {}\n    for k in batch[0]:\n        if k == \"labels\": out[k] = torch.stack([b[k] for b in batch])\n        elif k.startswith(\"t_\"):\n            padv = 0 if \"attention\" in k else t_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n        elif k.startswith(\"s_\"):\n            padv = 0 if \"attention\" in k else s_pad\n            out[k] = nn.utils.rnn.pad_sequence([b[k] for b in batch], batch_first=True, padding_value=padv)\n    return out\n\ntrain_loader = DataLoader(KDDataset(train_df, teacher_tok, student_tok, MAX_LEN),\n                          batch_size=BATCH_SIZE, shuffle=True,\n                          collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nval_loader = DataLoader(KDDataset(val_df, teacher_tok, student_tok, MAX_LEN),\n                        batch_size=BATCH_SIZE,\n                        collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\ntest_loader = DataLoader(KDDataset(test_df, teacher_tok, student_tok, MAX_LEN),\n                         batch_size=BATCH_SIZE,\n                         collate_fn=lambda b: pad_collate(b, teacher_tok.pad_token_id, student_tok.pad_token_id))\nprint(\"‚úÖ KD dataloaders ready (BanglaBERT‚ÜíMiniLM).\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:02:53.804998Z","iopub.execute_input":"2025-11-04T20:02:53.805796Z","iopub.status.idle":"2025-11-04T20:02:54.095836Z","shell.execute_reply.started":"2025-11-04T20:02:53.805770Z","shell.execute_reply":"2025-11-04T20:02:54.095135Z"}},"outputs":[{"name":"stdout","text":"‚úÖ KD dataloaders ready (BanglaBERT‚ÜíMiniLM).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# Student (MiniLM-L6-v2) head (logits + hidden)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import AutoModel\n\nclass StudentClassifier(nn.Module):\n    def __init__(self, base_model_id, num_labels=2, dropout=0.1):\n        super().__init__()\n        self.encoder = AutoModel.from_pretrained(base_model_id)\n        s_H = self.encoder.config.hidden_size            # MiniLM hidden size (often 384)\n        self.s_hidden = s_H\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(s_H, num_labels)\n    def forward(self, input_ids=None, attention_mask=None, **_):\n        out = self.encoder(input_ids=input_ids,\n                           attention_mask=attention_mask,\n                           output_hidden_states=True,\n                           return_dict=True)\n        cls = out.last_hidden_state[:, 0, :]\n        logits = self.fc(self.dropout(cls))\n        return {\"logits\": logits, \"hidden_states\": out.hidden_states}\n\nstudent = StudentClassifier(STUDENT_MODEL_ID).to(DEVICE)\nprint(\"‚úÖ Student initialized. Hidden size =\", student.s_hidden)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:03:00.025491Z","iopub.execute_input":"2025-11-04T20:03:00.025985Z","iopub.status.idle":"2025-11-04T20:03:01.270019Z","shell.execute_reply.started":"2025-11-04T20:03:00.025961Z","shell.execute_reply":"2025-11-04T20:03:01.269338Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1624ede64c7840648c2830909100eb2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"606e2365883f420c88383c98e4b77274"}},"metadata":{}},{"name":"stdout","text":"‚úÖ Student initialized. Hidden size = 384\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"# KD Projection + Loss (CE + KL + HiddenProj)","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n\n# discover sizes\nt_hidden = teacher.config.hidden_size    # XLM-R = 768\ns_hidden = student.s_hidden              # MiniLM-L6-v2 = 384\n\nclass KDProjectionHead(nn.Module):\n    def __init__(self, in_dim, out_dim):\n        super().__init__()\n        self.bridge = nn.Sequential(\n            nn.Linear(in_dim, out_dim),\n            nn.GELU(),\n            nn.LayerNorm(out_dim)\n        )\n    def forward(self, x):\n        return self.bridge(x)\n\nproj_head = KDProjectionHead(s_hidden, t_hidden).to(DEVICE)\n\nclass KDLossProj(nn.Module):\n    def __init__(self, T=3.0, alpha=0.5, gamma_h=1.0):\n        super().__init__()\n        self.T, self.alpha, self.gamma_h = T, alpha, gamma_h\n        self.ce  = nn.CrossEntropyLoss()\n        self.kld = nn.KLDivLoss(reduction=\"batchmean\")\n        self.mse = nn.MSELoss()\n\n    @staticmethod\n    def map_layers(n_s, n_t):\n        # skip embeddings index 0; map hidden layers 1..n\n        s_idx = list(range(1, n_s))  # student hidden_states length includes embeddings at 0\n        t_idx = torch.linspace(1, n_t-1, steps=len(s_idx)).round().long().tolist()\n        return list(zip(s_idx, t_idx))\n\n    def forward(self, s_pack, t_pack, labels):\n        # logits: CE + KL\n        logits_s, logits_t = s_pack[\"logits\"], t_pack[\"logits\"]\n        hard = self.ce(logits_s, labels)\n        soft = self.kld(F.log_softmax(logits_s/self.T, dim=-1),\n                        F.softmax(logits_t/self.T,  dim=-1)) * (self.T**2)\n        loss = (1 - self.alpha)*hard + self.alpha*soft\n\n        # hidden: MSE(proj(student_h), teacher_h) with proportional mapping\n        hs, ht = s_pack.get(\"hidden_states\", []), t_pack.get(\"hidden_states\", [])\n        if hs and ht:\n            pairs = self.map_layers(len(hs), len(ht))\n            h_losses = []\n            for i_s, i_t in pairs:\n                s_h = proj_head(hs[i_s])           # [B, L, t_hidden]\n                t_h = ht[i_t]\n                L = min(s_h.size(1), t_h.size(1))\n                h_losses.append(self.mse(s_h[:, :L, :], t_h[:, :L, :]))\n            if h_losses:\n                loss = loss + GAMMA_HIDDEN * torch.stack(h_losses).mean()\n\n        return loss\n\ncriterion = KDLossProj(T=KD_T, alpha=KD_ALPHA, gamma_h=GAMMA_HIDDEN)\nprint(\"‚úÖ KD loss & projection ready. (student‚Üíteacher dims: {}‚Üí{})\".format(s_hidden, t_hidden))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:03:04.817944Z","iopub.execute_input":"2025-11-04T20:03:04.818282Z","iopub.status.idle":"2025-11-04T20:03:04.834164Z","shell.execute_reply.started":"2025-11-04T20:03:04.818244Z","shell.execute_reply":"2025-11-04T20:03:04.833383Z"}},"outputs":[{"name":"stdout","text":"‚úÖ KD loss & projection ready. (student‚Üíteacher dims: 384‚Üí768)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# KD Training (teacher frozen)","metadata":{}},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\nimport numpy as np\n\n# freeze teacher for KD\nteacher.eval()\nfor p in teacher.parameters(): p.requires_grad = False\n\nopt = AdamW(list(student.parameters()) + list(proj_head.parameters()),\n            lr=LR_STUDENT, weight_decay=WEIGHT_DECAY_S)\nnum_steps = EPOCHS_STUDENT * len(train_loader)\nsch = get_linear_schedule_with_warmup(opt, int(WARMUP_RATIO_S * num_steps), num_steps)\n\ndef metrics(preds, gold):\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\"),\n    }\n\n@torch.no_grad()\ndef eval_student(loader):\n    student.eval(); proj_head.eval()\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        out = student(input_ids=b[\"s_input_ids\"], attention_mask=b[\"s_attention_mask\"])\n        preds += out[\"logits\"].argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return metrics(np.array(preds), np.array(gold))\n\nbest_f1, wait = -1.0, 0\n\nfor ep in range(1, EPOCHS_STUDENT+1):\n    student.train(); proj_head.train()\n    run = 0.0\n\n    for b in tqdm(train_loader, desc=f\"[KD Epoch {ep}/{EPOCHS_STUDENT}]\"):\n        labels = b[\"labels\"].to(DEVICE)\n\n        s_out = student(input_ids=b[\"s_input_ids\"].to(DEVICE),\n                        attention_mask=b[\"s_attention_mask\"].to(DEVICE),\n                        )\n\n        with torch.no_grad():\n            t_raw = teacher(input_ids=b[\"t_input_ids\"].to(DEVICE),\n                            attention_mask=b[\"t_attention_mask\"].to(DEVICE),\n                            output_hidden_states=True,\n                            return_dict=True)\n            t_out = {\"logits\": t_raw.logits, \"hidden_states\": t_raw.hidden_states}\n\n        loss = criterion(s_out, t_out, labels)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(list(student.parameters()) + list(proj_head.parameters()), 1.0)\n        opt.step(); sch.step(); opt.zero_grad()\n        run += loss.item()\n\n    val = eval_student(val_loader)\n    print(f\"[KD] loss={run/len(train_loader):.4f} | Val Acc={val['accuracy']:.4f} | \"\n          f\"F1m={val['f1_macro']:.4f} | F1w={val['f1_weighted']:.4f}\")\n\n    if val[\"f1_macro\"] > best_f1:\n        best_f1, wait = val[\"f1_macro\"], 0\n        torch.save({\"student\": student.state_dict(), \"proj\": proj_head.state_dict()},\n                   WORK_DIR / \"student_minilm_kd_best.pt\")\n        print(\"üíæ Saved best student.\")\n    else:\n        wait += 1\n        if wait >= PATIENCE:\n            print(\"‚è∏Ô∏è Early stopping.\")\n            break\n\n# reload best\nckpt = torch.load(WORK_DIR / \"student_minilm_kd_best.pt\", map_location=DEVICE)\nstudent.load_state_dict(ckpt[\"student\"]); proj_head.load_state_dict(ckpt[\"proj\"])\nstudent.eval(); proj_head.eval()\nprint(\"‚úÖ KD complete & best reloaded.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:03:10.722619Z","iopub.execute_input":"2025-11-04T20:03:10.723113Z","iopub.status.idle":"2025-11-04T20:13:53.334620Z","shell.execute_reply.started":"2025-11-04T20:03:10.723091Z","shell.execute_reply":"2025-11-04T20:13:53.333696Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 1/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e85e12dd36fc4d0bbf3ee7af182c9842"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=1.8668 | Val Acc=0.9170 | F1m=0.8952 | F1w=0.9162\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 2/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36b4be58d54f4446aa760c8410147a67"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=1.0859 | Val Acc=0.9136 | F1m=0.8837 | F1w=0.9096\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 3/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"509d9e8aaea040c596c1b02452a8a8f0"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.9249 | Val Acc=0.9399 | F1m=0.9247 | F1w=0.9396\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 4/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2962639ba384496cae71022a0cd81f26"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.8058 | Val Acc=0.9458 | F1m=0.9331 | F1w=0.9459\nüíæ Saved best student.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"[KD Epoch 5/5]:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1d058f3e7fe4e408d0935278a80bbed"}},"metadata":{}},{"name":"stdout","text":"[KD] loss=0.7441 | Val Acc=0.9450 | F1m=0.9322 | F1w=0.9451\n‚úÖ KD complete & best reloaded.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"# Test Metrics + Alignment + Save","metadata":{}},{"cell_type":"code","source":"from scipy.special import softmax\nfrom scipy.spatial.distance import cosine\n\n@torch.no_grad()\ndef eval_model(model, loader, mode=\"student\"):\n    preds, gold = [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        inp = {\"input_ids\": b[\"t_input_ids\"], \"attention_mask\": b[\"t_attention_mask\"]} if mode==\"teacher\" else \\\n              {\"input_ids\": b[\"s_input_ids\"], \"attention_mask\": b[\"s_attention_mask\"]}\n        out = model(**inp)\n        logits = out[\"logits\"] if isinstance(out, dict) else out.logits\n        preds += logits.argmax(-1).cpu().tolist()\n        gold  += b[\"labels\"].cpu().tolist()\n    return {\n        \"accuracy\": accuracy_score(gold, preds),\n        \"f1_macro\": f1_score(gold, preds, average=\"macro\"),\n        \"f1_weighted\": f1_score(gold, preds, average=\"weighted\")\n    }\n\nprint(\"üß™ Evaluating on test‚Ä¶\")\nteacher_test = eval_model(teacher, test_loader, mode=\"teacher\")\nstudent_test = eval_model(student, test_loader, mode=\"student\")\nprint(\"[Teacher][Test]:\", teacher_test)\nprint(\"[Student][Test]:\", student_test)\n\n@torch.no_grad()\ndef alignment_metrics(teacher, student, loader):\n    cos_list, corr_list, agree = [], [], []\n    for b in loader:\n        b = {k:v.to(DEVICE) for k,v in b.items()}\n        t = teacher(b[\"t_input_ids\"], b[\"t_attention_mask\"])\n        s = student(b[\"s_input_ids\"], b[\"s_attention_mask\"])\n        t_logits = t.logits.detach().cpu().numpy()\n        s_logits = s[\"logits\"].detach().cpu().numpy()\n        t_probs  = softmax(t_logits, axis=-1)\n        s_probs  = softmax(s_logits, axis=-1)\n        for tl, sl, tp, sp in zip(t_logits, s_logits, t_probs, s_probs):\n            cos_list.append(1 - cosine(tl, sl))\n            corr_list.append(np.corrcoef(tp, sp)[0, 1])\n            agree.append(np.argmax(tp) == np.argmax(sp))\n    return {\n        \"logit_cosine\": float(np.nanmean(cos_list)),\n        \"prob_corr\": float(np.nanmean(corr_list)),\n        \"pred_alignment\": float(np.mean(agree))\n    }\n\nalign = alignment_metrics(teacher, student, test_loader)\nprint(f\"\"\"\nüß© Alignment (Test)\n  ‚Ä¢ Logit cosine : {align['logit_cosine']:.4f}\n  ‚Ä¢ Prob corr    : {align['prob_corr']:.4f}\n  ‚Ä¢ Agreement    : {align['pred_alignment']:.4f}\n\"\"\")\n\n# ---- Save artifacts\nSAVE_DIR = WORK_DIR / \"student_minilm_translit_kd_proj\"\nSAVE_DIR.mkdir(parents=True, exist_ok=True)\ntorch.save(student.state_dict(), SAVE_DIR / \"pytorch_model.bin\")\nfrom transformers import AutoTokenizer\n# student tokenizer saving\nAutoTokenizer.from_pretrained(STUDENT_MODEL_ID).save_pretrained(SAVE_DIR)\n\nmeta = {\n    \"teacher_model\": TEACHER_MODEL_ID,\n    \"student_model\": STUDENT_MODEL_ID,\n    \"kd_temperature\": KD_T,\n    \"alpha\": KD_ALPHA,\n    \"gamma_hidden\": GAMMA_HIDDEN,\n    \"max_len\": MAX_LEN,\n    \"lr_student\": LR_STUDENT,\n    \"epochs_student\": EPOCHS_STUDENT\n}\njson.dump(meta, open(SAVE_DIR / \"student_config.json\", \"w\"), indent=2, ensure_ascii=False)\n\ndef to_py(o):\n    if isinstance(o, dict): return {k: to_py(v) for k,v in o.items()}\n    if hasattr(o, \"item\"): return o.item()\n    return o\n\njson.dump({\"teacher_test\": to_py(teacher_test),\n           \"student_test\": to_py(student_test),\n           \"alignment\": to_py(align)},\n          open(WORK_DIR / \"metrics_minilm_kd_proj.json\", \"w\"), indent=2, ensure_ascii=False)\n\nprint(\"‚úÖ Saved student + metrics to:\", SAVE_DIR, \"and\", WORK_DIR / \"metrics_minilm_kd_proj.json\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:07.674017Z","iopub.execute_input":"2025-11-04T20:14:07.674666Z","iopub.status.idle":"2025-11-04T20:14:31.304840Z","shell.execute_reply.started":"2025-11-04T20:14:07.674646Z","shell.execute_reply":"2025-11-04T20:14:31.303985Z"}},"outputs":[{"name":"stdout","text":"üß™ Evaluating on test‚Ä¶\n[Teacher][Test]: {'accuracy': 0.9686706181202371, 'f1_macro': 0.961136147554033, 'f1_weighted': 0.9686561287537637}\n[Student][Test]: {'accuracy': 0.9458086367485182, 'f1_macro': 0.9329615280911632, 'f1_weighted': 0.9458583354280438}\n\nüß© Alignment (Test)\n  ‚Ä¢ Logit cosine : 0.9036\n  ‚Ä¢ Prob corr    : 0.9069\n  ‚Ä¢ Agreement    : 0.9534\n\n‚úÖ Saved student + metrics to: /kaggle/working/student_minilm_translit_kd_proj and /kaggle/working/metrics_minilm_kd_proj.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Load the Student Model ","metadata":{}},{"cell_type":"code","source":"import torch\nfrom pathlib import Path\n\nckpt_path = Path(\"/kaggle/working/student_minilm_kd_best.pt\")\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\n\n# find actual state_dict\nfor key in [\"student\", \"model\", \"state_dict\"]:\n    if key in ckpt:\n        state_dict = ckpt[key]\n        print(f\"Found state_dict under '{key}'\")\n        break\nelse:\n    state_dict = ckpt\n    print(\"No wrapper key found; using raw checkpoint\")\n\n# show first 40 parameter names\nprint(\"\\n\".join(list(state_dict.keys())[:40]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:40.344511Z","iopub.execute_input":"2025-11-04T20:14:40.344827Z","iopub.status.idle":"2025-11-04T20:14:40.431687Z","shell.execute_reply.started":"2025-11-04T20:14:40.344806Z","shell.execute_reply":"2025-11-04T20:14:40.430941Z"}},"outputs":[{"name":"stdout","text":"Found state_dict under 'student'\nencoder.embeddings.word_embeddings.weight\nencoder.embeddings.position_embeddings.weight\nencoder.embeddings.token_type_embeddings.weight\nencoder.embeddings.LayerNorm.weight\nencoder.embeddings.LayerNorm.bias\nencoder.encoder.layer.0.attention.self.query.weight\nencoder.encoder.layer.0.attention.self.query.bias\nencoder.encoder.layer.0.attention.self.key.weight\nencoder.encoder.layer.0.attention.self.key.bias\nencoder.encoder.layer.0.attention.self.value.weight\nencoder.encoder.layer.0.attention.self.value.bias\nencoder.encoder.layer.0.attention.output.dense.weight\nencoder.encoder.layer.0.attention.output.dense.bias\nencoder.encoder.layer.0.attention.output.LayerNorm.weight\nencoder.encoder.layer.0.attention.output.LayerNorm.bias\nencoder.encoder.layer.0.intermediate.dense.weight\nencoder.encoder.layer.0.intermediate.dense.bias\nencoder.encoder.layer.0.output.dense.weight\nencoder.encoder.layer.0.output.dense.bias\nencoder.encoder.layer.0.output.LayerNorm.weight\nencoder.encoder.layer.0.output.LayerNorm.bias\nencoder.encoder.layer.1.attention.self.query.weight\nencoder.encoder.layer.1.attention.self.query.bias\nencoder.encoder.layer.1.attention.self.key.weight\nencoder.encoder.layer.1.attention.self.key.bias\nencoder.encoder.layer.1.attention.self.value.weight\nencoder.encoder.layer.1.attention.self.value.bias\nencoder.encoder.layer.1.attention.output.dense.weight\nencoder.encoder.layer.1.attention.output.dense.bias\nencoder.encoder.layer.1.attention.output.LayerNorm.weight\nencoder.encoder.layer.1.attention.output.LayerNorm.bias\nencoder.encoder.layer.1.intermediate.dense.weight\nencoder.encoder.layer.1.intermediate.dense.bias\nencoder.encoder.layer.1.output.dense.weight\nencoder.encoder.layer.1.output.dense.bias\nencoder.encoder.layer.1.output.LayerNorm.weight\nencoder.encoder.layer.1.output.LayerNorm.bias\nencoder.encoder.layer.2.attention.self.query.weight\nencoder.encoder.layer.2.attention.self.query.bias\nencoder.encoder.layer.2.attention.self.key.weight\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"# Load the Student Model","metadata":{}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\nimport torch\nfrom pathlib import Path\n\nWORK_DIR = Path(\"/kaggle/working\")\nDEVICE = \"cpu\"\n\nckpt_path = WORK_DIR / \"student_minilm_kd_best.pt\"\nbase_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n\nckpt = torch.load(ckpt_path, map_location=\"cpu\")\nstate_dict = ckpt[\"student\"] if \"student\" in ckpt else ckpt.get(\"model\", ckpt.get(\"state_dict\", ckpt))\n\ndef remap_key(k: str) -> str | None:\n    # drop KD projection heads entirely\n    if k.startswith(\"proj.\") or \"projection\" in k:\n        return None\n    # classifier\n    if k.startswith(\"fc.\"):\n        return k.replace(\"fc.\", \"classifier.\")\n    # core renames from your checkpoint structure ‚Üí HF structure\n    k = k.replace(\"encoder.encoder.\", \"bert.encoder.\")      # encoder blocks\n    k = k.replace(\"encoder.embeddings.\", \"bert.embeddings.\")# embeddings\n    k = k.replace(\"encoder.pooler.\", \"bert.pooler.\")        # <-- FIXED: pooler\n    # handle odd double nesting we saw earlier\n    k = k.replace(\"bert.bert.\", \"bert.encoder.\")\n    k = k.replace(\"bert.encoder.encoder.\", \"bert.encoder.\")\n    return k\n\nfixed_state = {}\nfor k, v in state_dict.items():\n    nk = remap_key(k)\n    if nk is not None:\n        fixed_state[nk] = v\n\n# build 2-class MiniLM and load\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n\nmissing, unexpected = model.load_state_dict(fixed_state, strict=False)\n\nprint(\"‚úÖ MiniLM KD (2-class) loaded with correct mapping.\")\nprint(\"Missing keys:\", missing)\nprint(\"Unexpected keys:\", unexpected)\n\nmodel.to(DEVICE).eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:44.017196Z","iopub.execute_input":"2025-11-04T20:14:44.017519Z","iopub.status.idle":"2025-11-04T20:14:44.371817Z","shell.execute_reply.started":"2025-11-04T20:14:44.017496Z","shell.execute_reply":"2025-11-04T20:14:44.371019Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ MiniLM KD (2-class) loaded with correct mapping.\nMissing keys: []\nUnexpected keys: []\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 384, padding_idx=0)\n      (position_embeddings): Embedding(512, 384)\n      (token_type_embeddings): Embedding(2, 384)\n      (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-5): 6 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSdpaSelfAttention(\n              (query): Linear(in_features=384, out_features=384, bias=True)\n              (key): Linear(in_features=384, out_features=384, bias=True)\n              (value): Linear(in_features=384, out_features=384, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=384, out_features=384, bias=True)\n              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=384, out_features=1536, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=1536, out_features=384, bias=True)\n            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=384, out_features=384, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=384, out_features=2, bias=True)\n)"},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"CLEAN_DIR = WORK_DIR / \"student_minilm_kd_clean\"\nCLEAN_DIR.mkdir(parents=True, exist_ok=True)\nmodel.save_pretrained(CLEAN_DIR)\ntokenizer.save_pretrained(CLEAN_DIR)\nprint(\"Saved to:\", CLEAN_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:49.040909Z","iopub.execute_input":"2025-11-04T20:14:49.041770Z","iopub.status.idle":"2025-11-04T20:14:49.206128Z","shell.execute_reply.started":"2025-11-04T20:14:49.041736Z","shell.execute_reply":"2025-11-04T20:14:49.205414Z"}},"outputs":[{"name":"stdout","text":"Saved to: /kaggle/working/student_minilm_kd_clean\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"# prune ","metadata":{}},{"cell_type":"code","source":"import torch.nn.utils.prune as prune\n\namount = 0.30  # 30% unstructured weight pruning\nfor m in model.modules():\n    if isinstance(m, torch.nn.Linear):\n        prune.l1_unstructured(m, name=\"weight\", amount=amount)\n\n# Remove reparam so weights are real tensors\nfor m in model.modules():\n    if isinstance(m, torch.nn.Linear) and hasattr(m, \"weight_orig\"):\n        prune.remove(m, \"weight\")\n\nPRUNED_DIR = WORK_DIR / \"student_minilm_pruned\"\nPRUNED_DIR.mkdir(exist_ok=True, parents=True)\nmodel.save_pretrained(PRUNED_DIR)\ntokenizer.save_pretrained(PRUNED_DIR)\nprint(\"Pruned model saved to:\", PRUNED_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:51.649813Z","iopub.execute_input":"2025-11-04T20:14:51.650397Z","iopub.status.idle":"2025-11-04T20:14:52.340687Z","shell.execute_reply.started":"2025-11-04T20:14:51.650373Z","shell.execute_reply":"2025-11-04T20:14:52.339873Z"}},"outputs":[{"name":"stdout","text":"Pruned model saved to: /kaggle/working/student_minilm_pruned\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nteacher_path = \"/kaggle/working/finetuned_banglabert\"  # your real teacher ckpt path\nteacher = AutoModelForSequenceClassification.from_pretrained(teacher_path)\nteacher.to(DEVICE).eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:55.853898Z","iopub.execute_input":"2025-11-04T20:14:55.854512Z","iopub.status.idle":"2025-11-04T20:14:55.918496Z","shell.execute_reply.started":"2025-11-04T20:14:55.854490Z","shell.execute_reply":"2025-11-04T20:14:55.917877Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"ElectraForSequenceClassification(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): ElectraClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (activation): GELUActivation()\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n  )\n)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"batch = next(iter(train_loader))\nprint(\"Batch keys:\", batch.keys())\nfor k, v in batch.items():\n    print(f\"{k}: shape {v.shape if torch.is_tensor(v) else type(v)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:14:59.895129Z","iopub.execute_input":"2025-11-04T20:14:59.895871Z","iopub.status.idle":"2025-11-04T20:14:59.915375Z","shell.execute_reply.started":"2025-11-04T20:14:59.895843Z","shell.execute_reply":"2025-11-04T20:14:59.914685Z"}},"outputs":[{"name":"stdout","text":"Batch keys: dict_keys(['t_input_ids', 't_attention_mask', 's_input_ids', 's_attention_mask', 'labels'])\nt_input_ids: shape torch.Size([16, 128])\nt_attention_mask: shape torch.Size([16, 128])\ns_input_ids: shape torch.Size([16, 128])\ns_attention_mask: shape torch.Size([16, 128])\nlabels: shape torch.Size([16])\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"# finetune for Accuracy Recovery","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom tqdm.auto import tqdm\nfrom sklearn.metrics import accuracy_score, f1_score\n\n# ====== DEVICE & CONFIG ======\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nEPOCHS = 5\nLR = 2e-5\nALPHA = 0.7        # CE vs KD (logits)\nBETA = 0.3         # hidden-state KD weight\nTEMPERATURE = 3.0\n\nprint(\"‚úÖ Using device:\", DEVICE, \"| GPU:\", torch.cuda.get_device_name(0) if DEVICE.type==\"cuda\" else \"CPU\")\n\n# ====== SETUP ======\nteacher.to(DEVICE).eval()\nmodel.to(DEVICE).train()\n\n# Projection layer: ELECTRA(768) ‚Üí MiniLM(384)\nproj = nn.Linear(768, 384).to(DEVICE)\nmse = nn.MSELoss()\n\nopt = torch.optim.AdamW(\n    list(model.parameters()) + list(proj.parameters()),\n    lr=LR\n)\nscaler = torch.cuda.amp.GradScaler()   # ‚úÖ Mixed precision scaler\n\n# ====== TRAINING LOOP ======\nfor epoch in range(1, EPOCHS + 1):\n    model.train()\n    proj.train()\n    tot = 0\n\n    for batch in tqdm(train_loader, desc=f\"KD Recovery Epoch {epoch}/{EPOCHS}\"):\n        # Move batch to GPU\n        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n\n        with torch.cuda.amp.autocast():  # ‚úÖ FP16 mixed precision\n            # --- Forward (Student) ---\n            out_s = model(\n                input_ids=batch[\"s_input_ids\"],\n                attention_mask=batch[\"s_attention_mask\"],\n                output_hidden_states=True\n            )\n            logits_s = out_s.logits\n            s_hidden = out_s.hidden_states[-1]  # [B, T, 384]\n\n            # --- Forward (Teacher) ---\n            with torch.no_grad():\n                out_t = teacher(\n                    input_ids=batch[\"t_input_ids\"],\n                    attention_mask=batch[\"t_attention_mask\"],\n                    output_hidden_states=True\n                )\n                logits_t = out_t.logits\n                t_hidden = out_t.hidden_states[-1]  # [B, T, 768]\n\n            # --- Compute Losses ---\n            feat_loss = mse(s_hidden, proj(t_hidden))\n            kd_loss = F.kl_div(\n                F.log_softmax(logits_s / TEMPERATURE, dim=-1),\n                F.softmax(logits_t / TEMPERATURE, dim=-1),\n                reduction=\"batchmean\"\n            ) * (TEMPERATURE ** 2)\n            ce_loss = F.cross_entropy(logits_s, batch[\"labels\"])\n            loss = ALPHA * ce_loss + (1 - ALPHA) * kd_loss + BETA * feat_loss\n\n        opt.zero_grad()\n        scaler.scale(loss).backward()\n        scaler.step(opt)\n        scaler.update()\n\n        tot += loss.item()\n\n    print(f\"Epoch {epoch}: avg_loss = {tot / len(train_loader):.4f}\")\n    torch.cuda.empty_cache()  # ‚úÖ free VRAM between epochs\n\n# ====== SAVE MODEL ======\nREC_DIR = WORK_DIR / \"student_minilm_proj_kd_recovered_gpu\"\nREC_DIR.mkdir(parents=True, exist_ok=True)\n\nmodel.cpu().save_pretrained(REC_DIR)\ntokenizer.save_pretrained(REC_DIR)\ntorch.save(proj.state_dict(), REC_DIR / \"proj_layer.pt\")\n\nprint(\"‚úÖ Projection-KD MiniLM student saved to:\", REC_DIR)\n\n# ====== EVALUATION ======\nmodel.eval().to(DEVICE)\nproj.eval().to(DEVICE)\ny_true, y_pred = [], []\n\nfor batch in tqdm(test_loader, desc=\"Evaluating\"):\n    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        logits = model(\n            input_ids=batch[\"s_input_ids\"],\n            attention_mask=batch[\"s_attention_mask\"]\n        ).logits\n\n    preds = logits.argmax(-1).cpu().numpy()\n    y_pred.extend(preds)\n    y_true.extend(batch[\"labels\"].cpu().numpy())\n\nacc = accuracy_score(y_true, y_pred)\nf1 = f1_score(y_true, y_pred, average=\"macro\")\nprint(f\"‚úÖ Test Accuracy: {acc:.4f} | Macro F1: {f1:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:15:02.991672Z","iopub.execute_input":"2025-11-04T20:15:02.992473Z","iopub.status.idle":"2025-11-04T20:18:42.630895Z","shell.execute_reply.started":"2025-11-04T20:15:02.992450Z","shell.execute_reply":"2025-11-04T20:18:42.630234Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Using device: cuda | GPU: Tesla T4\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_37/2362264538.py:29: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = torch.cuda.amp.GradScaler()   # ‚úÖ Mixed precision scaler\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 1/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96c9a89a6af24f37b7c0220983165d33"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_37/2362264538.py:41: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.cuda.amp.autocast():  # ‚úÖ FP16 mixed precision\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: avg_loss = 0.2656\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 2/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4d05f3c816024234bdec1288a69df674"}},"metadata":{}},{"name":"stdout","text":"Epoch 2: avg_loss = 0.1508\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 3/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"09e67b7334ec40c0a8f953d8c2817a23"}},"metadata":{}},{"name":"stdout","text":"Epoch 3: avg_loss = 0.1125\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 4/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"314efdb55f5f4cae84470bcdf9fc47a9"}},"metadata":{}},{"name":"stdout","text":"Epoch 4: avg_loss = 0.0993\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"KD Recovery Epoch 5/5:   0%|          | 0/591 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70c75f8d517446f5954703e0e4199c15"}},"metadata":{}},{"name":"stdout","text":"Epoch 5: avg_loss = 0.0864\n‚úÖ Projection-KD MiniLM student saved to: /kaggle/working/student_minilm_proj_kd_recovered_gpu\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0903237752e64e62b5853084aba8adb5"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_37/2362264538.py:99: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with torch.no_grad(), torch.cuda.amp.autocast():\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Test Accuracy: 0.9500 | Macro F1: 0.9383\n","output_type":"stream"}],"execution_count":16},{"cell_type":"markdown","source":"# Load Fine-Tuned Recovered Student","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoTokenizer\nimport torch, os\nfrom pathlib import Path\n\nWORK_DIR = Path(\"/kaggle/working\")\nDEPLOY_DIR = WORK_DIR / \"student_minilm_proj_kd_recovered_gpu\"\n\n# Load fine-tuned MiniLM (post-KD recovery)\nmodel = AutoModelForSequenceClassification.from_pretrained(DEPLOY_DIR)\ntokenizer = AutoTokenizer.from_pretrained(DEPLOY_DIR)\n\nprint(\"‚úÖ Loaded fine-tuned MiniLM student for deployment\")\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Device:\", DEVICE)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:18:52.545761Z","iopub.execute_input":"2025-11-04T20:18:52.546037Z","iopub.status.idle":"2025-11-04T20:18:52.626379Z","shell.execute_reply.started":"2025-11-04T20:18:52.546017Z","shell.execute_reply":"2025-11-04T20:18:52.625616Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Loaded fine-tuned MiniLM student for deployment\nDevice: cuda\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"# Apply Dynamic Quantization (int8)","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.quantization\n\nmodel.eval().cpu()  # move to CPU for quantization\n\nquantized_model = torch.quantization.quantize_dynamic(\n    model, {nn.Linear}, dtype=torch.qint8\n)\n\ntorch.save(quantized_model.state_dict(), DEPLOY_DIR / \"minilm_quantized.pt\")\nprint(\"‚úÖ Quantized (int8) model ready ‚Äî much smaller & faster on CPU\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:18:55.636591Z","iopub.execute_input":"2025-11-04T20:18:55.637219Z","iopub.status.idle":"2025-11-04T20:18:56.019234Z","shell.execute_reply.started":"2025-11-04T20:18:55.637186Z","shell.execute_reply":"2025-11-04T20:18:56.018326Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Quantized (int8) model ready ‚Äî much smaller & faster on CPU\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# Export to ONNX (for edge / mobile inference)","metadata":{}},{"cell_type":"code","source":"import torch\n\n# use the fine-tuned (FP32) model, not quantized one\nexport_model = model.eval().cpu()\n\n# Example input\nsample = tokenizer(\n    \"‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ!\", \n    padding=\"max_length\", \n    truncation=True, \n    max_length=128, \n    return_tensors=\"pt\"\n)\n\nonnx_path = DEPLOY_DIR / \"student_minilm_fp32.onnx\"\n\ntorch.onnx.export(\n    export_model,\n    (sample[\"input_ids\"], sample[\"attention_mask\"]),\n    onnx_path,\n    input_names=[\"input_ids\", \"attention_mask\"],\n    output_names=[\"logits\"],\n    dynamic_axes={\"input_ids\": {0: \"batch\"}, \"attention_mask\": {0: \"batch\"}},\n    opset_version=17,\n    do_constant_folding=True\n)\n\nprint(\"‚úÖ Exported MiniLM (FP32) ONNX model:\", onnx_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:18:59.337773Z","iopub.execute_input":"2025-11-04T20:18:59.338409Z","iopub.status.idle":"2025-11-04T20:19:01.056016Z","shell.execute_reply.started":"2025-11-04T20:18:59.338383Z","shell.execute_reply":"2025-11-04T20:19:01.055309Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Exported MiniLM (FP32) ONNX model: /kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_fp32.onnx\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"# Load & Run ONNX Model with ONNXRuntime","metadata":{}},{"cell_type":"code","source":"!pip install -q onnxruntime\n\nimport onnxruntime as ort\nimport numpy as np\n\n# Load ONNX model\nonnx_sess = ort.InferenceSession(str(onnx_path), providers=[\"CPUExecutionProvider\"])\n\n# Prepare input for ONNX\ninputs_onnx = {\n    \"input_ids\": sample[\"input_ids\"].numpy(),\n    \"attention_mask\": sample[\"attention_mask\"].numpy()\n}\n\n# Run inference\noutputs = onnx_sess.run([\"logits\"], inputs_onnx)\npred = np.argmax(outputs[0], axis=-1)\nprint(\"‚úÖ ONNX inference output logits:\", outputs[0])\nprint(\"‚úÖ Predicted class:\", pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:19:03.668336Z","iopub.execute_input":"2025-11-04T20:19:03.669083Z","iopub.status.idle":"2025-11-04T20:19:09.240351Z","shell.execute_reply.started":"2025-11-04T20:19:03.669057Z","shell.execute_reply":"2025-11-04T20:19:09.239392Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.4/17.4 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h‚úÖ ONNX inference output logits: [[-2.5146873  2.3884728]]\n‚úÖ Predicted class: [1]\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"from onnxruntime.quantization import quantize_dynamic, QuantType\nfrom pathlib import Path\n\nfp32_path = DEPLOY_DIR / \"student_minilm_fp32.onnx\"\nint8_path = DEPLOY_DIR / \"student_minilm_int8.onnx\"\n\nquantize_dynamic(\n    model_input=str(fp32_path),\n    model_output=str(int8_path),\n    weight_type=QuantType.QInt8\n)\n\nprint(\"‚úÖ Quantized ONNX model saved at:\", int8_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:19:12.590072Z","iopub.execute_input":"2025-11-04T20:19:12.590582Z","iopub.status.idle":"2025-11-04T20:19:14.396470Z","shell.execute_reply.started":"2025-11-04T20:19:12.590549Z","shell.execute_reply":"2025-11-04T20:19:14.395321Z"}},"outputs":[{"name":"stdout","text":"‚úÖ Quantized ONNX model saved at: /kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_int8.onnx\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Compare Model File Sizes","metadata":{}},{"cell_type":"code","source":"import os\nfrom pathlib import Path\n\n# Adjust to your actual directory\nREC_DIR = Path(\"/kaggle/working/student_minilm_proj_kd_recovered_gpu\")\nDEPLOY_DIR = REC_DIR  # since all your exports are inside same folder\n\n# Common possible file names\n\nonnx_fp32_path    = DEPLOY_DIR / \"student_minilm_fp32.onnx\"  # ONNX FP32 export\nonnx_int8_path    = DEPLOY_DIR / \"student_minilm_int8.onnx\"  # Quantized ONNX\n\ndef get_size(path):\n    try:\n        size_mb = os.path.getsize(path) / (1024 * 1024)\n        return f\"{size_mb:.2f} MB\"\n    except FileNotFoundError:\n        return \"‚ùå Not found\"\n\nprint(\"üì¶ Model Size Comparison\")\n\nprint(f\"ONNX FP32:   {get_size(onnx_fp32_path)}\")\nprint(f\"ONNX INT8:   {get_size(onnx_int8_path)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:19:16.861872Z","iopub.execute_input":"2025-11-04T20:19:16.862147Z","iopub.status.idle":"2025-11-04T20:19:16.868470Z","shell.execute_reply.started":"2025-11-04T20:19:16.862127Z","shell.execute_reply":"2025-11-04T20:19:16.867609Z"}},"outputs":[{"name":"stdout","text":"üì¶ Model Size Comparison\nONNX FP32:   86.78 MB\nONNX INT8:   21.98 MB\n","output_type":"stream"}],"execution_count":22},{"cell_type":"markdown","source":"# Benchmark ONNX FP32 vs INT8 Models","metadata":{}},{"cell_type":"code","source":"import time, onnxruntime as ort, numpy as np, pandas as pd\nfrom tqdm.auto import tqdm\n\n# ---------------------------------------------------------------------\n#  ‚úÖ CONFIG\n# ---------------------------------------------------------------------\nDEVICE = \"cpu\"\n\n# Example Bangla sentences for inference\ntexts = [\n    \"‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ!\",         # positive\n    \"‡¶è‡¶á ‡¶∞‡ßá‡¶∏‡ßç‡¶§‡ßã‡¶∞‡¶æ‡¶Å‡¶Ø‡¶º ‡¶Ü‡¶∞ ‡¶ï‡¶ñ‡¶®‡ßã ‡¶Ø‡¶æ‡¶¨ ‡¶®‡¶æ\"     # negative\n]\n\nonnx_fp32_path = DEPLOY_DIR / \"student_minilm_fp32.onnx\"\nonnx_int8_path = DEPLOY_DIR / \"student_minilm_int8.onnx\"\n\n# ---------------------------------------------------------------------\n#  üîπ Benchmark helper\n# ---------------------------------------------------------------------\ndef benchmark_onnx(model_path, text_list, runs=20):\n    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n    inputs = tokenizer(text_list, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n    start = time.time()\n    for _ in range(runs):\n        _ = sess.run([\"logits\"], {\n            \"input_ids\": inputs[\"input_ids\"],\n            \"attention_mask\": inputs[\"attention_mask\"]\n        })\n    end = time.time()\n    return (end - start) / runs * 1000  # average latency (ms)\n\n# ---------------------------------------------------------------------\n#  üîπ Run latency benchmarks\n# ---------------------------------------------------------------------\nlat_onnx_fp32 = benchmark_onnx(onnx_fp32_path, texts)\nlat_onnx_int8 = benchmark_onnx(onnx_int8_path, texts)\n\n# ---------------------------------------------------------------------\n#  üîπ Inference results (for demonstration)\n# ---------------------------------------------------------------------\ndef predict_onnx(model_path, text):\n    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n    inputs = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n    logits = sess.run([\"logits\"], {\n        \"input_ids\": inputs[\"input_ids\"],\n        \"attention_mask\": inputs[\"attention_mask\"]\n    })[0]\n    pred = np.argmax(logits, axis=-1).item()\n    return pred, logits\n\nfor t in texts:\n    p_fp32, l_fp32 = predict_onnx(onnx_fp32_path, t)\n    p_int8, l_int8 = predict_onnx(onnx_int8_path, t)\n    print(f\"\\nüìù Text: {t}\")\n    print(f\"ONNX FP32 ‚Üí Pred: {p_fp32}, Logits: {np.round(l_fp32, 3)}\")\n    print(f\"ONNX INT8 ‚Üí Pred: {p_int8}, Logits: {np.round(l_int8, 3)}\")\n\n# ---------------------------------------------------------------------\n#  üîπ Summary Table\n# ---------------------------------------------------------------------\ndata = {\n    \"Model\": [\"ONNX FP32\", \"ONNX INT8\"],\n    \"Size (MB)\": [86.78, 21.98],      # from your previous results\n    \"Latency (ms)\": [lat_onnx_fp32, lat_onnx_int8],\n    \"Notes\": [\n        \"Baseline full-precision export\",\n        \"Quantized deployment (fast + compact)\"\n    ]\n}\n\nsummary = pd.DataFrame(data)\nprint(\"\\nüèÅ Deployment Performance Summary:\")\ndisplay(summary)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T20:19:20.701496Z","iopub.execute_input":"2025-11-04T20:19:20.702096Z","iopub.status.idle":"2025-11-04T20:19:23.237985Z","shell.execute_reply.started":"2025-11-04T20:19:20.702071Z","shell.execute_reply":"2025-11-04T20:19:23.237300Z"}},"outputs":[{"name":"stdout","text":"\nüìù Text: ‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ!\nONNX FP32 ‚Üí Pred: 1, Logits: [[-2.515  2.388]]\nONNX INT8 ‚Üí Pred: 1, Logits: [[-1.985  1.908]]\n\nüìù Text: ‡¶è‡¶á ‡¶∞‡ßá‡¶∏‡ßç‡¶§‡ßã‡¶∞‡¶æ‡¶Å‡¶Ø‡¶º ‡¶Ü‡¶∞ ‡¶ï‡¶ñ‡¶®‡ßã ‡¶Ø‡¶æ‡¶¨ ‡¶®‡¶æ\nONNX FP32 ‚Üí Pred: 1, Logits: [[-0.994  1.009]]\nONNX INT8 ‚Üí Pred: 1, Logits: [[-0.437  0.485]]\n\nüèÅ Deployment Performance Summary:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"       Model  Size (MB)  Latency (ms)                                  Notes\n0  ONNX FP32      86.78     40.381682         Baseline full-precision export\n1  ONNX INT8      21.98     33.916891  Quantized deployment (fast + compact)","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Model</th>\n      <th>Size (MB)</th>\n      <th>Latency (ms)</th>\n      <th>Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ONNX FP32</td>\n      <td>86.78</td>\n      <td>40.381682</td>\n      <td>Baseline full-precision export</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ONNX INT8</td>\n      <td>21.98</td>\n      <td>33.916891</td>\n      <td>Quantized deployment (fast + compact)</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"# ","metadata":{}},{"cell_type":"markdown","source":"# Evaluate ONNX FP32 vs INT8 Model Accuracy","metadata":{}},{"cell_type":"code","source":"import onnxruntime as ort\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, f1_score\nfrom tqdm.auto import tqdm\n\n# ------------------------------\n#  Setup ONNX sessions\n# ------------------------------\nsess_fp32 = ort.InferenceSession(str(onnx_fp32_path), providers=[\"CPUExecutionProvider\"])\nsess_int8 = ort.InferenceSession(str(onnx_int8_path), providers=[\"CPUExecutionProvider\"])\n\ny_true, y_fp32, y_int8 = [], [], []\n\n# ------------------------------\n#  Loop over test set\n# ------------------------------\nfor batch in tqdm(test_loader, desc=\"Evaluating ONNX Models\"):\n    # move tensors to CPU and convert to numpy\n    input_ids = batch[\"s_input_ids\"].cpu().numpy()\n    attention_mask = batch[\"s_attention_mask\"].cpu().numpy()\n    labels = batch[\"labels\"].cpu().numpy()\n\n    y_true.extend(labels)\n\n    # FP32 inference\n    logits_fp32 = sess_fp32.run([\"logits\"], {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask\n    })[0]\n    preds_fp32 = np.argmax(logits_fp32, axis=-1)\n    y_fp32.extend(preds_fp32)\n\n    # INT8 inference\n    logits_int8 = sess_int8.run([\"logits\"], {\n        \"input_ids\": input_ids,\n        \"attention_mask\": attention_mask\n    })[0]\n    preds_int8 = np.argmax(logits_int8, axis=-1)\n    y_int8.extend(preds_int8)\n\n# ------------------------------\n#  Compute accuracy & F1\n# ------------------------------\nacc_fp32 = accuracy_score(y_true, y_fp32)\nf1_fp32 = f1_score(y_true, y_fp32, average=\"macro\")\n\nacc_int8 = accuracy_score(y_true, y_int8)\nf1_int8 = f1_score(y_true, y_int8, average=\"macro\")\n\nprint(\"‚úÖ ONNX Model Accuracy Comparison\")\nprint(f\"ONNX FP32 ‚Üí Accuracy: {acc_fp32:.4f} | Macro F1: {f1_fp32:.4f}\")\nprint(f\"ONNX INT8 ‚Üí Accuracy: {acc_int8:.4f} | Macro F1: {f1_int8:.4f}\")\nprint(f\"Œî Accuracy: {(acc_int8 - acc_fp32)*100:.2f}% | Œî F1: {(f1_int8 - f1_fp32)*100:.2f}%\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T19:11:15.229511Z","iopub.execute_input":"2025-11-04T19:11:15.229766Z","iopub.status.idle":"2025-11-04T19:11:56.503253Z","shell.execute_reply.started":"2025-11-04T19:11:15.229751Z","shell.execute_reply":"2025-11-04T19:11:56.502572Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating ONNX Models:   0%|          | 0/74 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c04129b95e545958418f765ed984252"}},"metadata":{}},{"name":"stdout","text":"‚úÖ ONNX Model Accuracy Comparison\nONNX FP32 ‚Üí Accuracy: 0.9458 | Macro F1: 0.9318\nONNX INT8 ‚Üí Accuracy: 0.9416 | Macro F1: 0.9252\nŒî Accuracy: -0.42% | Œî F1: -0.66%\n","output_type":"stream"}],"execution_count":59},{"cell_type":"code","source":"import os, time, torch, numpy as np, pandas as pd, onnxruntime as ort\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# ==========================================================\n# üîß CONFIGURATION\n# ==========================================================\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Running on {DEVICE.upper()}\")\n\nbase_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\ntokenizer = AutoTokenizer.from_pretrained(base_model_name)\n\n# Your model directories\nmodel_paths = {\n    \"Teacher (BanglaBERT)\": \"/kaggle/input/teacher_checkpoint\",\n    \"Student (KD MiniLM)\": \"/kaggle/working/student_minilm_kd_best\",\n    \"Pruned 30%\": \"/kaggle/working/student_minilm_pruned\",\n    \"KD-Recovered\": \"/kaggle/working/student_minilm_proj_kd_recovered_gpu\",\n    \"Quantized (INT8)\": \"/kaggle/working/student_minilm_proj_kd_recovered_gpu/minilm_quantized.pt\",\n    \"ONNX FP32\": \"/kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_fp32.onnx\",\n    \"ONNX INT8\": \"/kaggle/working/student_minilm_proj_kd_recovered_gpu/student_minilm_int8.onnx\"\n}\n\ntexts = [\n    \"‡¶è‡¶ü‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¶‡¶æ‡¶∞‡ßÅ‡¶£ ‡¶Ö‡¶≠‡¶ø‡¶ú‡ßç‡¶û‡¶§‡¶æ!\",\n    \"‡¶è‡¶á ‡¶∏‡¶ø‡¶®‡ßá‡¶Æ‡¶æ‡¶ü‡¶æ ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶¨‡¶æ‡¶ú‡ßá ‡¶õ‡¶ø‡¶≤‡•§\",\n    \"‡¶ó‡¶≤‡ßç‡¶™‡¶ü‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶∏‡ßÅ‡¶®‡ßç‡¶¶‡¶∞‡¶≠‡¶æ‡¶¨‡ßá ‡¶â‡¶™‡¶∏‡ßç‡¶•‡¶æ‡¶™‡¶® ‡¶ï‡¶∞‡¶æ ‡¶π‡¶Ø‡¶º‡ßá‡¶õ‡ßá‡•§\",\n    \"‡¶®‡¶æ‡¶ü‡¶ï‡¶ü‡¶æ‡¶∞ ‡¶ó‡¶≤‡ßç‡¶™ ‡¶è‡¶ï‡¶¶‡¶Æ ‡¶¶‡ßÅ‡¶∞‡ßç‡¶¨‡¶≤, ‡¶ï‡ßã‡¶®‡ßã ‡¶∏‡¶Ç‡¶Ø‡ßã‡¶ó‡¶á ‡¶®‡ßá‡¶á‡•§\"\n]\n\n# Tokenize for PyTorch models\ninputs_pt = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n\n# ==========================================================\n# üß© HELPER FUNCTIONS\n# ==========================================================\ndef get_size(path):\n    \"\"\"Compute model file size in MB.\"\"\"\n    if os.path.isdir(path):\n        total = sum(os.path.getsize(os.path.join(root, f)) for root, _, files in os.walk(path) for f in files)\n        return round(total / (1024 * 1024), 2)\n    elif os.path.exists(path):\n        return round(os.path.getsize(path) / (1024 * 1024), 2)\n    return None\n\ndef benchmark_pytorch(model, inputs, runs=20):\n    \"\"\"Benchmark latency for PyTorch model.\"\"\"\n    model.eval()\n    inputs = {k: v.to(DEVICE) for k, v in inputs.items()}\n    with torch.no_grad():\n        for _ in range(3):  # warmup\n            _ = model(**inputs)\n        start = time.time()\n        for _ in range(runs):\n            _ = model(**inputs)\n        elapsed = time.time() - start\n    avg_ms = (elapsed / runs) * 1000\n    throughput = (len(texts) * runs) / elapsed\n    return round(avg_ms, 2), round(throughput, 2)\n\ndef benchmark_onnx(model_path, texts, runs=20):\n    \"\"\"Benchmark latency for ONNX model.\"\"\"\n    sess = ort.InferenceSession(str(model_path), providers=[\"CPUExecutionProvider\"])\n    total_time = 0\n    for _ in range(runs):\n        for text in texts:\n            inputs = tokenizer(text, return_tensors=\"np\", padding=\"max_length\", truncation=True, max_length=128)\n            start = time.time()\n            _ = sess.run([\"logits\"], {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]})\n            total_time += (time.time() - start)\n    avg_ms = (total_time / (len(texts) * runs)) * 1000\n    throughput = (len(texts) * runs) / total_time\n    return round(avg_ms, 2), round(throughput, 2)\n\n# ==========================================================\n# üöÄ BENCHMARK LOOP\n# ==========================================================\nresults = []\n\nfor name, path in model_paths.items():\n    print(f\"\\nüîç Evaluating: {name}\")\n    try:\n        if name.startswith(\"ONNX\"):\n            latency, throughput = benchmark_onnx(path, texts)\n        elif path.endswith(\".pt\"):\n            # Quantized PyTorch model\n            state_dict = torch.load(path, map_location=DEVICE)\n            model = AutoModelForSequenceClassification.from_pretrained(base_model_name, num_labels=2)\n            model.load_state_dict(state_dict, strict=False)\n            model.to(DEVICE)\n            latency, throughput = benchmark_pytorch(model, inputs_pt)\n        else:\n            model = AutoModelForSequenceClassification.from_pretrained(path)\n            model.to(DEVICE)\n            latency, throughput = benchmark_pytorch(model, inputs_pt)\n\n        results.append({\n            \"Model Variant\": name,\n            \"Size (MB)\": get_size(path),\n            \"Latency (ms/sample)\": latency,\n            \"Throughput (samples/s)\": throughput,\n            \"Notes\": \"‚úì OK\"\n        })\n    except Exception as e:\n        print(f\"‚ö†Ô∏è {name} skipped: {e}\")\n        results.append({\n            \"Model Variant\": name,\n            \"Size (MB)\": get_size(path),\n            \"Latency (ms/sample)\": None,\n            \"Throughput (samples/s)\": None,\n            \"Notes\": f\"Error: {str(e)[:60]}\"\n        })\n\n# ==========================================================\n# üìä TABLE SUMMARY\n# ==========================================================\nsummary = pd.DataFrame(results)\nsummary = summary.sort_values(\"Latency (ms/sample)\", ascending=False)\n\nprint(\"\\nüèÅ Model Benchmark Summary:\")\ndisplay(summary.style.set_properties(**{'text-align': 'center'}))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}