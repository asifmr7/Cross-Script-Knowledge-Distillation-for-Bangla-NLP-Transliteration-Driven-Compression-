{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841dda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use %pip so installs affect THIS kernel\n",
    "get_ipython().run_line_magic('pip', '-q install -U pip setuptools wheel')\n",
    "\n",
    "# 0) Clean out things that often cause conflicts\n",
    "get_ipython().run_line_magic('pip', '-q uninstall -y torch torchvision torchaudio preprocessing google-cloud-automl || true')\n",
    "\n",
    "# 1) PyTorch for CUDA 12.1 (Kaggle GPUs)\n",
    "get_ipython().run_line_magic('pip', '-q install --no-cache-dir --force-reinstall    --index-url https://download.pytorch.org/whl/cu121    torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1')\n",
    "\n",
    "# 2) Scientific stack (versions compiled for NumPy 1.26 ABI)\n",
    "get_ipython().run_line_magic('pip', '-q install --no-cache-dir --force-reinstall    numpy==1.26.4 scipy==1.11.4 scikit-learn==1.4.2')\n",
    "\n",
    "# 3) Core NLP libs\n",
    "get_ipython().run_line_magic('pip', '-q install    \"transformers>=4.43,<5\"    \"datasets>=2.19,<3\"    \"evaluate>=0.4.2,<0.5\"    conllu>=4.5.3 sentencepiece>=0.1.99    tqdm pandas nltk==3.9.1 supar==1.1.4')\n",
    "\n",
    "# 4) Harmony pins\n",
    "# - Protobuf 4.25.3 prevents the MessageFactory/GetPrototype error but stays compatible with HF\n",
    "# - datasets 2.21.0 wants fsspec <= 2024.6.1 (align gcsfs too)\n",
    "get_ipython().run_line_magic('pip', '-q install --no-cache-dir --force-reinstall protobuf==4.25.3 fsspec==2024.6.1 gcsfs==2024.6.1')\n",
    "\n",
    "# Optional: quieter logs; avoids a lot of TF/XLA noise if TF is present in the image\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Optional: purge stale wheels\n",
    "get_ipython().run_line_magic('pip', 'cache purge')\n",
    "\n",
    "print(\"‚úÖ Setup complete. Now restart the runtime (Kernel/Runtime ‚Üí Restart) before running the next cell.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638ebf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Keep TF/XLA chatty logs down if TF happens to be preinstalled\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "import torch, supar, transformers, datasets, evaluate, conllu, nltk\n",
    "import importlib.metadata\n",
    "\n",
    "print(\"torch:\", torch.__version__, \"| CUDA:\", torch.cuda.is_available())\n",
    "print(\"supar:\", supar.__version__)\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"evaluate:\", evaluate.__version__)\n",
    "print(\"conllu:\", importlib.metadata.version(\"conllu\"))\n",
    "print(\"nltk:\", nltk.__version__)\n",
    "print(\"OK ‚úÖ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf7705c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from pathlib import Path\n",
    "from conllu import parse_incr\n",
    "\n",
    "# --- Clean, correct config ---\n",
    "UD_RELEASE = \"2.16\"\n",
    "UD_REPOS = {\n",
    "    \"en_ewt\":  \"UniversalDependencies/UD_English-EWT\",\n",
    "    \"hi_hdtb\": \"UniversalDependencies/UD_Hindi-HDTB\",\n",
    "    \"ur_udtb\": \"UniversalDependencies/UD_Urdu-UDTB\",\n",
    "    \"bn_bru\":  \"UniversalDependencies/UD_Bengali-BRU\",\n",
    "    \"bn_pud\":  \"UniversalDependencies/UD_Bengali-PUD\",\n",
    "}\n",
    "\n",
    "# What splits are known to exist for r2.16 (avoid 404 spam)\n",
    "AVAILABLE_SPLITS = {\n",
    "    \"en_ewt\":  [\"train\", \"dev\", \"test\"],\n",
    "    \"hi_hdtb\": [\"train\", \"dev\", \"test\"],\n",
    "    \"ur_udtb\": [\"train\", \"dev\", \"test\"],\n",
    "    \"bn_bru\":  [\"test\"],               # BRU only has test\n",
    "    \"bn_pud\":  [],                     # PUD has no files in r2.16 for Bengali\n",
    "}\n",
    "\n",
    "def ud_url(tb: str, split: str, release: str = UD_RELEASE) -> str:\n",
    "    # Produce a raw.githubusercontent.com URL to the file on the rX.YY tag\n",
    "    return f\"https://raw.githubusercontent.com/{UD_REPOS[tb]}/r{release}/{tb}-ud-{split}.conllu\"\n",
    "\n",
    "DATA = Path(\"./data/ud\"); DATA.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def fetch(tb: str, sp: str):\n",
    "    \"\"\"Download a UD file if it exists; return Path or None.\"\"\"\n",
    "    p = DATA / tb / f\"{tb}-{sp}.conllu\"\n",
    "    p.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if p.exists():\n",
    "        print(\"CACHED\", p)\n",
    "        return p\n",
    "    url = ud_url(tb, sp)\n",
    "    try:\n",
    "        r = requests.get(url, timeout=60)\n",
    "    except Exception as e:\n",
    "        print(\"MISS \", url, \"ERR\", e)\n",
    "        return None\n",
    "    if r.status_code != 200:\n",
    "        print(\"MISS \", url, r.status_code)\n",
    "        return None\n",
    "    p.write_bytes(r.content)\n",
    "    print(\"OK   \", p)\n",
    "    return p\n",
    "\n",
    "def ensure_all():\n",
    "    files = []\n",
    "    for tb, splits in AVAILABLE_SPLITS.items():\n",
    "        for sp in splits:\n",
    "            p = fetch(tb, sp)\n",
    "            if p is not None:\n",
    "                files.append(p)\n",
    "    return files\n",
    "\n",
    "def read_conllu_rows(path: Path):\n",
    "    rows = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for sent in parse_incr(f):\n",
    "            toks   = [t[\"form\"]   for t in sent if isinstance(t[\"id\"], int)]\n",
    "            upos   = [t[\"upos\"]   for t in sent if isinstance(t[\"id\"], int)]\n",
    "            head   = [t[\"head\"]   for t in sent if isinstance(t[\"id\"], int)]\n",
    "            deprel = [t[\"deprel\"] for t in sent if isinstance(t[\"id\"], int)]\n",
    "            rows.append({\"tokens\": toks, \"upos\": upos, \"head\": head, \"deprel\": deprel})\n",
    "    return rows\n",
    "\n",
    "files = ensure_all()\n",
    "print(\"\\nAvailable files:\")\n",
    "for f in files:\n",
    "    print(\" -\", f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a942a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zero-shot BN -> Train EN (EWT) -> EN dev eval -> BN re-test + comparisons\n",
    "# - Saves logs, learning_curve.csv, metrics.json, predictions\n",
    "# - Robust to SuPar/transformers quirks (no parser=train() reassign, JSON-safe metrics)\n",
    "# ------------------------------------------------------------------------------------------------\n",
    "import os, io, sys, json, re\n",
    "from contextlib import redirect_stdout, redirect_stderr\n",
    "from pathlib import Path\n",
    "\n",
    "# ---- BEFORE importing torch/transformers: CUDA & logging knobs ----\n",
    "os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"max_split_size_mb=128\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "os.environ.setdefault(\"PYTHONUNBUFFERED\", \"1\")\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------\n",
    "# HF optimizer shim (SuPar expects these names)\n",
    "# ---------------------------\n",
    "try:\n",
    "    from transformers import AdamW as _TestAdamW  # noqa: F401\n",
    "    from transformers import get_linear_schedule_with_warmup as _TestSched  # noqa: F401\n",
    "except Exception:\n",
    "    import transformers\n",
    "    import torch.optim as optim\n",
    "    from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "    def get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps):\n",
    "        def lr_lambda(current_step):\n",
    "            if current_step < num_warmup_steps:\n",
    "                return float(current_step) / float(max(1, num_warmup_steps))\n",
    "            return max(\n",
    "                0.0,\n",
    "                float(num_training_steps - current_step)\n",
    "                / float(max(1, num_training_steps - num_warmup_steps)),\n",
    "            )\n",
    "        return LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "    transformers.AdamW = optim.AdamW\n",
    "    transformers.get_linear_schedule_with_warmup = get_linear_schedule_with_warmup\n",
    "# ---------------------------\n",
    "\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "# Make matmul cheaper on T4\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# ---- config/paths ----\n",
    "XLMR = \"xlm-roberta-base\"\n",
    "DATA = Path(\"./data/ud\")\n",
    "\n",
    "# EN (EWT)\n",
    "EN_DIR = DATA / \"en_ewt\"\n",
    "EN_TRAIN = EN_DIR / \"en_ewt-train.conllu\"\n",
    "EN_DEV   = EN_DIR / \"en_ewt-dev.conllu\"\n",
    "\n",
    "# BN (BRU) ‚Äî test only for now\n",
    "BN_DIR   = DATA / \"bn_bru\"\n",
    "BN_TEST  = BN_DIR / \"bn_bru-test.conllu\"\n",
    "\n",
    "# Outputs\n",
    "OUT = Path(\"./outputs_dep\"); OUT.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR = OUT / \"supar_crf2o_xlmr_en\"; MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_PT = MODEL_DIR / \"parser.pt\"\n",
    "LOG_TXT  = MODEL_DIR / \"train_stdout.txt\"\n",
    "CURVE_CSV= MODEL_DIR / \"learning_curve.csv\"\n",
    "METRICS_JSON = MODEL_DIR / \"metrics.json\"\n",
    "\n",
    "# BN predictions (pre vs post training)\n",
    "BN_PRED_PRE  = MODEL_DIR / \"bn_bru-test.pretrain.pred.conllu\"  # zero-shot\n",
    "BN_PRED_POST = MODEL_DIR / \"bn_bru-test.posttrain.pred.conllu\" # after EN training\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# ---- sanity checks ----\n",
    "assert EN_TRAIN.exists(), f\"Missing {EN_TRAIN}\"\n",
    "assert EN_DEV.exists(),   f\"Missing {EN_DEV}\"\n",
    "assert BN_TEST.exists(),  f\"Missing {BN_TEST} (required for BN evaluation)\"\n",
    "\n",
    "# ---- log tee ----\n",
    "class Tee(io.StringIO):\n",
    "    \"\"\"\n",
    "    Writes to: 1) live console, 2) a file, and 3) in-memory buffer (self).\n",
    "    You can choose file mode: 'w' (overwrite) or 'a' (append).\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path: Path, mode: str = \"w\"):\n",
    "        super().__init__()\n",
    "        self._f = open(file_path, mode, encoding=\"utf-8\")\n",
    "\n",
    "    def write(self, s):\n",
    "        sys.__stdout__.write(s)\n",
    "        self._f.write(s)\n",
    "        return super().write(s)\n",
    "\n",
    "    def flush(self):\n",
    "        sys.__stdout__.flush()\n",
    "        self._f.flush()\n",
    "        return super().flush()\n",
    "\n",
    "    def close(self):\n",
    "        try:\n",
    "            self._f.close()\n",
    "        finally:\n",
    "            super().close()\n",
    "\n",
    "# ---- JSON-safe metrics helpers ----\n",
    "_NUM = r\"[-+]?(?:\\d*\\.\\d+|\\d+)(?:[eE][-+]?\\d+)?\"\n",
    "\n",
    "def _metrics_from_string(s: str):\n",
    "    \"\"\"\n",
    "    Parse strings like: \"(2.1538, UCM: 44.64% LCM: 17.86% UAS: 62.35% LAS: 47.77%)\"\n",
    "    Returns dict of floats when present.\n",
    "    \"\"\"\n",
    "    out = {}\n",
    "    mloss = re.search(r\"\\(\\s*(\" + _NUM + r\")\", s)\n",
    "    if mloss: out[\"loss\"] = float(mloss.group(1))\n",
    "    for k in [\"UCM\", \"LCM\", \"UAS\", \"LAS\"]:\n",
    "        m = re.search(rf\"{k}\\s*:\\s*({_NUM})\\s*%?\", s, re.I)\n",
    "        if m:\n",
    "            out[k] = float(m.group(1))\n",
    "    return out\n",
    "\n",
    "def metrics_to_plain(obj):\n",
    "    \"\"\"\n",
    "    Convert SuPar outputs into JSON-serializable dicts/floats/strings.\n",
    "    Tries to parse UAS/LAS/UCM/LCM from string repr when needed.\n",
    "    \"\"\"\n",
    "    if isinstance(obj, (int, float, str)):\n",
    "        return obj\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: metrics_to_plain(v) for k, v in obj.items()}\n",
    "    if isinstance(obj, (list, tuple)):\n",
    "        try:\n",
    "            s = str(obj)\n",
    "            parsed = _metrics_from_string(s)\n",
    "            if parsed:\n",
    "                return parsed\n",
    "        except Exception:\n",
    "            pass\n",
    "        return [metrics_to_plain(x) for x in obj]\n",
    "    try:\n",
    "        s = str(obj)\n",
    "        parsed = _metrics_from_string(s)\n",
    "        if parsed:\n",
    "            return parsed\n",
    "        return s\n",
    "    except Exception:\n",
    "        return repr(obj)\n",
    "\n",
    "# =========================================================================================\n",
    "# Step 0: Build parser ONCE (with pretrained XLM-R features), but don't train yet.\n",
    "#         We still pass EN train/dev so SuPar can build fields/vocabs.\n",
    "#         Then perform ZERO-SHOT BN evaluation BEFORE training.\n",
    "# =========================================================================================\n",
    "print(\"Building parser (pre-training)...\")\n",
    "tee = Tee(LOG_TXT, mode=\"w\")\n",
    "with redirect_stdout(tee), redirect_stderr(tee):\n",
    "    parser = CRF2oDependencyParser.build(\n",
    "        path=str(MODEL_PT),         # where best checkpoint will later be saved\n",
    "        train=str(EN_TRAIN),        # used to build fields/vocabs\n",
    "        dev=str(EN_DEV),\n",
    "        encoder=\"bert\",\n",
    "        bert=XLMR,\n",
    "        finetune=True,\n",
    "        # NOTE: n_layers=1 uses only the last XLM-R layer (memory friendly).\n",
    "        # Remove this arg to enable scalar-mix over layers for a small accuracy bump (uses more VRAM).\n",
    "        n_layers=1,\n",
    "        seed=42,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "\n",
    "    # Try to enable HF gradient checkpointing to save memory\n",
    "    try:\n",
    "        bert_model = parser.model.encoder.bert\n",
    "        if hasattr(bert_model, \"gradient_checkpointing_enable\"):\n",
    "            bert_model.gradient_checkpointing_enable()\n",
    "        if hasattr(bert_model.config, \"gradient_checkpointing\"):\n",
    "            bert_model.config.gradient_checkpointing = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# ---- BN zero-shot (pre-training) ----\n",
    "metrics = {}\n",
    "try:\n",
    "    print(\"üß™ Zero-shot BN test (pre-training)...\")\n",
    "    with redirect_stdout(tee), redirect_stderr(tee):\n",
    "        parser.predict(str(BN_TEST), pred=str(BN_PRED_PRE), batch_size=1000, device=DEVICE)\n",
    "        bn_scores_pre = parser.evaluate(str(BN_TEST), batch_size=1000, device=DEVICE)\n",
    "    metrics[\"bn_test_pretrain_raw\"] = str(bn_scores_pre)\n",
    "    metrics[\"bn_test_pretrain\"] = metrics_to_plain(bn_scores_pre)\n",
    "    print(\"BN zero-shot predictions:\", BN_PRED_PRE)\n",
    "    print(\"BN BRU zero-shot test:\", bn_scores_pre)\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è BN zero-shot prediction/evaluation failed:\", e)\n",
    "\n",
    "tee.flush()\n",
    "tee.close()\n",
    "\n",
    "# =========================================================================================\n",
    "# Step 1: REBUILD parser FRESH for EN training (avoid stale args like 'data' from evaluate)\n",
    "# =========================================================================================\n",
    "print(\"Rebuilding parser (fresh) for EN training...\")\n",
    "tee = Tee(LOG_TXT, mode=\"a\")  # append to the same log\n",
    "with redirect_stdout(tee), redirect_stderr(tee):\n",
    "    parser = CRF2oDependencyParser.build(\n",
    "        path=str(MODEL_PT),\n",
    "        train=str(EN_TRAIN),\n",
    "        dev=str(EN_DEV),\n",
    "        encoder=\"bert\",\n",
    "        bert=XLMR,\n",
    "        finetune=True,\n",
    "        n_layers=1,\n",
    "        seed=42,\n",
    "        device=DEVICE,\n",
    "    )\n",
    "    # Best-effort: enable HF gradient checkpointing\n",
    "    try:\n",
    "        bert_model = parser.model.encoder.bert\n",
    "        if hasattr(bert_model, \"gradient_checkpointing_enable\"):\n",
    "            bert_model.gradient_checkpointing_enable()\n",
    "        if hasattr(bert_model.config, \"gradient_checkpointing\"):\n",
    "            bert_model.config.gradient_checkpointing = True\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # Extra guard (if a future SuPar version carries stale keys)\n",
    "    try:\n",
    "        if hasattr(parser, \"args\") and isinstance(parser.args, dict):\n",
    "            parser.args.pop(\"data\", None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    print(\"Starting EN (EWT) training...\")\n",
    "    parser.train(\n",
    "        train=str(EN_TRAIN),\n",
    "        dev=str(EN_DEV),\n",
    "        test=str(EN_DEV),   # SuPar expects a real path\n",
    "        optimizer=\"adam\",\n",
    "        lr=5e-5,            # encoder LR\n",
    "        lr_rate=20.0,       # non-encoder head LR multiplier => 1e-3\n",
    "        warmup=0.1,\n",
    "        clip=5.0,\n",
    "        checkpoint=False,\n",
    "        batch_size=1000,    # micro-batch size in tokens\n",
    "        update_steps=6,     # gradient accumulation\n",
    "        buckets=16,\n",
    "        epochs=25,\n",
    "        patience=5,\n",
    "    )\n",
    "\n",
    "tee.flush()\n",
    "tee.close()\n",
    "print(f\"‚úÖ Training finished. Model saved at: {MODEL_PT} (exists={MODEL_PT.exists()})\")\n",
    "print(f\"Raw log saved to: {LOG_TXT}\")\n",
    "\n",
    "# ---- (Re)load the best checkpoint safely and move to device ----\n",
    "try:\n",
    "    if MODEL_PT.exists():\n",
    "        parser = CRF2oDependencyParser.load(str(MODEL_PT))\n",
    "        try:\n",
    "            parser.model.to(DEVICE)\n",
    "        except Exception:\n",
    "            pass\n",
    "except Exception as _e:\n",
    "    print(\"‚ö†Ô∏è Reloading best checkpoint failed; continuing with in-memory model:\", _e)\n",
    "\n",
    "# =========================================================================================\n",
    "# Step 2: Evaluate EN dev (post-training)\n",
    "# =========================================================================================\n",
    "try:\n",
    "    dev_scores = parser.evaluate(str(EN_DEV), batch_size=1000, device=DEVICE)\n",
    "    metrics[\"en_dev_raw\"] = str(dev_scores)\n",
    "    metrics[\"en_dev\"] = metrics_to_plain(dev_scores)\n",
    "    print(\"EN dev (post-training):\", dev_scores)\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è Dev evaluation failed:\", e)\n",
    "\n",
    "# =========================================================================================\n",
    "# Step 3: BN re-test (post-training)\n",
    "# =========================================================================================\n",
    "try:\n",
    "    parser.predict(str(BN_TEST), pred=str(BN_PRED_POST), batch_size=1000, device=DEVICE)\n",
    "    bn_scores_post = parser.evaluate(str(BN_TEST), batch_size=1000, device=DEVICE)\n",
    "    metrics[\"bn_test_trained_raw\"] = str(bn_scores_post)\n",
    "    metrics[\"bn_test_trained\"] = metrics_to_plain(bn_scores_post)\n",
    "    print(\"BN predictions after EN training:\", BN_PRED_POST)\n",
    "    print(\"BN BRU post-training test:\", bn_scores_post)\n",
    "except Exception as e:\n",
    "    print(\"‚ö†Ô∏è BN post-training prediction/evaluation failed:\", e)\n",
    "\n",
    "# =========================================================================================\n",
    "# Step 4: Parse learning curve from train log & save CSV\n",
    "# =========================================================================================\n",
    "raw_log = Path(LOG_TXT).read_text(encoding=\"utf-8\")\n",
    "raw_log = raw_log.replace(\"\\r\", \"\\n\")\n",
    "raw_log = re.sub(r\"\\x1b\\[[0-9;]*m\", \"\", raw_log)  # strip ANSI\n",
    "\n",
    "epoch, tr_loss, dv_uas, dv_las = [], [], [], []\n",
    "patterns = [\n",
    "    # \"Epoch 1 ... loss: 1.23 ... UAS: 89.10 ... LAS: 87.55\"\n",
    "    re.compile(\n",
    "        r\"Epoch\\s*(\\d+)[^\\n]*?loss[^0-9-]*(\" + _NUM + r\")[^\\n]*?UAS[^0-9-]*(\" + _NUM + r\")[^\\n]*?LAS[^0-9-]*(\" + _NUM + r\")\",\n",
    "        re.I,\n",
    "    ),\n",
    "    # equals style\n",
    "    re.compile(\n",
    "        r\"Epoch\\s*(\\d+)[^\\n]*?loss\\s*=\\s*(\" + _NUM + r\")[^\\n]*?UAS\\s*=\\s*(\" + _NUM + r\")[^\\n]*?LAS\\s*=\\s*(\" + _NUM + r\")\",\n",
    "        re.I,\n",
    "    ),\n",
    "]\n",
    "for line in raw_log.splitlines():\n",
    "    for pat in patterns:\n",
    "        m = pat.search(line)\n",
    "        if m:\n",
    "            epoch.append(int(m.group(1)))\n",
    "            tr_loss.append(float(m.group(2)))\n",
    "            dv_uas.append(float(m.group(3)))\n",
    "            dv_las.append(float(m.group(4)))\n",
    "            break\n",
    "\n",
    "curve_df = pd.DataFrame({\n",
    "    \"epoch\": epoch,\n",
    "    \"train_loss\": tr_loss,\n",
    "    \"dev_UAS\": dv_uas,\n",
    "    \"dev_LAS\": dv_las\n",
    "})\n",
    "curve_df.to_csv(CURVE_CSV, index=False)\n",
    "print(f\"Learning curve points: {len(curve_df)}  -> {CURVE_CSV}\")\n",
    "\n",
    "# =========================================================================================\n",
    "# Step 5: Save metrics (JSON-safe)\n",
    "# =========================================================================================\n",
    "Path(METRICS_JSON).write_text(json.dumps(metrics, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "print(\"Saved metrics:\", METRICS_JSON)\n",
    "\n",
    "# =========================================================================================\n",
    "# Step 6: Plots\n",
    "#   - Training loss per epoch\n",
    "#   - Dev UAS/LAS per epoch\n",
    "#   - Correlation matrix among (train_loss, dev_UAS, dev_LAS)\n",
    "#   - BN pre vs post (UAS/LAS) comparison\n",
    "# =========================================================================================\n",
    "if len(curve_df) > 0:\n",
    "    # 1) Train loss\n",
    "    plt.figure()\n",
    "    plt.plot(curve_df[\"epoch\"], curve_df[\"train_loss\"], marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Train loss\"); plt.title(\"Training Loss per Epoch\")\n",
    "    plt.grid(True); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 2) Dev UAS & LAS\n",
    "    plt.figure()\n",
    "    plt.plot(curve_df[\"epoch\"], curve_df[\"dev_UAS\"], marker=\"o\", label=\"UAS\")\n",
    "    plt.plot(curve_df[\"epoch\"], curve_df[\"dev_LAS\"], marker=\"o\", label=\"LAS\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Score\"); plt.title(\"Dev UAS/LAS per Epoch\")\n",
    "    plt.legend(); plt.grid(True); plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 3) Correlation matrix among metrics\n",
    "    corr = curve_df[[\"train_loss\", \"dev_UAS\", \"dev_LAS\"]].corr(method=\"pearson\")\n",
    "    print(\"\\nCorrelation matrix (Pearson):\\n\", corr)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.imshow(corr.values, interpolation=\"nearest\")\n",
    "    plt.xticks(range(corr.shape[1]), corr.columns, rotation=45, ha=\"right\")\n",
    "    plt.yticks(range(corr.shape[0]), corr.index)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(corr.shape[1]):\n",
    "            plt.text(j, i, f\"{corr.values[i,j]:.2f}\", ha=\"center\", va=\"center\")\n",
    "    plt.title(\"Correlation Matrix (train_loss, dev_UAS, dev_LAS)\")\n",
    "    plt.colorbar()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No epoch lines were parsed from the log. SuPar‚Äôs logging format may differ across versions.\")\n",
    "\n",
    "# 4) BN pre vs post (UAS/LAS) comparison\n",
    "bn_pre = metrics.get(\"bn_test_pretrain\", {})\n",
    "bn_post = metrics.get(\"bn_test_trained\", {})\n",
    "\n",
    "if \"UAS\" in bn_pre and \"UAS\" in bn_post:\n",
    "    plt.figure()\n",
    "    plt.bar([\"BN zero-shot\", \"BN post-train\"], [bn_pre[\"UAS\"], bn_post[\"UAS\"]])\n",
    "    plt.ylabel(\"UAS\"); plt.title(\"BN UAS: Zero-shot vs Post-EN-Training\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "if \"LAS\" in bn_pre and \"LAS\" in bn_post:\n",
    "    plt.figure()\n",
    "    plt.bar([\"BN zero-shot\", \"BN post-train\"], [bn_pre[\"LAS\"], bn_post[\"LAS\"]])\n",
    "    plt.ylabel(\"LAS\"); plt.title(\"BN LAS: Zero-shot vs Post-EN-Training\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Print quick deltas to console\n",
    "if \"UAS\" in bn_pre and \"UAS\" in bn_post:\n",
    "    print(f\"ŒîBN UAS (post - pre): {bn_post['UAS'] - bn_pre['UAS']:.2f}\")\n",
    "if \"LAS\" in bn_pre and \"LAS\" in bn_post:\n",
    "    print(f\"ŒîBN LAS (post - pre): {bn_post['LAS'] - bn_pre['LAS']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c077833",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_dep/supar_crf2o_xlmr_en/parser.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00616e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Load your best trained parser\n",
    "parser = CRF2oDependencyParser.load(\"outputs_dep/supar_crf2o_xlmr_en/parser.pt\")\n",
    "parser.model.to(DEVICE)\n",
    "\n",
    "# Your Bengali sentence\n",
    "sent = \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡ßç‡¶¶‡ßã-‡¶Ü‡¶∞‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ì ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§‡•§ ‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶∏‡¶™‡ßç‡¶§‡¶Æ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"\n",
    "\n",
    "# Run parsing (batch of one sentence)\n",
    "pred = parser.predict([sent], prob=True, verbose=True, lang='bn')\n",
    "\n",
    "# Inspect tokens + heads + relations\n",
    "for tree in pred.sentences:\n",
    "    print(\"\\n--- Parsed sentence ---\")\n",
    "    for tok in tree:\n",
    "        print(f\"{tok.id}\\t{tok.form}\\thead={tok.head}\\tdeprel={tok.deprel}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acbdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import torch\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "MODEL_PT = \"outputs_dep/supar_crf2o_xlmr_en/parser.pt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text = (\n",
    "    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡ßç‡¶¶‡ßã-‡¶Ü‡¶∞‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ì ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§‡•§ \"\n",
    "    \"‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶∏‡¶™‡ßç‡¶§‡¶Æ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"\n",
    ")\n",
    "\n",
    "# --- simple sentence splitter for Bengali (split on danda/question/exclamation) ---\n",
    "def split_sentences(s: str):\n",
    "    # Keep the delimiter as its own token (we'll reattach to sentence)\n",
    "    parts = re.split(r'([‡•§!?])', s.strip())\n",
    "    sents = []\n",
    "    cur = \"\"\n",
    "    for i in range(0, len(parts), 2):\n",
    "        seg = parts[i].strip()\n",
    "        punct = parts[i+1] if i+1 < len(parts) else \"\"\n",
    "        if seg:\n",
    "            sents.append((seg + (punct or \"\")).strip())\n",
    "    return sents\n",
    "\n",
    "# --- tokenizer: words + keep punctuation as separate tokens ---\n",
    "# \\w in Unicode covers Bengali letters/digits/marks; we also capture hyphenated tokens.\n",
    "WORD_OR_PUNCT = re.compile(r\"[^\\W_][\\w\\u200c\\u200d-]*|[^\\s\\w]\", re.UNICODE)\n",
    "\n",
    "def tokenize(sent: str):\n",
    "    return WORD_OR_PUNCT.findall(sent)\n",
    "\n",
    "# Pre-tokenize\n",
    "sents_tok = [tokenize(s) for s in split_sentences(text)]\n",
    "\n",
    "print(\"Pre-tokenized sentences:\")\n",
    "for i, toks in enumerate(sents_tok, 1):\n",
    "    print(f\"{i:>2}: {' '.join(toks)}\")\n",
    "\n",
    "# Load model\n",
    "parser = CRF2oDependencyParser.load(MODEL_PT)\n",
    "try:\n",
    "    parser.model.to(DEVICE)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Run prediction with pre-tokenized input: pass list[list[str]] and lang=None\n",
    "pred = parser.predict(sents_tok, prob=False, verbose=True, lang=None)\n",
    "\n",
    "# Pretty print tokens, heads, labels\n",
    "for si, tree in enumerate(pred.sentences, 1):\n",
    "    print(f\"\\n--- Parsed sentence {si} ---\")\n",
    "    print(\"{:>3}  {:<20} {:>4}  {:<10}\".format(\"ID\", \"FORM\", \"HEAD\", \"DEPREL\"))\n",
    "    print(\"-\"*45)\n",
    "    for tok in tree:\n",
    "        print(\"{:>3}  {:<20} {:>4}  {:<10}\".format(tok.id, tok.form, tok.head, tok.deprel))\n",
    "\n",
    "# (Optional) save as CoNLL-U for manual checking/visualization\n",
    "SAVE_CONLLU = False\n",
    "if SAVE_CONLLU:\n",
    "    out_path = \"one_para_bn.pred.conllu\"\n",
    "    parser.predict(sents_tok, pred=out_path, lang=None)\n",
    "    print(\"\\nSaved CoNLL-U to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6351bdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import os, re, tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "MODEL_PT = \"outputs_dep/supar_crf2o_xlmr_en/parser.pt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text = (\n",
    "    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡ßç‡¶¶‡ßã-‡¶Ü‡¶∞‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ì ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§‡•§ \"\n",
    "    \"‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶∏‡¶™‡ßç‡¶§‡¶Æ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"\n",
    ")\n",
    "\n",
    "# -------- Bengali-friendly sentence split (keep ‡•§?!) --------\n",
    "def split_sentences(s: str):\n",
    "    parts = re.split(r'([‡•§!?])', s.strip())\n",
    "    out = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        seg = parts[i].strip()\n",
    "        punct = parts[i+1] if i+1 < len(parts) else \"\"\n",
    "        if seg:\n",
    "            out.append((seg + (punct or \"\")).strip())\n",
    "    return out\n",
    "\n",
    "# -------- Bengali word tokenizer (no external deps) ----------\n",
    "# Treat sequences in U+0980‚ÄìU+09FF as words (includes Bengali letters & marks),\n",
    "# allow internal hyphens; separate everything else as single-char tokens.\n",
    "BN_BLOCK = r\"\\u0980-\\u09FF\"\n",
    "WORD_RE = re.compile(fr\"[{BN_BLOCK}]+(?:-[{BN_BLOCK}]+)*|\\d+|[^\\s]\", re.UNICODE)\n",
    "\n",
    "def tokenize_bn(sent: str):\n",
    "    return WORD_RE.findall(sent)\n",
    "\n",
    "def to_conllu(sent_tokens, sent_id=1):\n",
    "    lines = [\"# sent_id = {}\".format(sent_id)]\n",
    "    lines.append(\"# text = \" + \" \".join(sent_tokens))\n",
    "    for i, tok in enumerate(sent_tokens, 1):\n",
    "        # ID  FORM  LEMMA  UPOS  XPOS  FEATS  HEAD  DEPREL  DEPS  MISC\n",
    "        lines.append(f\"{i}\\t{tok}\\t_\\t_\\t_\\t_\\t0\\tdep\\t_\\t_\")\n",
    "    lines.append(\"\")  # sentence boundary\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# 1) Pre-tokenize properly\n",
    "sents = [tokenize_bn(s) for s in split_sentences(text)]\n",
    "print(\"Pre-tokenized sentences:\")\n",
    "for i, toks in enumerate(sents, 1):\n",
    "    print(f\"{i:>2}: {' '.join(toks)}\")\n",
    "\n",
    "# 2) Write a minimal temporary CoNLL-U\n",
    "tmp_dir = tempfile.mkdtemp(prefix=\"bn_manual_\")\n",
    "in_path = Path(tmp_dir) / \"in.conllu\"\n",
    "out_path = Path(tmp_dir) / \"pred.conllu\"\n",
    "\n",
    "with open(in_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sid, toks in enumerate(sents, 1):\n",
    "        f.write(to_conllu(toks, sent_id=sid))\n",
    "\n",
    "# 3) Load model and predict (NO stanza, NO lang)\n",
    "parser = CRF2oDependencyParser.load(MODEL_PT)\n",
    "try:\n",
    "    parser.model.to(DEVICE)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Important: give it a file path & lang=None\n",
    "parser.predict(str(in_path), pred=str(out_path), lang=None, verbose=True)\n",
    "\n",
    "# 4) Read predictions and pretty-print\n",
    "print(f\"\\nPredicted CoNLL-U: {out_path}\")\n",
    "cur_sent = 0\n",
    "print_row = lambda i, form, head, rel: print(f\"{i:>3}  {form:<20} {head:>4}  {rel:<12}\")\n",
    "with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    print(\"\\n--- Parsed output ---\")\n",
    "    for line in f:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line:\n",
    "            continue\n",
    "        if line.startswith(\"# sent_id\"):\n",
    "            cur_sent += 1\n",
    "            print(f\"\\nSentence {cur_sent}\")\n",
    "            print(\" ID  FORM                 HEAD  DEPREL\")\n",
    "            print(\"---- -------------------- ----  ------------\")\n",
    "            continue\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        cols = line.split(\"\\t\")\n",
    "        if len(cols) != 10:  # skip odd lines\n",
    "            continue\n",
    "        tid, form, _, _, _, _, head, rel, _, _ = cols\n",
    "        # skip multi-word tokens / empty nodes if ever present\n",
    "        if \"-\" in tid or \".\" in tid:\n",
    "            continue\n",
    "        print_row(int(tid), form, head, rel)\n",
    "\n",
    "print(\"\\nDone. You can open the .conllu to inspect full details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c45788",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import re, tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "MODEL_PT = \"outputs_dep/supar_crf2o_xlmr_en/parser.pt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text = (\n",
    "    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡ßç‡¶¶‡ßã-‡¶Ü‡¶∞‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ì ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§‡•§ \"\n",
    "    \"‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶∏‡¶™‡ßç‡¶§‡¶Æ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"\n",
    ")\n",
    "\n",
    "# --- sentence split ---\n",
    "def split_sentences(s: str):\n",
    "    parts = re.split(r'([‡•§!?])', s.strip())\n",
    "    out = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        seg = parts[i].strip()\n",
    "        punct = parts[i+1] if i+1 < len(parts) else \"\"\n",
    "        if seg:\n",
    "            out.append((seg + (punct or \"\")).strip())\n",
    "    return out\n",
    "\n",
    "# --- Bengali tokenizer (keep Bengali word blocks together, punctuation separate) ---\n",
    "BN_BLOCK = r\"\\u0980-\\u09FF\"\n",
    "WORD_RE = re.compile(fr\"[{BN_BLOCK}]+(?:-[{BN_BLOCK}]+)*|\\d+|[^\\s]\", re.UNICODE)\n",
    "def tokenize_bn(sent: str): return WORD_RE.findall(sent)\n",
    "\n",
    "def to_conllu(sent_tokens, sent_id=1):\n",
    "    lines = [f\"# sent_id = {sent_id}\", \"# text = \" + \" \".join(sent_tokens)]\n",
    "    for i, tok in enumerate(sent_tokens, 1):\n",
    "        lines.append(f\"{i}\\t{tok}\\t_\\t_\\t_\\t_\\t0\\tdep\\t_\\t_\")\n",
    "    lines.append(\"\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# 1) pre-tokenize\n",
    "sents = [tokenize_bn(s) for s in split_sentences(text)]\n",
    "print(\"Pre-tokenized sentences:\")\n",
    "for i, toks in enumerate(sents, 1):\n",
    "    print(f\"{i:>2}: {' '.join(toks)}\")\n",
    "\n",
    "# 2) temp CoNLL-U paths\n",
    "tmp = Path(tempfile.mkdtemp(prefix=\"bn_manual_\"))\n",
    "in_path  = tmp / \"in.conllu\"\n",
    "out_path = tmp / \"pred.conllu\"\n",
    "with open(in_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sid, toks in enumerate(sents, 1):\n",
    "        f.write(to_conllu(toks, sid))\n",
    "\n",
    "# 3) load parser\n",
    "parser = CRF2oDependencyParser.load(MODEL_PT)\n",
    "try: parser.model.to(DEVICE)\n",
    "except Exception: pass\n",
    "\n",
    "# (Patch) ensure transform doesn‚Äôt request 'probs'\n",
    "try:\n",
    "    tr = parser.transform\n",
    "    # fields and tgt are lists of field names\n",
    "    if hasattr(tr, \"fields\") and isinstance(tr.fields, list):\n",
    "        tr.fields = [x for x in tr.fields if x != \"probs\"]\n",
    "    if hasattr(tr, \"tgt\") and isinstance(tr.tgt, list):\n",
    "        tr.tgt = [x for x in tr.tgt if x != \"probs\"]\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# 4) predict with explicit flags (important!)\n",
    "parser.predict(str(in_path), pred=str(out_path),\n",
    "               lang=None, prob=False, mbr=False, tree=False, verbose=True)\n",
    "\n",
    "# 5) pretty print\n",
    "print(f\"\\nPredicted CoNLL-U: {out_path}\")\n",
    "print(\"\\n--- Parsed output ---\")\n",
    "cur = 0\n",
    "with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line: continue\n",
    "        if line.startswith(\"# sent_id\"):\n",
    "            cur += 1\n",
    "            print(f\"\\nSentence {cur}\")\n",
    "            print(\" ID  FORM                 HEAD  DEPREL\")\n",
    "            print(\"---- -------------------- ----  ------------\")\n",
    "            continue\n",
    "        if line.startswith(\"#\"): continue\n",
    "        cols = line.split(\"\\t\")\n",
    "        if len(cols) != 10: continue\n",
    "        tid, form, _, _, _, _, head, rel, _, _ = cols\n",
    "        if \"-\" in tid or \".\" in tid:  # skip MWT/empty nodes\n",
    "            continue\n",
    "        print(f\"{int(tid):>3}  {form:<20} {head:>4}  {rel:<12}\")\n",
    "\n",
    "print(\"\\nDone. (Open the .conllu file to inspect full details.)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b5350b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "import re, tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "MODEL_PT = \"outputs_dep/supar_crf2o_xlmr_en/parser.pt\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "text = (\n",
    "    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡ßç‡¶¶‡ßã-‡¶Ü‡¶∞‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ì ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§‡•§ \"\n",
    "    \"‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶∏‡¶™‡ßç‡¶§‡¶Æ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"\n",
    ")\n",
    "\n",
    "# --- sentence split ---\n",
    "def split_sentences(s: str):\n",
    "    parts = re.split(r'([‡•§!?])', s.strip())\n",
    "    out = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        seg = parts[i].strip()\n",
    "        punct = parts[i+1] if i+1 < len(parts) else \"\"\n",
    "        if seg:\n",
    "            out.append((seg + (punct or \"\")).strip())\n",
    "    return out\n",
    "\n",
    "# --- Bengali tokenizer ---\n",
    "BN_BLOCK = r\"\\u0980-\\u09FF\"\n",
    "WORD_RE = re.compile(fr\"[{BN_BLOCK}]+(?:-[{BN_BLOCK}]+)*|\\d+|[^\\s]\", re.UNICODE)\n",
    "def tokenize_bn(sent: str): return WORD_RE.findall(sent)\n",
    "\n",
    "def to_conllu(sent_tokens, sent_id=1):\n",
    "    lines = [f\"# sent_id = {sent_id}\", \"# text = \" + \" \".join(sent_tokens)]\n",
    "    for i, tok in enumerate(sent_tokens, 1):\n",
    "        # ID  FORM  LEMMA  UPOS  XPOS  FEATS  HEAD  DEPREL  DEPS  MISC\n",
    "        lines.append(f\"{i}\\t{tok}\\t_\\t_\\t_\\t_\\t0\\tdep\\t_\\t_\")\n",
    "    lines.append(\"\")  # <- blank line to end the sentence\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "# 1) pre-tokenize\n",
    "sents = [tokenize_bn(s) for s in split_sentences(text)]\n",
    "print(\"Pre-tokenized sentences:\")\n",
    "for i, toks in enumerate(sents, 1):\n",
    "    print(f\"{i:>2}: {' '.join(toks)}\")\n",
    "\n",
    "# 2) write temp CoNLL-U (with final newline)\n",
    "tmp = Path(tempfile.mkdtemp(prefix=\"bn_manual_\"))\n",
    "in_path  = tmp / \"in.conllu\"\n",
    "out_path = tmp / \"pred.conllu\"\n",
    "with open(in_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for sid, toks in enumerate(sents, 1):\n",
    "        f.write(to_conllu(toks, sid) + \"\\n\")   # ensure extra newline at EOF\n",
    "\n",
    "# 3) load model\n",
    "parser = CRF2oDependencyParser.load(MODEL_PT)\n",
    "try: parser.model.to(DEVICE)\n",
    "except Exception: pass\n",
    "\n",
    "# 4) predict ‚Äî force single bucket to skip k-means\n",
    "parser.predict(\n",
    "    str(in_path),\n",
    "    pred=str(out_path),\n",
    "    lang=None,\n",
    "    prob=False, mbr=False, tree=False,\n",
    "    buckets=1,\n",
    "    batch_size=5000,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# 5) pretty print\n",
    "print(f\"\\nPredicted CoNLL-U: {out_path}\")\n",
    "print(\"\\n--- Parsed output ---\")\n",
    "cur = 0\n",
    "with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip(\"\\n\")\n",
    "        if not line: continue\n",
    "        if line.startswith(\"# sent_id\"):\n",
    "            cur += 1\n",
    "            print(f\"\\nSentence {cur}\")\n",
    "            print(\" ID  FORM                 HEAD  DEPREL\")\n",
    "            print(\"---- -------------------- ----  ------------\")\n",
    "            continue\n",
    "        if line.startswith(\"#\"): continue\n",
    "        cols = line.split(\"\\t\")\n",
    "        if len(cols) != 10: continue\n",
    "        tid, form, _, _, _, _, head, rel, _, _ = cols\n",
    "        if \"-\" in tid or \".\" in tid:  # skip MWT/empty nodes\n",
    "            continue\n",
    "        print(f\"{int(tid):>3}  {form:<20} {head:>4}  {rel:<12}\")\n",
    "\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd278d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# after: parser = CRF2oDependencyParser.load(MODEL_PT)\n",
    "\n",
    "def _strip_field(transform, name: str):\n",
    "    \"\"\"Remove a field name from SuPar's transform bookkeeping (list/tuple).\"\"\"\n",
    "    for attr in (\"fields\", \"src\", \"tgt\"):\n",
    "        if hasattr(transform, attr):\n",
    "            v = getattr(transform, attr)\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                nv = [x for x in v if x != name]\n",
    "                setattr(transform, attr, type(v)(nv))\n",
    "    # Some versions cache a flattened copy; wipe it so it's recomputed\n",
    "    if hasattr(transform, \"_flattened_fields\"):\n",
    "        try:\n",
    "            setattr(transform, \"_flattened_fields\", None)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# --- apply the patch ---\n",
    "tr = parser.transform\n",
    "# (optional) see before/after\n",
    "try:\n",
    "    print(\"BEFORE fields:\", getattr(tr, \"fields\", None))\n",
    "    print(\"BEFORE src:\",    getattr(tr, \"src\", None))\n",
    "    print(\"BEFORE tgt:\",    getattr(tr, \"tgt\", None))\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_strip_field(tr, \"probs\")   # <-- the culprit in your traceback\n",
    "\n",
    "try:\n",
    "    print(\"AFTER  fields:\", getattr(tr, \"fields\", None))\n",
    "    print(\"AFTER  src:\",    getattr(tr, \"src\", None))\n",
    "    print(\"AFTER  tgt:\",    getattr(tr, \"tgt\", None))\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f7bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Manual one-paragraph parse with SuPar (CRF2oDependencyParser) for Bengali.\n",
    "- No Stanza needed (we pre-tokenize)\n",
    "- Strips lingering 'probs' field from the saved transform\n",
    "- Forces single-bucket inference to avoid k-means issues\n",
    "\"\"\"\n",
    "import re\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "# ==== CONFIG ====\n",
    "MODEL_PT = \"outputs_dep/supar_crf2o_xlmr_en/parser.pt\"\n",
    "TEXT = (\n",
    "    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡ßç‡¶¶‡ßã-‡¶Ü‡¶∞‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ì ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§‡•§ \"\n",
    "    \"‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶∏‡¶™‡ßç‡¶§‡¶Æ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"\n",
    ")\n",
    "FORCE_SAVE_CONLLU = True  # also write a local copy: ./bn_manual.pred.conllu\n",
    "\n",
    "# ==== UTILITIES ====\n",
    "def split_sentences(s: str):\n",
    "    \"\"\"Split on Bengali danda/question/exclamation, keep punctuation attached.\"\"\"\n",
    "    parts = re.split(r'([‡•§!?])', s.strip())\n",
    "    out = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        seg = parts[i].strip()\n",
    "        punct = parts[i+1] if i+1 < len(parts) else \"\"\n",
    "        if seg:\n",
    "            out.append((seg + (punct or \"\")).strip())\n",
    "    return out\n",
    "\n",
    "# Bengali word block (U+0980..U+09FF). Keep hyphenated Bengali words, digits, or single non-space chars (punct).\n",
    "BN_BLOCK = r\"\\u0980-\\u09FF\"\n",
    "WORD_RE = re.compile(fr\"[{BN_BLOCK}]+(?:-[{BN_BLOCK}]+)*|\\d+|[^\\s]\", re.UNICODE)\n",
    "\n",
    "def tokenize_bn(sent: str):\n",
    "    return WORD_RE.findall(sent)\n",
    "\n",
    "def to_conllu(sent_tokens, sent_id=1):\n",
    "    lines = [f\"# sent_id = {sent_id}\", \"# text = \" + \" \".join(sent_tokens)]\n",
    "    for i, tok in enumerate(sent_tokens, 1):\n",
    "        # ID  FORM  LEMMA  UPOS  XPOS  FEATS  HEAD  DEPREL  DEPS  MISC\n",
    "        lines.append(f\"{i}\\t{tok}\\t_\\t_\\t_\\t_\\t0\\tdep\\t_\\t_\")\n",
    "    lines.append(\"\")  # blank line to end the sentence\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def strip_field_everywhere(transform, name: str):\n",
    "    \"\"\"Robustly remove a field name from SuPar's transform bookkeeping.\"\"\"\n",
    "    for attr in (\"fields\", \"src\", \"tgt\"):\n",
    "        if hasattr(transform, attr):\n",
    "            v = getattr(transform, attr)\n",
    "            if isinstance(v, (list, tuple)):\n",
    "                nv = [x for x in list(v) if x != name]\n",
    "                try:\n",
    "                    setattr(transform, attr, type(v)(nv))\n",
    "                except Exception:\n",
    "                    setattr(transform, attr, nv)\n",
    "    # clear any cached flattened lists\n",
    "    for cache_attr in (\"_flattened_fields\", \"flattened\", \"__flattened\"):\n",
    "        if hasattr(transform, cache_attr):\n",
    "            try:\n",
    "                setattr(transform, cache_attr, None)\n",
    "            except Exception:\n",
    "                pass\n",
    "    # delete a stray attribute if present\n",
    "    if hasattr(transform, name):\n",
    "        try:\n",
    "            delattr(transform, name)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 1) Pre-tokenize\n",
    "    sents = [tokenize_bn(s) for s in split_sentences(TEXT)]\n",
    "    print(\"Pre-tokenized sentences:\")\n",
    "    for i, toks in enumerate(sents, 1):\n",
    "        print(f\"{i:>2}: {' '.join(toks)}\")\n",
    "\n",
    "    # 2) Write temp CoNLL-U (ensure final newline)\n",
    "    tmp = Path(tempfile.mkdtemp(prefix=\"bn_manual_\"))\n",
    "    in_path  = tmp / \"in.conllu\"\n",
    "    out_path = tmp / \"pred.conllu\"\n",
    "    with open(in_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sid, toks in enumerate(sents, 1):\n",
    "            f.write(to_conllu(toks, sid) + \"\\n\")\n",
    "\n",
    "    # 3) Load parser & move to device\n",
    "    parser = CRF2oDependencyParser.load(MODEL_PT)\n",
    "    try:\n",
    "        parser.model.to(device)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) Patch transform: remove 'probs' everywhere (prevents AttributeError)\n",
    "    tr = parser.transform\n",
    "    # debug (optional): uncomment next lines to inspect pre/after\n",
    "    # print(\"[DEBUG] BEFORE fields:\", getattr(tr, \"fields\", None))\n",
    "    # print(\"[DEBUG] BEFORE src   :\", getattr(tr, \"src\", None))\n",
    "    # print(\"[DEBUG] BEFORE tgt   :\", getattr(tr, \"tgt\", None))\n",
    "\n",
    "    strip_field_everywhere(tr, \"probs\")\n",
    "\n",
    "    # print(\"[DEBUG] AFTER  fields:\", getattr(tr, \"fields\", None))\n",
    "    # print(\"[DEBUG] AFTER  src   :\", getattr(tr, \"src\", None))\n",
    "    # print(\"[DEBUG] AFTER  tgt   :\", getattr(tr, \"tgt\", None))\n",
    "\n",
    "    # 5) Predict ‚Äî skip k-means, skip probability/extra decoding\n",
    "    parser.predict(\n",
    "        str(in_path),\n",
    "        pred=str(out_path),\n",
    "        lang=None,\n",
    "        prob=False, mbr=False, tree=False,\n",
    "        buckets=1, batch_size=5000,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 6) Pretty-print predictions\n",
    "    print(f\"\\nPredicted CoNLL-U: {out_path}\")\n",
    "    print(\"\\n--- Parsed output ---\")\n",
    "    cur = 0\n",
    "    with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"# sent_id\"):\n",
    "                cur += 1\n",
    "                print(f\"\\nSentence {cur}\")\n",
    "                print(\" ID  FORM                 HEAD  DEPREL\")\n",
    "                print(\"---- -------------------- ----  ------------\")\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            tid, form, _, _, _, _, head, rel, _, _ = cols\n",
    "            if \"-\" in tid or \".\" in tid:\n",
    "                continue\n",
    "            print(f\"{int(tid):>3}  {form:<20} {head:>4}  {rel:<12}\")\n",
    "\n",
    "    # 7) Optionally save a convenient copy next to the script\n",
    "    if FORCE_SAVE_CONLLU:\n",
    "        local_out = Path(\"bn_manual.pred.conllu\")\n",
    "        local_out.write_text(Path(out_path).read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "        print(f\"\\nSaved copy to: {local_out.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eee322",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Manual parse of a Bengali paragraph with a saved SuPar CRF2o model.\n",
    "- Pre-tokenizes (no Stanza).\n",
    "- Removes stray 'probs' from transform.fields (does NOT touch read-only src/tgt).\n",
    "- Forces single-bucket inference to avoid k-means issues.\n",
    "\"\"\"\n",
    "import re\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "# === CONFIG ===\n",
    "MODEL_PT = \"outputs_dep/supar_crf2o_xlmr_en/parser.pt\"\n",
    "TEXT = (\n",
    "    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡ßç‡¶¶‡ßã-‡¶Ü‡¶∞‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ì ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§‡•§ \"\n",
    "    \"‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶∏‡¶™‡ßç‡¶§‡¶Æ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"\n",
    ")\n",
    "SAVE_COPY = True  # write bn_manual.pred.conllu next to the script\n",
    "\n",
    "# --- simple Bengali sentence split (keep ‡•§?! as sentence-final) ---\n",
    "def split_sentences(s: str):\n",
    "    parts = re.split(r'([‡•§!?])', s.strip())\n",
    "    out = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        seg = parts[i].strip()\n",
    "        punct = parts[i+1] if i+1 < len(parts) else \"\"\n",
    "        if seg:\n",
    "            out.append((seg + (punct or \"\")).strip())\n",
    "    return out\n",
    "\n",
    "# --- Bengali tokenizer: keep U+0980‚ÄìU+09FF word blocks (allow hyphens), digits, or single punctuation ---\n",
    "BN_BLOCK = r\"\\u0980-\\u09FF\"\n",
    "WORD_RE = re.compile(fr\"[{BN_BLOCK}]+(?:-[{BN_BLOCK}]+)*|\\d+|[^\\s]\", re.UNICODE)\n",
    "def tokenize_bn(sent: str): return WORD_RE.findall(sent)\n",
    "\n",
    "def to_conllu(tokens, sent_id=1):\n",
    "    lines = [f\"# sent_id = {sent_id}\", \"# text = \" + \" \".join(tokens)]\n",
    "    for i, tok in enumerate(tokens, 1):\n",
    "        # ID  FORM  LEMMA  UPOS  XPOS  FEATS  HEAD  DEPREL  DEPS  MISC\n",
    "        lines.append(f\"{i}\\t{tok}\\t_\\t_\\t_\\t_\\t0\\tdep\\t_\\t_\")\n",
    "    lines.append(\"\")  # blank line terminator\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def strip_probs_from_fields(transform):\n",
    "    \"\"\"\n",
    "    Only remove the *name* 'probs' from transform.fields.\n",
    "    DO NOT touch read-only properties like src/tgt.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fields = getattr(transform, \"fields\", None)\n",
    "        if fields is None:\n",
    "            return\n",
    "        # normalize to list, filter, then write back using safest method\n",
    "        flist = list(fields)\n",
    "        if \"probs\" in flist:\n",
    "            flist = [x for x in flist if x != \"probs\"]\n",
    "            try:\n",
    "                transform.fields = type(fields)(flist)  # preserve list/tuple type\n",
    "            except Exception:\n",
    "                # fallback: write directly into __dict__\n",
    "                transform.__dict__[\"fields\"] = flist\n",
    "        # clear any cached flattened fields so SuPar recomputes them\n",
    "        for cache_attr in (\"_flattened_fields\", \"flattened\", \"__flattened\"):\n",
    "            if hasattr(transform, cache_attr):\n",
    "                try:\n",
    "                    setattr(transform, cache_attr, None)\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        delattr(transform, cache_attr)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    except Exception:\n",
    "        # Last resort: if transform has attribute named 'probs', drop it so getattr won't fail\n",
    "        if hasattr(transform, \"probs\"):\n",
    "            try:\n",
    "                delattr(transform, \"probs\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 1) Pre-tokenize the paragraph\n",
    "    sents = [tokenize_bn(s) for s in split_sentences(TEXT)]\n",
    "    print(\"Pre-tokenized sentences:\")\n",
    "    for i, toks in enumerate(sents, 1):\n",
    "        print(f\"{i:>2}: {' '.join(toks)}\")\n",
    "\n",
    "    # 2) Write a clean temporary CoNLL-U file\n",
    "    tmpdir = Path(tempfile.mkdtemp(prefix=\"bn_manual_\"))\n",
    "    in_path  = tmpdir / \"in.conllu\"\n",
    "    out_path = tmpdir / \"pred.conllu\"\n",
    "    with open(in_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sid, toks in enumerate(sents, 1):\n",
    "            f.write(to_conllu(toks, sid) + \"\\n\")  # ensure final newline\n",
    "\n",
    "    # 3) Load model and move to device\n",
    "    parser = CRF2oDependencyParser.load(MODEL_PT)\n",
    "    try:\n",
    "        parser.model.to(device)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) Remove stray 'probs' ONLY from fields (do not touch src/tgt)\n",
    "    strip_probs_from_fields(parser.transform)\n",
    "\n",
    "    # 5) Predict: skip probabilities and bucketing\n",
    "    parser.predict(\n",
    "        str(in_path),\n",
    "        pred=str(out_path),\n",
    "        lang=None,\n",
    "        prob=False, mbr=False, tree=False,\n",
    "        buckets=1, batch_size=5000,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 6) Pretty-print a compact view\n",
    "    print(f\"\\nPredicted CoNLL-U: {out_path}\")\n",
    "    print(\"\\n--- Parsed output ---\")\n",
    "    cur = 0\n",
    "    with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"# sent_id\"):\n",
    "                cur += 1\n",
    "                print(f\"\\nSentence {cur}\")\n",
    "                print(\" ID  FORM                 HEAD  DEPREL\")\n",
    "                print(\"---- -------------------- ----  ------------\")\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            tid, form, *_rest, head, rel, _deps, _misc = cols[0], cols[1], cols[2:7], cols[6], cols[7], cols[8], cols[9]\n",
    "            if \"-\" in tid or \".\" in tid:  # skip MWT/empty nodes if ever present\n",
    "                continue\n",
    "            print(f\"{int(tid):>3}  {form:<20} {head:>4}  {rel:<12}\")\n",
    "\n",
    "    if SAVE_COPY:\n",
    "        local = Path(\"bn_manual.pred.conllu\")\n",
    "        local.write_text(out_path.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "        print(f\"\\nSaved copy: {local.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe9e3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Manual parse of a Bengali paragraph with a saved SuPar CRF2o model.\n",
    "- Pre-tokenizes (no Stanza).\n",
    "- Removes stray 'probs' from transform.fields (does NOT touch read-only src/tgt).\n",
    "- Forces single-bucket inference and tree decoding to get a proper UD tree.\n",
    "\"\"\"\n",
    "import re\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from supar.parsers import CRF2oDependencyParser\n",
    "\n",
    "# === CONFIG ===\n",
    "MODEL_PT = \"outputs_dep/supar_crf2o_xlmr_en/parser.pt\"\n",
    "TEXT = (\n",
    "    \"‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á‡¶®‡ßç‡¶¶‡ßã-‡¶Ü‡¶∞‡ßç‡¶Ø ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶Ø‡¶æ ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶¶‡ßá‡¶∂ ‡¶ì ‡¶≠‡¶æ‡¶∞‡¶§‡ßá‡¶∞ ‡¶™‡¶∂‡ßç‡¶ö‡¶ø‡¶Æ‡¶¨‡¶ô‡ßç‡¶ó‡¶∏‡¶π ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶Ö‡¶û‡ßç‡¶ö‡¶≤‡ßá ‡¶™‡ßç‡¶∞‡¶ö‡¶≤‡¶ø‡¶§‡•§ \"\n",
    "    \"‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶∂‡ßç‡¶¨‡ßá‡¶∞ ‡¶∏‡¶™‡ßç‡¶§‡¶Æ ‡¶¨‡ßÉ‡¶π‡¶§‡ßç‡¶§‡¶Æ ‡¶≠‡¶æ‡¶∑‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ‡¶≠‡¶æ‡¶∑‡ßÄ ‡¶ú‡¶æ‡¶§‡¶ø‡¶∞ ‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"\n",
    ")\n",
    "SAVE_COPY = True  # write bn_manual.pred.conllu next to the script\n",
    "\n",
    "# --- simple Bengali sentence split (keep ‡•§?! as sentence-final) ---\n",
    "def split_sentences(s: str):\n",
    "    parts = re.split(r'([‡•§!?])', s.strip())\n",
    "    out = []\n",
    "    for i in range(0, len(parts), 2):\n",
    "        seg = parts[i].strip()\n",
    "        punct = parts[i+1] if i+1 < len(parts) else \"\"\n",
    "        if seg:\n",
    "            out.append((seg + (punct or \"\")).strip())\n",
    "    return out\n",
    "\n",
    "# --- Bengali tokenizer: keep U+0980‚ÄìU+09FF word blocks (allow hyphens), digits, or single punctuation ---\n",
    "BN_BLOCK = r\"\\u0980-\\u09FF\"\n",
    "WORD_RE = re.compile(fr\"[{BN_BLOCK}]+(?:-[{BN_BLOCK}]+)*|\\d+|[^\\s]\", re.UNICODE)\n",
    "def tokenize_bn(sent: str): return WORD_RE.findall(sent)\n",
    "\n",
    "def to_conllu(tokens, sent_id=1):\n",
    "    lines = [f\"# sent_id = {sent_id}\", \"# text = \" + \" \".join(tokens)]\n",
    "    for i, tok in enumerate(tokens, 1):\n",
    "        # ID  FORM  LEMMA  UPOS  XPOS  FEATS  HEAD  DEPREL  DEPS  MISC\n",
    "        lines.append(f\"{i}\\t{tok}\\t_\\t_\\t_\\t_\\t0\\tdep\\t_\\t_\")\n",
    "    lines.append(\"\")  # blank line terminator\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def strip_probs_from_fields(transform):\n",
    "    \"\"\"\n",
    "    Only remove the *name* 'probs' from transform.fields.\n",
    "    DO NOT touch read-only properties like src/tgt.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        fields = getattr(transform, \"fields\", None)\n",
    "        if fields is None:\n",
    "            return\n",
    "        flist = list(fields)\n",
    "        if \"probs\" in flist:\n",
    "            flist = [x for x in flist if x != \"probs\"]\n",
    "            try:\n",
    "                transform.fields = type(fields)(flist)  # preserve list/tuple\n",
    "            except Exception:\n",
    "                # fallback: write into __dict__ if setter is missing\n",
    "                transform.__dict__[\"fields\"] = flist\n",
    "        # clear any cached flattened fields so SuPar recomputes them\n",
    "        for cache_attr in (\"_flattened_fields\", \"flattened\", \"__flattened\"):\n",
    "            if hasattr(transform, cache_attr):\n",
    "                try:\n",
    "                    setattr(transform, cache_attr, None)\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        delattr(transform, cache_attr)\n",
    "                    except Exception:\n",
    "                        pass\n",
    "    except Exception:\n",
    "        # last resort: if transform has attribute literally named 'probs', drop it\n",
    "        if hasattr(transform, \"probs\"):\n",
    "            try:\n",
    "                delattr(transform, \"probs\")\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "def main():\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    # 1) Pre-tokenize the paragraph\n",
    "    sents = [tokenize_bn(s) for s in split_sentences(TEXT)]\n",
    "    print(\"Pre-tokenized sentences:\")\n",
    "    for i, toks in enumerate(sents, 1):\n",
    "        print(f\"{i:>2}: {' '.join(toks)}\")\n",
    "\n",
    "    # 2) Write a clean temporary CoNLL-U file (ensure final newline)\n",
    "    tmpdir = Path(tempfile.mkdtemp(prefix=\"bn_manual_\"))\n",
    "    in_path  = tmpdir / \"in.conllu\"\n",
    "    out_path = tmpdir / \"pred.conllu\"\n",
    "    with open(in_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for sid, toks in enumerate(sents, 1):\n",
    "            f.write(to_conllu(toks, sid) + \"\\n\")\n",
    "\n",
    "    # 3) Load model and move to device\n",
    "    parser = CRF2oDependencyParser.load(MODEL_PT)\n",
    "    try:\n",
    "        parser.model.to(device)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 4) Remove stray 'probs' ONLY from fields (do not touch src/tgt)\n",
    "    strip_probs_from_fields(parser.transform)\n",
    "\n",
    "    # 5) Predict ‚Äî enforce tree, skip probabilities, avoid k-means bucketing\n",
    "    parser.predict(\n",
    "        str(in_path),\n",
    "        pred=str(out_path),\n",
    "        lang=None,\n",
    "        prob=False, mbr=False,\n",
    "        tree=True,            # <- enforce single-root tree (head=0, deprel=root)\n",
    "        proj=False,           # allow non-projective (UD-friendly)\n",
    "        buckets=1, batch_size=5000,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    # 6) Pretty-print a compact view\n",
    "    print(f\"\\nPredicted CoNLL-U: {out_path}\")\n",
    "    print(\"\\n--- Parsed output ---\")\n",
    "    cur = 0\n",
    "    with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"# sent_id\"):\n",
    "                cur += 1\n",
    "                print(f\"\\nSentence {cur}\")\n",
    "                print(\" ID  FORM                 HEAD  DEPREL\")\n",
    "                print(\"---- -------------------- ----  ------------\")\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            tid, form, lemma, upos, xpos, feats, head, rel, deps, misc = cols\n",
    "            if \"-\" in tid or \".\" in tid:  # skip MWT/empty nodes if ever present\n",
    "                continue\n",
    "            print(f\"{int(tid):>3}  {form:<20} {head:>4}  {rel:<12}\")\n",
    "\n",
    "    # 7) Sanity check: count roots & self-loops\n",
    "    roots = self_loops = sents_count = 0\n",
    "    with open(out_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            if line.startswith(\"# sent_id\"):\n",
    "                sents_count += 1\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            tid, head, rel = cols[0], cols[6], cols[7]\n",
    "            if \"-\" in tid or \".\" in tid:\n",
    "                continue\n",
    "            if head == \"0\" and rel.lower() == \"root\":\n",
    "                roots += 1\n",
    "            if tid == head:\n",
    "                self_loops += 1\n",
    "    print(f\"\\n[check] roots={roots}/{sents_count}, self_loops={self_loops}  (expect roots={sents_count}, self_loops=0)\")\n",
    "\n",
    "    if SAVE_COPY:\n",
    "        local = Path(\"bn_manual.pred.conllu\")\n",
    "        local.write_text(out_path.read_text(encoding=\"utf-8\"), encoding=\"utf-8\")\n",
    "        print(f\"\\nSaved copy: {local.resolve()}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9660bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold = read_conllu_rows(BN_TEST)\n",
    "pred = read_conllu_rows(PRED_BN)\n",
    "assert len(gold)==len(pred), \"Mismatch #sents\"\n",
    "\n",
    "labels = sorted(list({x for r in gold for x in r[\"deprel\"]} | {x for r in pred for x in r[\"deprel\"]}))\n",
    "lab2i = {l:i for i,l in enumerate(labels)}\n",
    "cm = np.zeros((len(labels), len(labels)), dtype=np.int64)\n",
    "for g, p in zip(gold, pred):\n",
    "    n = min(len(g[\"deprel\"]), len(p[\"deprel\"]))\n",
    "    for i in range(n):\n",
    "        cm[lab2i[g[\"deprel\"][i]], lab2i[p[\"deprel\"][i]]] += 1\n",
    "\n",
    "cmn = cm/(cm.sum(axis=1, keepdims=True)+1e-9)\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(cmn, xticklabels=labels, yticklabels=labels, cmap=\"Blues\")\n",
    "plt.title(\"BN-BRU DEPREL Confusion (Zero-shot)\"); plt.xlabel(\"Pred\"); plt.ylabel(\"Gold\")\n",
    "plt.tight_layout(); plt.show()\n",
    "\n",
    "corr = np.corrcoef(cmn + 1e-12)\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr, xticklabels=labels, yticklabels=labels, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"DEPREL Row Correlation\"); plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817c2af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gimport re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "BN_TEST = \"data/ud/bn_bru/bn_bru-test.conllu\"                 # gold\n",
    "PRED_BN = \"outputs_dep/supar_crf2o_xlmr_en/bn_bru-test.posttrain.pred.conllu\"  # pred\n",
    "\n",
    "def read_conllu_rows(path, strip_subtypes=True):\n",
    "    \"\"\"\n",
    "    Returns a list of sentences; each sentence is a dict with:\n",
    "      - 'deprel': [list of dependency labels per token]\n",
    "    Skips multi-word tokens (1-2) and empty nodes (1.1).\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    cur_deps = []\n",
    "    re_id_ok = re.compile(r\"^\\d+$\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if cur_deps:\n",
    "                    sents.append({\"deprel\": cur_deps})\n",
    "                    cur_deps = []\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            tid = cols[0]\n",
    "            if not re_id_ok.match(tid):\n",
    "                # skip MWT and empty nodes\n",
    "                continue\n",
    "            rel = cols[7]\n",
    "            if strip_subtypes and \":\" in rel:\n",
    "                rel = rel.split(\":\", 1)[0]\n",
    "            cur_deps.append(rel)\n",
    "    if cur_deps:\n",
    "        sents.append({\"deprel\": cur_deps})\n",
    "    return sents\n",
    "\n",
    "# ---- read data ----\n",
    "gold = read_conllu_rows(BN_TEST, strip_subtypes=True)\n",
    "pred = read_conllu_rows(PRED_BN, strip_subtypes=True)\n",
    "assert len(gold) == len(pred), f\"Mismatch #sents gold={len(gold)} pred={len(pred)}\"\n",
    "\n",
    "# ---- label set (union), ordered by gold frequency desc then alpha ----\n",
    "gold_counts = Counter([rel for s in gold for rel in s[\"deprel\"]])\n",
    "pred_counts = Counter([rel for s in pred for rel in s[\"deprel\"]])\n",
    "all_labels = set(gold_counts) | set(pred_counts)\n",
    "labels = sorted(all_labels, key=lambda x: (-gold_counts.get(x, 0), x))\n",
    "lab2i = {l: i for i, l in enumerate(labels)}\n",
    "\n",
    "# ---- confusion matrix ----\n",
    "cm = np.zeros((len(labels), len(labels)), dtype=np.int64)\n",
    "for g, p in zip(gold, pred):\n",
    "    n = min(len(g[\"deprel\"]), len(p[\"deprel\"]))\n",
    "    for i in range(n):\n",
    "        gi = lab2i[g[\"deprel\"][i]]\n",
    "        pi = lab2i[p[\"deprel\"][i]]\n",
    "        cm[gi, pi] += 1\n",
    "\n",
    "# ---- row-normalized matrix (gold rows sum to 1) ----\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "cmn = cm / np.clip(row_sums, 1, None)  # avoid division by 0\n",
    "\n",
    "# ---- plots ----\n",
    "plt.figure(figsize=(max(10, 0.5*len(labels)), max(8, 0.5*len(labels))))\n",
    "sns.heatmap(cmn, xticklabels=labels, yticklabels=labels, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "plt.title(\"BN-BRU DEPREL Confusion (row-normalized; gold on rows)\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"Gold label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Row correlation (how similar each gold label's confusion distribution is to others)\n",
    "# Add tiny epsilon & handle NaNs from constant rows\n",
    "eps = 1e-12\n",
    "corr = np.corrcoef(cmn + eps)\n",
    "corr = np.nan_to_num(corr, nan=0.0)\n",
    "\n",
    "plt.figure(figsize=(max(8, 0.45*len(labels)), max(6, 0.45*len(labels))))\n",
    "sns.heatmap(corr, xticklabels=labels, yticklabels=labels, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"DEPREL Row Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- quick textual summary ----\n",
    "print(\"Top-10 gold labels by frequency:\")\n",
    "for lab, cnt in gold_counts.most_common(10):\n",
    "    acc = (cm[lab2i[lab], lab2i[lab]] / max(1, cm[lab2i[lab], :].sum())) * 100\n",
    "    print(f\"{lab:>10s}  count={cnt:>5d}  diag-acc={acc:5.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1045505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "BN_TEST = \"data/ud/bn_bru/bn_bru-test.conllu\"                 # gold\n",
    "PRED_BN = \"outputs_dep/supar_crf2o_xlmr_en/bn_bru-test.posttrain.pred.conllu\"  # pred\n",
    "\n",
    "def read_conllu_rows(path, strip_subtypes=True):\n",
    "    \"\"\"\n",
    "    Returns a list of sentences; each sentence is a dict with:\n",
    "      - 'deprel': [list of dependency labels per token]\n",
    "    Skips multi-word tokens (1-2) and empty nodes (1.1).\n",
    "    \"\"\"\n",
    "    sents = []\n",
    "    cur_deps = []\n",
    "    re_id_ok = re.compile(r\"^\\d+$\")\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "            if not line:\n",
    "                if cur_deps:\n",
    "                    sents.append({\"deprel\": cur_deps})\n",
    "                    cur_deps = []\n",
    "                continue\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "            cols = line.split(\"\\t\")\n",
    "            if len(cols) != 10:\n",
    "                continue\n",
    "            tid = cols[0]\n",
    "            if not re_id_ok.match(tid):\n",
    "                # skip MWT and empty nodes\n",
    "                continue\n",
    "            rel = cols[7]\n",
    "            if strip_subtypes and \":\" in rel:\n",
    "                rel = rel.split(\":\", 1)[0]\n",
    "            cur_deps.append(rel)\n",
    "    if cur_deps:\n",
    "        sents.append({\"deprel\": cur_deps})\n",
    "    return sents\n",
    "\n",
    "# ---- read data ----\n",
    "gold = read_conllu_rows(BN_TEST, strip_subtypes=True)\n",
    "pred = read_conllu_rows(PRED_BN, strip_subtypes=True)\n",
    "assert len(gold) == len(pred), f\"Mismatch #sents gold={len(gold)} pred={len(pred)}\"\n",
    "\n",
    "# ---- label set (union), ordered by gold frequency desc then alpha ----\n",
    "gold_counts = Counter([rel for s in gold for rel in s[\"deprel\"]])\n",
    "pred_counts = Counter([rel for s in pred for rel in s[\"deprel\"]])\n",
    "all_labels = set(gold_counts) | set(pred_counts)\n",
    "labels = sorted(all_labels, key=lambda x: (-gold_counts.get(x, 0), x))\n",
    "lab2i = {l: i for i, l in enumerate(labels)}\n",
    "\n",
    "# ---- confusion matrix ----\n",
    "cm = np.zeros((len(labels), len(labels)), dtype=np.int64)\n",
    "for g, p in zip(gold, pred):\n",
    "    n = min(len(g[\"deprel\"]), len(p[\"deprel\"]))\n",
    "    for i in range(n):\n",
    "        gi = lab2i[g[\"deprel\"][i]]\n",
    "        pi = lab2i[p[\"deprel\"][i]]\n",
    "        cm[gi, pi] += 1\n",
    "\n",
    "# ---- row-normalized matrix (gold rows sum to 1) ----\n",
    "row_sums = cm.sum(axis=1, keepdims=True)\n",
    "cmn = cm / np.clip(row_sums, 1, None)  # avoid division by 0\n",
    "\n",
    "# ---- plots ----\n",
    "plt.figure(figsize=(max(10, 0.5*len(labels)), max(8, 0.5*len(labels))))\n",
    "sns.heatmap(cmn, xticklabels=labels, yticklabels=labels, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "plt.title(\"BN-BRU DEPREL Confusion (row-normalized; gold on rows)\")\n",
    "plt.xlabel(\"Predicted label\")\n",
    "plt.ylabel(\"Gold label\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Row correlation (how similar each gold label's confusion distribution is to others)\n",
    "# Add tiny epsilon & handle NaNs from constant rows\n",
    "eps = 1e-12\n",
    "corr = np.corrcoef(cmn + eps)\n",
    "corr = np.nan_to_num(corr, nan=0.0)\n",
    "\n",
    "plt.figure(figsize=(max(8, 0.45*len(labels)), max(6, 0.45*len(labels))))\n",
    "sns.heatmap(corr, xticklabels=labels, yticklabels=labels, cmap=\"coolwarm\", center=0)\n",
    "plt.title(\"DEPREL Row Correlation\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- quick textual summary ----\n",
    "print(\"Top-10 gold labels by frequency:\")\n",
    "for lab, cnt in gold_counts.most_common(10):\n",
    "    acc = (cm[lab2i[lab], lab2i[lab]] / max(1, cm[lab2i[lab], :].sum())) * 100\n",
    "    print(f\"{lab:>10s}  count={cnt:>5d}  diag-acc={acc:5.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
