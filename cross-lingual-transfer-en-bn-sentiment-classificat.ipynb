{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13120715,"sourceType":"datasetVersion","datasetId":8311621},{"sourceId":13120732,"sourceType":"datasetVersion","datasetId":8311630},{"sourceId":13121138,"sourceType":"datasetVersion","datasetId":8311864},{"sourceId":13121244,"sourceType":"datasetVersion","datasetId":8311927}],"dockerImageVersionId":31090,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"1db34d8e","cell_type":"code","source":"import transformers\nprint(transformers.__version__)\nfrom transformers import TrainingArguments\nhelp(TrainingArguments)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:14:33.765897Z","iopub.execute_input":"2025-09-21T07:14:33.766110Z","iopub.status.idle":"2025-09-21T07:14:43.941535Z","shell.execute_reply.started":"2025-09-21T07:14:33.766056Z","shell.execute_reply":"2025-09-21T07:14:43.940743Z"}},"outputs":[{"name":"stdout","text":"4.52.4\nHelp on class TrainingArguments in module transformers.training_args:\n\nclass TrainingArguments(builtins.object)\n |  TrainingArguments(output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict, str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict, str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict, str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n |  \n |  TrainingArguments is the subset of the arguments we use in our example scripts **which relate to the training loop\n |  itself**.\n |  \n |  Using [`HfArgumentParser`] we can turn this class into\n |  [argparse](https://docs.python.org/3/library/argparse#module-argparse) arguments that can be specified on the\n |  command line.\n |  \n |  Parameters:\n |      output_dir (`str`, *optional*, defaults to `\"trainer_output\"`):\n |          The output directory where the model predictions and checkpoints will be written.\n |      overwrite_output_dir (`bool`, *optional*, defaults to `False`):\n |          If `True`, overwrite the content of the output directory. Use this to continue training if `output_dir`\n |          points to a checkpoint directory.\n |      do_train (`bool`, *optional*, defaults to `False`):\n |          Whether to run training or not. This argument is not directly used by [`Trainer`], it's intended to be used\n |          by your training/evaluation scripts instead. See the [example\n |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n |      do_eval (`bool`, *optional*):\n |          Whether to run evaluation on the validation set or not. Will be set to `True` if `eval_strategy` is\n |          different from `\"no\"`. This argument is not directly used by [`Trainer`], it's intended to be used by your\n |          training/evaluation scripts instead. See the [example\n |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n |      do_predict (`bool`, *optional*, defaults to `False`):\n |          Whether to run predictions on the test set or not. This argument is not directly used by [`Trainer`], it's\n |          intended to be used by your training/evaluation scripts instead. See the [example\n |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n |      eval_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n |          The evaluation strategy to adopt during training. Possible values are:\n |  \n |              - `\"no\"`: No evaluation is done during training.\n |              - `\"steps\"`: Evaluation is done (and logged) every `eval_steps`.\n |              - `\"epoch\"`: Evaluation is done at the end of each epoch.\n |  \n |      prediction_loss_only (`bool`, *optional*, defaults to `False`):\n |          When performing evaluation and generating predictions, only returns the loss.\n |      per_device_train_batch_size (`int`, *optional*, defaults to 8):\n |          The batch size per device accelerator core/CPU for training.\n |      per_device_eval_batch_size (`int`, *optional*, defaults to 8):\n |          The batch size per device accelerator core/CPU for evaluation.\n |      gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n |          Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n |  \n |          <Tip warning={true}>\n |  \n |          When using gradient accumulation, one step is counted as one step with backward pass. Therefore, logging,\n |          evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training examples.\n |  \n |          </Tip>\n |  \n |      eval_accumulation_steps (`int`, *optional*):\n |          Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If\n |          left unset, the whole predictions are accumulated on the device accelerator before being moved to the CPU (faster but\n |          requires more memory).\n |      eval_delay (`float`, *optional*):\n |          Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n |          eval_strategy.\n |      torch_empty_cache_steps (`int`, *optional*):\n |          Number of steps to wait before calling `torch.<device>.empty_cache()`. If left unset or set to None, cache will not be emptied.\n |  \n |          <Tip>\n |  \n |          This can help avoid CUDA out-of-memory errors by lowering peak VRAM usage at a cost of about [10% slower performance](https://github.com/huggingface/transformers/issues/31372).\n |  \n |          </Tip>\n |  \n |      learning_rate (`float`, *optional*, defaults to 5e-5):\n |          The initial learning rate for [`AdamW`] optimizer.\n |      weight_decay (`float`, *optional*, defaults to 0):\n |          The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in [`AdamW`]\n |          optimizer.\n |      adam_beta1 (`float`, *optional*, defaults to 0.9):\n |          The beta1 hyperparameter for the [`AdamW`] optimizer.\n |      adam_beta2 (`float`, *optional*, defaults to 0.999):\n |          The beta2 hyperparameter for the [`AdamW`] optimizer.\n |      adam_epsilon (`float`, *optional*, defaults to 1e-8):\n |          The epsilon hyperparameter for the [`AdamW`] optimizer.\n |      max_grad_norm (`float`, *optional*, defaults to 1.0):\n |          Maximum gradient norm (for gradient clipping).\n |      num_train_epochs(`float`, *optional*, defaults to 3.0):\n |          Total number of training epochs to perform (if not an integer, will perform the decimal part percents of\n |          the last epoch before stopping training).\n |      max_steps (`int`, *optional*, defaults to -1):\n |          If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n |          For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n |          `max_steps` is reached.\n |      lr_scheduler_type (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n |          The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n |      lr_scheduler_kwargs ('dict', *optional*, defaults to {}):\n |          The extra arguments for the lr_scheduler. See the documentation of each scheduler for possible values.\n |      warmup_ratio (`float`, *optional*, defaults to 0.0):\n |          Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n |      warmup_steps (`int`, *optional*, defaults to 0):\n |          Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of `warmup_ratio`.\n |      log_level (`str`, *optional*, defaults to `passive`):\n |          Logger log level to use on the main process. Possible choices are the log levels as strings: 'debug',\n |          'info', 'warning', 'error' and 'critical', plus a 'passive' level which doesn't set anything and keeps the\n |          current log level for the Transformers library (which will be `\"warning\"` by default).\n |      log_level_replica (`str`, *optional*, defaults to `\"warning\"`):\n |          Logger log level to use on replicas. Same choices as `log_level`\"\n |      log_on_each_node (`bool`, *optional*, defaults to `True`):\n |          In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n |          node.\n |      logging_dir (`str`, *optional*):\n |          [TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\n |          *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\n |      logging_strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n |          The logging strategy to adopt during training. Possible values are:\n |  \n |              - `\"no\"`: No logging is done during training.\n |              - `\"epoch\"`: Logging is done at the end of each epoch.\n |              - `\"steps\"`: Logging is done every `logging_steps`.\n |  \n |      logging_first_step (`bool`, *optional*, defaults to `False`):\n |          Whether to log the first `global_step` or not.\n |      logging_steps (`int` or `float`, *optional*, defaults to 500):\n |          Number of update steps between two logs if `logging_strategy=\"steps\"`. Should be an integer or a float in\n |          range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n |      logging_nan_inf_filter (`bool`, *optional*, defaults to `True`):\n |          Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is `nan`\n |          or `inf` is filtered and the average loss of the current logging window is taken instead.\n |  \n |          <Tip>\n |  \n |          `logging_nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n |          gradient is computed or applied to the model.\n |  \n |          </Tip>\n |  \n |      save_strategy (`str` or [`~trainer_utils.SaveStrategy`], *optional*, defaults to `\"steps\"`):\n |          The checkpoint save strategy to adopt during training. Possible values are:\n |  \n |              - `\"no\"`: No save is done during training.\n |              - `\"epoch\"`: Save is done at the end of each epoch.\n |              - `\"steps\"`: Save is done every `save_steps`.\n |              - `\"best\"`: Save is done whenever a new `best_metric` is achieved.\n |  \n |              If `\"epoch\"` or `\"steps\"` is chosen, saving will also be performed at the\n |              very end of training, always.\n |      save_steps (`int` or `float`, *optional*, defaults to 500):\n |          Number of updates steps before two checkpoint saves if `save_strategy=\"steps\"`. Should be an integer or a\n |          float in range `[0,1)`. If smaller than 1, will be interpreted as ratio of total training steps.\n |      save_total_limit (`int`, *optional*):\n |          If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n |          `output_dir`. When `load_best_model_at_end` is enabled, the \"best\" checkpoint according to\n |          `metric_for_best_model` will always be retained in addition to the most recent ones. For example, for\n |          `save_total_limit=5` and `load_best_model_at_end`, the four last checkpoints will always be retained\n |          alongside the best model. When `save_total_limit=1` and `load_best_model_at_end`, it is possible that two\n |          checkpoints are saved: the last one and the best one (if they are different).\n |      save_safetensors (`bool`, *optional*, defaults to `True`):\n |          Use [safetensors](https://huggingface.co/docs/safetensors) saving and loading for state dicts instead of\n |          default `torch.load` and `torch.save`.\n |      save_on_each_node (`bool`, *optional*, defaults to `False`):\n |          When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on\n |          the main one.\n |  \n |          This should not be activated when the different nodes use the same storage as the files will be saved with\n |          the same names for each node.\n |      save_only_model (`bool`, *optional*, defaults to `False`):\n |          When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state.\n |          Note that when this is true, you won't be able to resume training from checkpoint.\n |          This enables you to save storage by not storing the optimizer, scheduler & rng state.\n |          You can only load the model using `from_pretrained` with this option set to `True`.\n |      restore_callback_states_from_checkpoint (`bool`, *optional*, defaults to `False`):\n |          Whether to restore the callback states from the checkpoint. If `True`, will override\n |          callbacks passed to the `Trainer` if they exist in the checkpoint.\"\n |      use_cpu (`bool`, *optional*, defaults to `False`):\n |          Whether or not to use cpu. If set to False, we will use cuda or mps device if available.\n |      seed (`int`, *optional*, defaults to 42):\n |          Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use the\n |          [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized parameters.\n |      data_seed (`int`, *optional*):\n |          Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n |          same seed as `seed`. This can be used to ensure reproducibility of data sampling, independent of the model\n |          seed.\n |      jit_mode_eval (`bool`, *optional*, defaults to `False`):\n |          Whether or not to use PyTorch jit trace for inference.\n |      use_ipex (`bool`, *optional*, defaults to `False`):\n |          Use Intel extension for PyTorch when it is available. [IPEX\n |          installation](https://github.com/intel/intel-extension-for-pytorch).\n |      bf16 (`bool`, *optional*, defaults to `False`):\n |          Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires Ampere or higher\n |          NVIDIA architecture or using CPU (use_cpu) or Ascend NPU. This is an experimental API and it may change.\n |      fp16 (`bool`, *optional*, defaults to `False`):\n |          Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.\n |      fp16_opt_level (`str`, *optional*, defaults to 'O1'):\n |          For `fp16` training, Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']. See details on\n |          the [Apex documentation](https://nvidia.github.io/apex/amp).\n |      fp16_backend (`str`, *optional*, defaults to `\"auto\"`):\n |          This argument is deprecated. Use `half_precision_backend` instead.\n |      half_precision_backend (`str`, *optional*, defaults to `\"auto\"`):\n |          The backend to use for mixed precision training. Must be one of `\"auto\", \"apex\", \"cpu_amp\"`. `\"auto\"` will\n |          use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the\n |          requested backend.\n |      bf16_full_eval (`bool`, *optional*, defaults to `False`):\n |          Whether to use full bfloat16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n |          metric values. This is an experimental API and it may change.\n |      fp16_full_eval (`bool`, *optional*, defaults to `False`):\n |          Whether to use full float16 evaluation instead of 32-bit. This will be faster and save memory but can harm\n |          metric values.\n |      tf32 (`bool`, *optional*):\n |          Whether to enable the TF32 mode, available in Ampere and newer GPU architectures. The default value depends\n |          on PyTorch's version default of `torch.backends.cuda.matmul.allow_tf32`. For more details please refer to\n |          the [TF32](https://huggingface.co/docs/transformers/perf_train_gpu_one#tf32) documentation. This is an\n |          experimental API and it may change.\n |      local_rank (`int`, *optional*, defaults to -1):\n |          Rank of the process during distributed training.\n |      ddp_backend (`str`, *optional*):\n |          The backend to use for distributed training. Must be one of `\"nccl\"`, `\"mpi\"`, `\"ccl\"`, `\"gloo\"`, `\"hccl\"`.\n |      tpu_num_cores (`int`, *optional*):\n |          When training on TPU, the number of TPU cores (automatically passed by launcher script).\n |      dataloader_drop_last (`bool`, *optional*, defaults to `False`):\n |          Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size)\n |          or not.\n |      eval_steps (`int` or `float`, *optional*):\n |          Number of update steps between two evaluations if `eval_strategy=\"steps\"`. Will default to the same\n |          value as `logging_steps` if not set. Should be an integer or a float in range `[0,1)`. If smaller than 1,\n |          will be interpreted as ratio of total training steps.\n |      dataloader_num_workers (`int`, *optional*, defaults to 0):\n |          Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the\n |          main process.\n |      past_index (`int`, *optional*, defaults to -1):\n |          Some models like [TransformerXL](../model_doc/transformerxl) or [XLNet](../model_doc/xlnet) can make use of\n |          the past hidden states for their predictions. If this argument is set to a positive int, the `Trainer` will\n |          use the corresponding output (usually index 2) as the past state and feed it to the model at the next\n |          training step under the keyword argument `mems`.\n |      run_name (`str`, *optional*, defaults to `output_dir`):\n |          A descriptor for the run. Typically used for [wandb](https://www.wandb.com/),\n |          [mlflow](https://www.mlflow.org/), [comet](https://www.comet.com/site) and [swanlab](https://swanlab.cn)\n |          logging. If not specified, will be the same as `output_dir`.\n |      disable_tqdm (`bool`, *optional*):\n |          Whether or not to disable the tqdm progress bars and table of metrics produced by\n |          [`~notebook.NotebookTrainingTracker`] in Jupyter Notebooks. Will default to `True` if the logging level is\n |          set to warn or lower (default), `False` otherwise.\n |      remove_unused_columns (`bool`, *optional*, defaults to `True`):\n |          Whether or not to automatically remove the columns unused by the model forward method.\n |      label_names (`List[str]`, *optional*):\n |          The list of keys in your dictionary of inputs that correspond to the labels.\n |  \n |          Will eventually default to the list of argument names accepted by the model that contain the word \"label\",\n |          except if the model used is one of the `XxxForQuestionAnswering` in which case it will also include the\n |          `[\"start_positions\", \"end_positions\"]` keys.\n |      load_best_model_at_end (`bool`, *optional*, defaults to `False`):\n |          Whether or not to load the best model found during training at the end of training. When this option is\n |          enabled, the best checkpoint will always be saved. See\n |          [`save_total_limit`](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments.save_total_limit)\n |          for more.\n |  \n |          <Tip>\n |  \n |          When set to `True`, the parameters `save_strategy` needs to be the same as `eval_strategy`, and in\n |          the case it is \"steps\", `save_steps` must be a round multiple of `eval_steps`.\n |  \n |          </Tip>\n |  \n |      metric_for_best_model (`str`, *optional*):\n |          Use in conjunction with `load_best_model_at_end` to specify the metric to use to compare two different\n |          models. Must be the name of a metric returned by the evaluation with or without the prefix `\"eval_\"`.\n |  \n |          If not specified, this will default to `\"loss\"` when either `load_best_model_at_end == True`\n |          or `lr_scheduler_type == SchedulerType.REDUCE_ON_PLATEAU` (to use the evaluation loss).\n |  \n |          If you set this value, `greater_is_better` will default to `True` unless the name ends with \"loss\".\n |          Don't forget to set it to `False` if your metric is better when lower.\n |      greater_is_better (`bool`, *optional*):\n |          Use in conjunction with `load_best_model_at_end` and `metric_for_best_model` to specify if better models\n |          should have a greater metric or not. Will default to:\n |  \n |          - `True` if `metric_for_best_model` is set to a value that doesn't end in `\"loss\"`.\n |          - `False` if `metric_for_best_model` is not set, or set to a value that ends in `\"loss\"`.\n |      ignore_data_skip (`bool`, *optional*, defaults to `False`):\n |          When resuming training, whether or not to skip the epochs and batches to get the data loading at the same\n |          stage as in the previous training. If set to `True`, the training will begin faster (as that skipping step\n |          can take a long time) but will not yield the same results as the interrupted training would have.\n |      fsdp (`bool`, `str` or list of [`~trainer_utils.FSDPOption`], *optional*, defaults to `''`):\n |          Use PyTorch Distributed Parallel Training (in distributed training only).\n |  \n |          A list of options along the following:\n |  \n |          - `\"full_shard\"`: Shard parameters, gradients and optimizer states.\n |          - `\"shard_grad_op\"`: Shard optimizer states and gradients.\n |          - `\"hybrid_shard\"`: Apply `FULL_SHARD` within a node, and replicate parameters across nodes.\n |          - `\"hybrid_shard_zero2\"`: Apply `SHARD_GRAD_OP` within a node, and replicate parameters across nodes.\n |          - `\"offload\"`: Offload parameters and gradients to CPUs (only compatible with `\"full_shard\"` and\n |            `\"shard_grad_op\"`).\n |          - `\"auto_wrap\"`: Automatically recursively wrap layers with FSDP using `default_auto_wrap_policy`.\n |      fsdp_config (`str` or `dict`, *optional*):\n |          Config to be used with fsdp (Pytorch Distributed Parallel Training). The value is either a location of\n |          fsdp json config file (e.g., `fsdp_config.json`) or an already loaded json file as `dict`.\n |  \n |          A List of config and its options:\n |              - min_num_params (`int`, *optional*, defaults to `0`):\n |                  FSDP's minimum number of parameters for Default Auto Wrapping. (useful only when `fsdp` field is\n |                  passed).\n |              - transformer_layer_cls_to_wrap (`List[str]`, *optional*):\n |                  List of transformer layer class names (case-sensitive) to wrap, e.g, `BertLayer`, `GPTJBlock`,\n |                  `T5Block` .... (useful only when `fsdp` flag is passed).\n |              - backward_prefetch (`str`, *optional*)\n |                  FSDP's backward prefetch mode. Controls when to prefetch next set of parameters (useful only when\n |                  `fsdp` field is passed).\n |  \n |                  A list of options along the following:\n |  \n |                  - `\"backward_pre\"` : Prefetches the next set of parameters before the current set of parameter's\n |                    gradient\n |                      computation.\n |                  - `\"backward_post\"` : This prefetches the next set of parameters after the current set of\n |                    parameterâ€™s\n |                      gradient computation.\n |              - forward_prefetch (`bool`, *optional*, defaults to `False`)\n |                  FSDP's forward prefetch mode (useful only when `fsdp` field is passed).\n |                   If `\"True\"`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\n |                   forward pass.\n |              - limit_all_gathers (`bool`, *optional*, defaults to `False`)\n |                  FSDP's limit_all_gathers (useful only when `fsdp` field is passed).\n |                   If `\"True\"`, FSDP explicitly synchronizes the CPU thread to prevent too many in-flight\n |                   all-gathers.\n |              - use_orig_params (`bool`, *optional*, defaults to `True`)\n |                  If `\"True\"`, allows non-uniform `requires_grad` during init, which means support for interspersed\n |                  frozen and trainable parameters. Useful in cases such as parameter-efficient fine-tuning. Please\n |                  refer this\n |                  [blog](https://dev-discuss.pytorch.org/t/rethinking-pytorch-fully-sharded-data-parallel-fsdp-from-first-principles/1019\n |              - sync_module_states (`bool`, *optional*, defaults to `True`)\n |                  If `\"True\"`, each individually wrapped FSDP unit will broadcast module parameters from rank 0 to\n |                  ensure they are the same across all ranks after initialization\n |              - cpu_ram_efficient_loading (`bool`, *optional*, defaults to `False`)\n |                  If `\"True\"`, only the first process loads the pretrained model checkpoint while all other processes\n |                  have empty weights.  When this setting as `\"True\"`, `sync_module_states` also must to be `\"True\"`,\n |                  otherwise all the processes except the main process would have random weights leading to unexpected\n |                  behaviour during training.\n |              - activation_checkpointing (`bool`, *optional*, defaults to `False`):\n |                  If `\"True\"`, activation checkpointing is a technique to reduce memory usage by clearing activations of\n |                  certain layers and recomputing them during a backward pass. Effectively, this trades extra\n |                  computation time for reduced memory usage.\n |              - xla (`bool`, *optional*, defaults to `False`):\n |                  Whether to use PyTorch/XLA Fully Sharded Data Parallel Training. This is an experimental feature\n |                  and its API may evolve in the future.\n |              - xla_fsdp_settings (`dict`, *optional*)\n |                  The value is a dictionary which stores the XLA FSDP wrapping parameters.\n |  \n |                  For a complete list of options, please see [here](\n |                  https://github.com/pytorch/xla/blob/master/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py).\n |              - xla_fsdp_grad_ckpt (`bool`, *optional*, defaults to `False`):\n |                  Will use gradient checkpointing over each nested XLA FSDP wrapped layer. This setting can only be\n |                  used when the xla flag is set to true, and an auto wrapping policy is specified through\n |                  fsdp_min_num_params or fsdp_transformer_layer_cls_to_wrap.\n |      deepspeed (`str` or `dict`, *optional*):\n |          Use [Deepspeed](https://github.com/deepspeedai/DeepSpeed). This is an experimental feature and its API may\n |          evolve in the future. The value is either the location of DeepSpeed json config file (e.g.,\n |          `ds_config.json`) or an already loaded json file as a `dict`\"\n |  \n |          <Tip warning={true}>\n |              If enabling any Zero-init, make sure that your model is not initialized until\n |              *after* initializing the `TrainingArguments`, else it will not be applied.\n |          </Tip>\n |  \n |      accelerator_config (`str`, `dict`, or `AcceleratorConfig`, *optional*):\n |          Config to be used with the internal `Accelerator` implementation. The value is either a location of\n |          accelerator json config file (e.g., `accelerator_config.json`), an already loaded json file as `dict`,\n |          or an instance of [`~trainer_pt_utils.AcceleratorConfig`].\n |  \n |          A list of config and its options:\n |              - split_batches (`bool`, *optional*, defaults to `False`):\n |                  Whether or not the accelerator should split the batches yielded by the dataloaders across the devices. If\n |                  `True` the actual batch size used will be the same on any kind of distributed processes, but it must be a\n |                  round multiple of the `num_processes` you are using. If `False`, actual batch size used will be the one set\n |                  in your script multiplied by the number of processes.\n |              - dispatch_batches (`bool`, *optional*):\n |                  If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\n |                  and then the batches are split and broadcast to each process. Will default to `True` for `DataLoader` whose\n |                  underlying dataset is an `IterableDataset`, `False` otherwise.\n |              - even_batches (`bool`, *optional*, defaults to `True`):\n |                  If set to `True`, in cases where the total batch size across all processes does not exactly divide the\n |                  dataset, samples at the start of the dataset will be duplicated so the batch can be divided equally among\n |                  all workers.\n |              - use_seedable_sampler (`bool`, *optional*, defaults to `True`):\n |                  Whether or not use a fully seedable random sampler ([`accelerate.data_loader.SeedableRandomSampler`]). Ensures\n |                  training results are fully reproducible using a different sampling technique. While seed-to-seed results\n |                  may differ, on average the differences are negligible when using multiple different seeds to compare. Should\n |                  also be ran with [`~utils.set_seed`] for the best results.\n |              - use_configured_state (`bool`, *optional*, defaults to `False`):\n |                  Whether or not to use a pre-configured `AcceleratorState` or `PartialState` defined before calling `TrainingArguments`.\n |                  If `True`, an `Accelerator` or `PartialState` must be initialized. Note that by doing so, this could lead to issues\n |                  with hyperparameter tuning.\n |  \n |      label_smoothing_factor (`float`, *optional*, defaults to 0.0):\n |          The label smoothing factor to use. Zero means no label smoothing, otherwise the underlying onehot-encoded\n |          labels are changed from 0s and 1s to `label_smoothing_factor/num_labels` and `1 - label_smoothing_factor +\n |          label_smoothing_factor/num_labels` respectively.\n |      debug (`str` or list of [`~debug_utils.DebugOption`], *optional*, defaults to `\"\"`):\n |          Enable one or more debug features. This is an experimental feature.\n |  \n |          Possible options are:\n |  \n |          - `\"underflow_overflow\"`: detects overflow in model's input/outputs and reports the last frames that led to\n |            the event\n |          - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n |  \n |          The options should be separated by whitespaces.\n |      optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n |          The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n |          \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n |          for a full list of optimizers.\n |      optim_args (`str`, *optional*):\n |          Optional arguments that are supplied to optimizers such as AnyPrecisionAdamW, AdEMAMix, and GaLore.\n |      group_by_length (`bool`, *optional*, defaults to `False`):\n |          Whether or not to group together samples of roughly the same length in the training dataset (to minimize\n |          padding applied and be more efficient). Only useful if applying dynamic padding.\n |      length_column_name (`str`, *optional*, defaults to `\"length\"`):\n |          Column name for precomputed lengths. If the column exists, grouping by length will use these values rather\n |          than computing them on train startup. Ignored unless `group_by_length` is `True` and the dataset is an\n |          instance of `Dataset`.\n |      report_to (`str` or `List[str]`, *optional*, defaults to `\"all\"`):\n |          The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n |          `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`, `\"neptune\"`,\n |          `\"swanlab\"`, `\"tensorboard\"`, and `\"wandb\"`. Use `\"all\"` to report to all integrations installed, `\"none\"`\n |          for no integrations.\n |      ddp_find_unused_parameters (`bool`, *optional*):\n |          When using distributed training, the value of the flag `find_unused_parameters` passed to\n |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n |      ddp_bucket_cap_mb (`int`, *optional*):\n |          When using distributed training, the value of the flag `bucket_cap_mb` passed to `DistributedDataParallel`.\n |      ddp_broadcast_buffers (`bool`, *optional*):\n |          When using distributed training, the value of the flag `broadcast_buffers` passed to\n |          `DistributedDataParallel`. Will default to `False` if gradient checkpointing is used, `True` otherwise.\n |      dataloader_pin_memory (`bool`, *optional*, defaults to `True`):\n |          Whether you want to pin memory in data loaders or not. Will default to `True`.\n |      dataloader_persistent_workers (`bool`, *optional*, defaults to `False`):\n |          If True, the data loader will not shut down the worker processes after a dataset has been consumed once.\n |          This allows to maintain the workers Dataset instances alive. Can potentially speed up training, but will\n |          increase RAM usage. Will default to `False`.\n |      dataloader_prefetch_factor (`int`, *optional*):\n |          Number of batches loaded in advance by each worker.\n |          2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n |      skip_memory_metrics (`bool`, *optional*, defaults to `True`):\n |          Whether to skip adding of memory profiler reports to metrics. This is skipped by default because it slows\n |          down the training and evaluation speed.\n |      push_to_hub (`bool`, *optional*, defaults to `False`):\n |          Whether or not to push the model to the Hub every time the model is saved. If this is activated,\n |          `output_dir` will begin a git directory synced with the repo (determined by `hub_model_id`) and the content\n |          will be pushed each time a save is triggered (depending on your `save_strategy`). Calling\n |          [`~Trainer.save_model`] will also trigger a push.\n |  \n |          <Tip warning={true}>\n |  \n |          If `output_dir` exists, it needs to be a local clone of the repository to which the [`Trainer`] will be\n |          pushed.\n |  \n |          </Tip>\n |  \n |      resume_from_checkpoint (`str`, *optional*):\n |          The path to a folder with a valid checkpoint for your model. This argument is not directly used by\n |          [`Trainer`], it's intended to be used by your training/evaluation scripts instead. See the [example\n |          scripts](https://github.com/huggingface/transformers/tree/main/examples) for more details.\n |      hub_model_id (`str`, *optional*):\n |          The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n |          which case the model will be pushed in your namespace. Otherwise it should be the whole repository name,\n |          for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of with\n |          `\"organization_name/model\"`. Will default to `user_name/output_dir_name` with *output_dir_name* being the\n |          name of `output_dir`.\n |  \n |          Will default to the name of `output_dir`.\n |      hub_strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n |          Defines the scope of what is pushed to the Hub and when. Possible values are:\n |  \n |          - `\"end\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n |            draft of a model card when the [`~Trainer.save_model`] method is called.\n |          - `\"every_save\"`: push the model, its configuration, the processing class e.g. tokenizer (if passed along to the [`Trainer`]) and\n |            a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n |            training, and in case the save are very frequent, a new push is only attempted if the previous one is\n |            finished. A last push is made with the final model at the end of training.\n |          - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n |            last-checkpoint, allowing you to resume training easily with\n |            `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n |          - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the output\n |            folder (so you will get one checkpoint folder per folder in your final repository)\n |  \n |      hub_token (`str`, *optional*):\n |          The token to use to push the model to the Hub. Will default to the token in the cache folder obtained with\n |          `huggingface-cli login`.\n |      hub_private_repo (`bool`, *optional*):\n |          Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n |      hub_always_push (`bool`, *optional*, defaults to `False`):\n |          Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not finished.\n |      gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n |          If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n |      gradient_checkpointing_kwargs (`dict`, *optional*, defaults to `None`):\n |          Key word arguments to be passed to the `gradient_checkpointing_enable` method.\n |      include_inputs_for_metrics (`bool`, *optional*, defaults to `False`):\n |          This argument is deprecated. Use `include_for_metrics` instead, e.g, `include_for_metrics = [\"inputs\"]`.\n |      include_for_metrics (`List[str]`, *optional*, defaults to `[]`):\n |          Include additional data in the `compute_metrics` function if needed for metrics computation.\n |          Possible options to add to `include_for_metrics` list:\n |          - `\"inputs\"`: Input data passed to the model, intended for calculating input dependent metrics.\n |          - `\"loss\"`: Loss values computed during evaluation, intended for calculating loss dependent metrics.\n |      eval_do_concat_batches (`bool`, *optional*, defaults to `True`):\n |          Whether to recursively concat inputs/losses/labels/predictions across batches. If `False`,\n |          will instead store them as lists, with each batch kept separate.\n |      auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n |          Whether to find a batch size that will fit into memory automatically through exponential decay, avoiding\n |          CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n |      full_determinism (`bool`, *optional*, defaults to `False`)\n |          If `True`, [`enable_full_determinism`] is called instead of [`set_seed`] to ensure reproducible results in\n |          distributed training. Important: this will negatively impact the performance, so only use it for debugging.\n |      torchdynamo (`str`, *optional*):\n |          If set, the backend compiler for TorchDynamo. Possible choices are `\"eager\"`, `\"aot_eager\"`, `\"inductor\"`,\n |          `\"nvfuser\"`, `\"aot_nvfuser\"`, `\"aot_cudagraphs\"`, `\"ofi\"`, `\"fx2trt\"`, `\"onnxrt\"` and `\"ipex\"`.\n |      ray_scope (`str`, *optional*, defaults to `\"last\"`):\n |          The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray will\n |          then use the last checkpoint of all trials, compare those, and select the best one. However, other options\n |          are also available. See the [Ray documentation](\n |          https://docs.ray.io/en/latest/tune/api_docs/analysis.html#ray.tune.ExperimentAnalysis.get_best_trial) for\n |          more options.\n |      ddp_timeout (`int`, *optional*, defaults to 1800):\n |          The timeout for `torch.distributed.init_process_group` calls, used to avoid GPU socket timeouts when\n |          performing slow operations in distributed runnings. Please refer the [PyTorch documentation]\n |          (https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group) for more\n |          information.\n |      use_mps_device (`bool`, *optional*, defaults to `False`):\n |          This argument is deprecated.`mps` device will be used if it is available similar to `cuda` device.\n |      torch_compile (`bool`, *optional*, defaults to `False`):\n |          Whether or not to compile the model using PyTorch 2.0\n |          [`torch.compile`](https://pytorch.org/get-started/pytorch-2.0/).\n |  \n |          This will use the best defaults for the [`torch.compile`\n |          API](https://pytorch.org/docs/stable/generated/torch.compile.html?highlight=torch+compile#torch.compile).\n |          You can customize the defaults with the argument `torch_compile_backend` and `torch_compile_mode` but we\n |          don't guarantee any of them will work as the support is progressively rolled in in PyTorch.\n |  \n |          This flag and the whole compile API is experimental and subject to change in future releases.\n |      torch_compile_backend (`str`, *optional*):\n |          The backend to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n |  \n |          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n |  \n |          This flag is experimental and subject to change in future releases.\n |      torch_compile_mode (`str`, *optional*):\n |          The mode to use in `torch.compile`. If set to any value, `torch_compile` will be set to `True`.\n |  \n |          Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n |  \n |          This flag is experimental and subject to change in future releases.\n |      include_tokens_per_second (`bool`, *optional*):\n |          Whether or not to compute the number of tokens per second per device for training speed metrics.\n |  \n |          This will iterate over the entire training dataloader once beforehand,\n |  \n |          and will slow down the entire process.\n |  \n |      include_num_input_tokens_seen (`bool`, *optional*):\n |          Whether or not to track the number of input tokens seen throughout training.\n |  \n |          May be slower in distributed training as gather operations must be called.\n |  \n |      neftune_noise_alpha (`Optional[float]`):\n |          If not `None`, this will activate NEFTune noise embeddings. This can drastically improve model performance\n |          for instruction fine-tuning. Check out the [original paper](https://arxiv.org/abs/2310.05914) and the\n |          [original code](https://github.com/neelsjain/NEFTune). Support transformers `PreTrainedModel` and also\n |          `PeftModel` from peft. The original paper used values in the range [5.0, 15.0].\n |      optim_target_modules (`Union[str, List[str]]`, *optional*):\n |          The target modules to optimize, i.e. the module names that you would like to train.\n |          Currently used for the GaLore algorithm (https://arxiv.org/abs/2403.03507) and APOLLO algorithm (https://arxiv.org/abs/2412.05270).\n |          See GaLore implementation (https://github.com/jiaweizzhao/GaLore) and APOLLO implementation (https://github.com/zhuhanqing/APOLLO) for more details.\n |          You need to make sure to pass a valid GaLore or APOLLO optimizer, e.g., one of: \"apollo_adamw\", \"galore_adamw\", \"galore_adamw_8bit\", \"galore_adafactor\" and make sure that the target modules are `nn.Linear` modules only.\n |  \n |      batch_eval_metrics (`Optional[bool]`, defaults to `False`):\n |          If set to `True`, evaluation will call compute_metrics at the end of each batch to accumulate statistics\n |          rather than saving all eval logits in memory. When set to `True`, you must pass a compute_metrics function\n |          that takes a boolean argument `compute_result`, which when passed `True`, will trigger the final global\n |          summary statistics from the batch-level summary statistics you've accumulated over the evaluation set.\n |  \n |      eval_on_start (`bool`, *optional*, defaults to `False`):\n |          Whether to perform a evaluation step (sanity check) before the training to ensure the validation steps works correctly.\n |  \n |      eval_use_gather_object (`bool`, *optional*, defaults to `False`):\n |          Whether to run recursively gather object in a nested list/tuple/dictionary of objects from all devices. This should only be enabled if users are not just returning tensors, and this is actively discouraged by PyTorch.\n |  \n |      use_liger_kernel (`bool`, *optional*, defaults to `False`):\n |          Whether enable [Liger](https://github.com/linkedin/Liger-Kernel) Kernel for LLM model training.\n |          It can effectively increase multi-GPU training throughput by ~20% and reduces memory usage by ~60%, works out of the box with\n |          flash attention, PyTorch FSDP, and Microsoft DeepSpeed. Currently, it supports llama, mistral, mixtral and gemma models.\n |  \n |      average_tokens_across_devices (`bool`, *optional*, defaults to `False`):\n |          Whether or not to average tokens across devices. If enabled, will use all_reduce to synchronize\n |          num_tokens_in_batch for precise loss calculation. Reference:\n |          https://github.com/huggingface/transformers/issues/34242\n |  \n |  Methods defined here:\n |  \n |  __eq__(self, other)\n |      Return self==value.\n |  \n |  __init__(self, output_dir: Optional[str] = None, overwrite_output_dir: bool = False, do_train: bool = False, do_eval: bool = False, do_predict: bool = False, eval_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'no', prediction_loss_only: bool = False, per_device_train_batch_size: int = 8, per_device_eval_batch_size: int = 8, per_gpu_train_batch_size: Optional[int] = None, per_gpu_eval_batch_size: Optional[int] = None, gradient_accumulation_steps: int = 1, eval_accumulation_steps: Optional[int] = None, eval_delay: Optional[float] = 0, torch_empty_cache_steps: Optional[int] = None, learning_rate: float = 5e-05, weight_decay: float = 0.0, adam_beta1: float = 0.9, adam_beta2: float = 0.999, adam_epsilon: float = 1e-08, max_grad_norm: float = 1.0, num_train_epochs: float = 3.0, max_steps: int = -1, lr_scheduler_type: Union[transformers.trainer_utils.SchedulerType, str] = 'linear', lr_scheduler_kwargs: Union[dict, str, NoneType] = <factory>, warmup_ratio: float = 0.0, warmup_steps: int = 0, log_level: Optional[str] = 'passive', log_level_replica: Optional[str] = 'warning', log_on_each_node: bool = True, logging_dir: Optional[str] = None, logging_strategy: Union[transformers.trainer_utils.IntervalStrategy, str] = 'steps', logging_first_step: bool = False, logging_steps: float = 500, logging_nan_inf_filter: bool = True, save_strategy: Union[transformers.trainer_utils.SaveStrategy, str] = 'steps', save_steps: float = 500, save_total_limit: Optional[int] = None, save_safetensors: Optional[bool] = True, save_on_each_node: bool = False, save_only_model: bool = False, restore_callback_states_from_checkpoint: bool = False, no_cuda: bool = False, use_cpu: bool = False, use_mps_device: bool = False, seed: int = 42, data_seed: Optional[int] = None, jit_mode_eval: bool = False, use_ipex: bool = False, bf16: bool = False, fp16: bool = False, fp16_opt_level: str = 'O1', half_precision_backend: str = 'auto', bf16_full_eval: bool = False, fp16_full_eval: bool = False, tf32: Optional[bool] = None, local_rank: int = -1, ddp_backend: Optional[str] = None, tpu_num_cores: Optional[int] = None, tpu_metrics_debug: bool = False, debug: Union[str, list[transformers.debug_utils.DebugOption]] = '', dataloader_drop_last: bool = False, eval_steps: Optional[float] = None, dataloader_num_workers: int = 0, dataloader_prefetch_factor: Optional[int] = None, past_index: int = -1, run_name: Optional[str] = None, disable_tqdm: Optional[bool] = None, remove_unused_columns: Optional[bool] = True, label_names: Optional[list[str]] = None, load_best_model_at_end: Optional[bool] = False, metric_for_best_model: Optional[str] = None, greater_is_better: Optional[bool] = None, ignore_data_skip: bool = False, fsdp: Union[list[transformers.trainer_utils.FSDPOption], str, NoneType] = '', fsdp_min_num_params: int = 0, fsdp_config: Union[dict, str, NoneType] = None, fsdp_transformer_layer_cls_to_wrap: Optional[str] = None, accelerator_config: Union[dict, str, NoneType] = None, deepspeed: Union[dict, str, NoneType] = None, label_smoothing_factor: float = 0.0, optim: Union[transformers.training_args.OptimizerNames, str] = 'adamw_torch', optim_args: Optional[str] = None, adafactor: bool = False, group_by_length: bool = False, length_column_name: Optional[str] = 'length', report_to: Union[NoneType, str, list[str]] = None, ddp_find_unused_parameters: Optional[bool] = None, ddp_bucket_cap_mb: Optional[int] = None, ddp_broadcast_buffers: Optional[bool] = None, dataloader_pin_memory: bool = True, dataloader_persistent_workers: bool = False, skip_memory_metrics: bool = True, use_legacy_prediction_loop: bool = False, push_to_hub: bool = False, resume_from_checkpoint: Optional[str] = None, hub_model_id: Optional[str] = None, hub_strategy: Union[transformers.trainer_utils.HubStrategy, str] = 'every_save', hub_token: Optional[str] = None, hub_private_repo: Optional[bool] = None, hub_always_push: bool = False, gradient_checkpointing: bool = False, gradient_checkpointing_kwargs: Union[dict, str, NoneType] = None, include_inputs_for_metrics: bool = False, include_for_metrics: list[str] = <factory>, eval_do_concat_batches: bool = True, fp16_backend: str = 'auto', push_to_hub_model_id: Optional[str] = None, push_to_hub_organization: Optional[str] = None, push_to_hub_token: Optional[str] = None, mp_parameters: str = '', auto_find_batch_size: bool = False, full_determinism: bool = False, torchdynamo: Optional[str] = None, ray_scope: Optional[str] = 'last', ddp_timeout: Optional[int] = 1800, torch_compile: bool = False, torch_compile_backend: Optional[str] = None, torch_compile_mode: Optional[str] = None, include_tokens_per_second: Optional[bool] = False, include_num_input_tokens_seen: Optional[bool] = False, neftune_noise_alpha: Optional[float] = None, optim_target_modules: Union[NoneType, str, list[str]] = None, batch_eval_metrics: bool = False, eval_on_start: bool = False, use_liger_kernel: Optional[bool] = False, eval_use_gather_object: Optional[bool] = False, average_tokens_across_devices: Optional[bool] = False) -> None\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  __post_init__(self)\n |  \n |  __repr__ = __str__(self)\n |  \n |  __str__(self)\n |      Return str(self).\n |  \n |  get_process_log_level(self)\n |      Returns the log level to be used depending on whether this process is the main process of node 0, main process\n |      of node non-0, or a non-main process.\n |      \n |      For the main process the log level defaults to the logging level set (`logging.WARNING` if you didn't do\n |      anything) unless overridden by `log_level` argument.\n |      \n |      For the replica processes the log level defaults to `logging.WARNING` unless overridden by `log_level_replica`\n |      argument.\n |      \n |      The choice between the main and replica process settings is made according to the return value of `should_log`.\n |  \n |  get_warmup_steps(self, num_training_steps: int)\n |      Get number of steps used for a linear warmup.\n |  \n |  main_process_first(self, local=True, desc='work')\n |      A context manager for torch distributed environment where on needs to do something on the main process, while\n |      blocking replicas, and when it's finished releasing the replicas.\n |      \n |      One such use is for `datasets`'s `map` feature which to be efficient should be run once on the main process,\n |      which upon completion saves a cached version of results and which then automatically gets loaded by the\n |      replicas.\n |      \n |      Args:\n |          local (`bool`, *optional*, defaults to `True`):\n |              if `True` first means process of rank 0 of each node if `False` first means process of rank 0 of node\n |              rank 0 In multi-node environment with a shared filesystem you most likely will want to use\n |              `local=False` so that only the main process of the first node will do the processing. If however, the\n |              filesystem is not shared, then the main process of each node will need to do the processing, which is\n |              the default behavior.\n |          desc (`str`, *optional*, defaults to `\"work\"`):\n |              a work description to be used in debug logs\n |  \n |  set_dataloader(self, train_batch_size: int = 8, eval_batch_size: int = 8, drop_last: bool = False, num_workers: int = 0, pin_memory: bool = True, persistent_workers: bool = False, prefetch_factor: Optional[int] = None, auto_find_batch_size: bool = False, ignore_data_skip: bool = False, sampler_seed: Optional[int] = None)\n |      A method that regroups all arguments linked to the dataloaders creation.\n |      \n |      Args:\n |          drop_last (`bool`, *optional*, defaults to `False`):\n |              Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch\n |              size) or not.\n |          num_workers (`int`, *optional*, defaults to 0):\n |              Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in\n |              the main process.\n |          pin_memory (`bool`, *optional*, defaults to `True`):\n |              Whether you want to pin memory in data loaders or not. Will default to `True`.\n |          persistent_workers (`bool`, *optional*, defaults to `False`):\n |              If True, the data loader will not shut down the worker processes after a dataset has been consumed\n |              once. This allows to maintain the workers Dataset instances alive. Can potentially speed up training,\n |              but will increase RAM usage. Will default to `False`.\n |          prefetch_factor (`int`, *optional*):\n |              Number of batches loaded in advance by each worker.\n |              2 means there will be a total of 2 * num_workers batches prefetched across all workers.\n |          auto_find_batch_size (`bool`, *optional*, defaults to `False`)\n |              Whether to find a batch size that will fit into memory automatically through exponential decay,\n |              avoiding CUDA Out-of-Memory errors. Requires accelerate to be installed (`pip install accelerate`)\n |          ignore_data_skip (`bool`, *optional*, defaults to `False`):\n |              When resuming training, whether or not to skip the epochs and batches to get the data loading at the\n |              same stage as in the previous training. If set to `True`, the training will begin faster (as that\n |              skipping step can take a long time) but will not yield the same results as the interrupted training\n |              would have.\n |          sampler_seed (`int`, *optional*):\n |              Random seed to be used with data samplers. If not set, random generators for data sampling will use the\n |              same seed as `self.seed`. This can be used to ensure reproducibility of data sampling, independent of\n |              the model seed.\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_dataloader(train_batch_size=16, eval_batch_size=64)\n |      >>> args.per_device_train_batch_size\n |      16\n |      ```\n |  \n |  set_evaluate(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'no', steps: int = 500, batch_size: int = 8, accumulation_steps: Optional[int] = None, delay: Optional[float] = None, loss_only: bool = False, jit_mode: bool = False)\n |      A method that regroups all arguments linked to evaluation.\n |      \n |      Args:\n |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"no\"`):\n |              The evaluation strategy to adopt during training. Possible values are:\n |      \n |                  - `\"no\"`: No evaluation is done during training.\n |                  - `\"steps\"`: Evaluation is done (and logged) every `steps`.\n |                  - `\"epoch\"`: Evaluation is done at the end of each epoch.\n |      \n |              Setting a `strategy` different from `\"no\"` will set `self.do_eval` to `True`.\n |          steps (`int`, *optional*, defaults to 500):\n |              Number of update steps between two evaluations if `strategy=\"steps\"`.\n |          batch_size (`int` *optional*, defaults to 8):\n |              The batch size per device (GPU/TPU core/CPU...) used for evaluation.\n |          accumulation_steps (`int`, *optional*):\n |              Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU.\n |              If left unset, the whole predictions are accumulated on GPU/TPU before being moved to the CPU (faster\n |              but requires more memory).\n |          delay (`float`, *optional*):\n |              Number of epochs or steps to wait for before the first evaluation can be performed, depending on the\n |              eval_strategy.\n |          loss_only (`bool`, *optional*, defaults to `False`):\n |              Ignores all outputs except the loss.\n |          jit_mode (`bool`, *optional*):\n |              Whether or not to use PyTorch jit trace for inference.\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_evaluate(strategy=\"steps\", steps=100)\n |      >>> args.eval_steps\n |      100\n |      ```\n |  \n |  set_logging(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps', steps: int = 500, report_to: Union[str, list[str]] = 'none', level: str = 'passive', first_step: bool = False, nan_inf_filter: bool = False, on_each_node: bool = False, replica_level: str = 'passive')\n |      A method that regroups all arguments linked to logging.\n |      \n |      Args:\n |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n |              The logging strategy to adopt during training. Possible values are:\n |      \n |                  - `\"no\"`: No logging is done during training.\n |                  - `\"epoch\"`: Logging is done at the end of each epoch.\n |                  - `\"steps\"`: Logging is done every `logging_steps`.\n |      \n |          steps (`int`, *optional*, defaults to 500):\n |              Number of update steps between two logs if `strategy=\"steps\"`.\n |          level (`str`, *optional*, defaults to `\"passive\"`):\n |              Logger log level to use on the main process. Possible choices are the log levels as strings: `\"debug\"`,\n |              `\"info\"`, `\"warning\"`, `\"error\"` and `\"critical\"`, plus a `\"passive\"` level which doesn't set anything\n |              and lets the application set the level.\n |          report_to (`str` or `List[str]`, *optional*, defaults to `\"all\"`):\n |              The list of integrations to report the results and logs to. Supported platforms are `\"azure_ml\"`,\n |              `\"clearml\"`, `\"codecarbon\"`, `\"comet_ml\"`, `\"dagshub\"`, `\"dvclive\"`, `\"flyte\"`, `\"mlflow\"`,\n |              `\"neptune\"`, `\"swanlab\"`, `\"tensorboard\"`, and `\"wandb\"`. Use `\"all\"` to report to all integrations\n |              installed, `\"none\"` for no integrations.\n |          first_step (`bool`, *optional*, defaults to `False`):\n |              Whether to log and evaluate the first `global_step` or not.\n |          nan_inf_filter (`bool`, *optional*, defaults to `True`):\n |              Whether to filter `nan` and `inf` losses for logging. If set to `True` the loss of every step that is\n |              `nan` or `inf` is filtered and the average loss of the current logging window is taken instead.\n |      \n |              <Tip>\n |      \n |              `nan_inf_filter` only influences the logging of loss values, it does not change the behavior the\n |              gradient is computed or applied to the model.\n |      \n |              </Tip>\n |      \n |          on_each_node (`bool`, *optional*, defaults to `True`):\n |              In multinode distributed training, whether to log using `log_level` once per node, or only on the main\n |              node.\n |          replica_level (`str`, *optional*, defaults to `\"passive\"`):\n |              Logger log level to use on replicas. Same choices as `log_level`\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_logging(strategy=\"steps\", steps=100)\n |      >>> args.logging_steps\n |      100\n |      ```\n |  \n |  set_lr_scheduler(self, name: Union[str, transformers.trainer_utils.SchedulerType] = 'linear', num_epochs: float = 3.0, max_steps: int = -1, warmup_ratio: float = 0, warmup_steps: int = 0)\n |      A method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.\n |      \n |      Args:\n |          name (`str` or [`SchedulerType`], *optional*, defaults to `\"linear\"`):\n |              The scheduler type to use. See the documentation of [`SchedulerType`] for all possible values.\n |          num_epochs(`float`, *optional*, defaults to 3.0):\n |              Total number of training epochs to perform (if not an integer, will perform the decimal part percents\n |              of the last epoch before stopping training).\n |          max_steps (`int`, *optional*, defaults to -1):\n |              If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n |              For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n |              `max_steps` is reached.\n |          warmup_ratio (`float`, *optional*, defaults to 0.0):\n |              Ratio of total training steps used for a linear warmup from 0 to `learning_rate`.\n |          warmup_steps (`int`, *optional*, defaults to 0):\n |              Number of steps used for a linear warmup from 0 to `learning_rate`. Overrides any effect of\n |              `warmup_ratio`.\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_lr_scheduler(name=\"cosine\", warmup_ratio=0.05)\n |      >>> args.warmup_ratio\n |      0.05\n |      ```\n |  \n |  set_optimizer(self, name: Union[str, transformers.training_args.OptimizerNames] = 'adamw_torch', learning_rate: float = 5e-05, weight_decay: float = 0, beta1: float = 0.9, beta2: float = 0.999, epsilon: float = 1e-08, args: Optional[str] = None)\n |      A method that regroups all arguments linked to the optimizer and its hyperparameters.\n |      \n |      Args:\n |          name (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n |              The optimizer to use: `\"adamw_torch\"`, `\"adamw_torch_fused\"`, `\"adamw_apex_fused\"`,\n |              `\"adamw_anyprecision\"` or `\"adafactor\"`.\n |          learning_rate (`float`, *optional*, defaults to 5e-5):\n |              The initial learning rate.\n |          weight_decay (`float`, *optional*, defaults to 0):\n |              The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights.\n |          beta1 (`float`, *optional*, defaults to 0.9):\n |              The beta1 hyperparameter for the adam optimizer or its variants.\n |          beta2 (`float`, *optional*, defaults to 0.999):\n |              The beta2 hyperparameter for the adam optimizer or its variants.\n |          epsilon (`float`, *optional*, defaults to 1e-8):\n |              The epsilon hyperparameter for the adam optimizer or its variants.\n |          args (`str`, *optional*):\n |              Optional arguments that are supplied to AnyPrecisionAdamW (only useful when\n |              `optim=\"adamw_anyprecision\"`).\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_optimizer(name=\"adamw_torch\", beta1=0.8)\n |      >>> args.optim\n |      'adamw_torch'\n |      ```\n |  \n |  set_push_to_hub(self, model_id: str, strategy: Union[str, transformers.trainer_utils.HubStrategy] = 'every_save', token: Optional[str] = None, private_repo: Optional[bool] = None, always_push: bool = False)\n |      A method that regroups all arguments linked to synchronizing checkpoints with the Hub.\n |      \n |      <Tip>\n |      \n |      Calling this method will set `self.push_to_hub` to `True`, which means the `output_dir` will begin a git\n |      directory synced with the repo (determined by `model_id`) and the content will be pushed each time a save is\n |      triggered (depending on your `self.save_strategy`). Calling [`~Trainer.save_model`] will also trigger a push.\n |      \n |      </Tip>\n |      \n |      Args:\n |          model_id (`str`):\n |              The name of the repository to keep in sync with the local *output_dir*. It can be a simple model ID in\n |              which case the model will be pushed in your namespace. Otherwise it should be the whole repository\n |              name, for instance `\"user_name/model\"`, which allows you to push to an organization you are a member of\n |              with `\"organization_name/model\"`.\n |          strategy (`str` or [`~trainer_utils.HubStrategy`], *optional*, defaults to `\"every_save\"`):\n |              Defines the scope of what is pushed to the Hub and when. Possible values are:\n |      \n |              - `\"end\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`]) and a\n |              draft of a model card when the [`~Trainer.save_model`] method is called.\n |              - `\"every_save\"`: push the model, its configuration, the processing_class e.g. tokenizer (if passed along to the [`Trainer`])\n |                and\n |              a draft of a model card each time there is a model save. The pushes are asynchronous to not block\n |              training, and in case the save are very frequent, a new push is only attempted if the previous one is\n |              finished. A last push is made with the final model at the end of training.\n |              - `\"checkpoint\"`: like `\"every_save\"` but the latest checkpoint is also pushed in a subfolder named\n |              last-checkpoint, allowing you to resume training easily with\n |              `trainer.train(resume_from_checkpoint=\"last-checkpoint\")`.\n |              - `\"all_checkpoints\"`: like `\"checkpoint\"` but all checkpoints are pushed like they appear in the\n |                output\n |              folder (so you will get one checkpoint folder per folder in your final repository)\n |      \n |          token (`str`, *optional*):\n |              The token to use to push the model to the Hub. Will default to the token in the cache folder obtained\n |              with `huggingface-cli login`.\n |          private_repo (`bool`, *optional*, defaults to `False`):\n |              Whether to make the repo private. If `None` (default), the repo will be public unless the organization's default is private. This value is ignored if the repo already exists.\n |          always_push (`bool`, *optional*, defaults to `False`):\n |              Unless this is `True`, the `Trainer` will skip pushing a checkpoint when the previous push is not\n |              finished.\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_push_to_hub(\"me/awesome-model\")\n |      >>> args.hub_model_id\n |      'me/awesome-model'\n |      ```\n |  \n |  set_save(self, strategy: Union[str, transformers.trainer_utils.IntervalStrategy] = 'steps', steps: int = 500, total_limit: Optional[int] = None, on_each_node: bool = False)\n |      A method that regroups all arguments linked to checkpoint saving.\n |      \n |      Args:\n |          strategy (`str` or [`~trainer_utils.IntervalStrategy`], *optional*, defaults to `\"steps\"`):\n |              The checkpoint save strategy to adopt during training. Possible values are:\n |      \n |                  - `\"no\"`: No save is done during training.\n |                  - `\"epoch\"`: Save is done at the end of each epoch.\n |                  - `\"steps\"`: Save is done every `save_steps`.\n |      \n |          steps (`int`, *optional*, defaults to 500):\n |              Number of updates steps before two checkpoint saves if `strategy=\"steps\"`.\n |          total_limit (`int`, *optional*):\n |              If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in\n |              `output_dir`.\n |          on_each_node (`bool`, *optional*, defaults to `False`):\n |              When doing multi-node distributed training, whether to save models and checkpoints on each node, or\n |              only on the main one.\n |      \n |              This should not be activated when the different nodes use the same storage as the files will be saved\n |              with the same names for each node.\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_save(strategy=\"steps\", steps=100)\n |      >>> args.save_steps\n |      100\n |      ```\n |  \n |  set_testing(self, batch_size: int = 8, loss_only: bool = False, jit_mode: bool = False)\n |      A method that regroups all basic arguments linked to testing on a held-out dataset.\n |      \n |      <Tip>\n |      \n |      Calling this method will automatically set `self.do_predict` to `True`.\n |      \n |      </Tip>\n |      \n |      Args:\n |          batch_size (`int` *optional*, defaults to 8):\n |              The batch size per device (GPU/TPU core/CPU...) used for testing.\n |          loss_only (`bool`, *optional*, defaults to `False`):\n |              Ignores all outputs except the loss.\n |          jit_mode (`bool`, *optional*):\n |              Whether or not to use PyTorch jit trace for inference.\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_testing(batch_size=32)\n |      >>> args.per_device_eval_batch_size\n |      32\n |      ```\n |  \n |  set_training(self, learning_rate: float = 5e-05, batch_size: int = 8, weight_decay: float = 0, num_epochs: float = 3, max_steps: int = -1, gradient_accumulation_steps: int = 1, seed: int = 42, gradient_checkpointing: bool = False)\n |      A method that regroups all basic arguments linked to the training.\n |      \n |      <Tip>\n |      \n |      Calling this method will automatically set `self.do_train` to `True`.\n |      \n |      </Tip>\n |      \n |      Args:\n |          learning_rate (`float`, *optional*, defaults to 5e-5):\n |              The initial learning rate for the optimizer.\n |          batch_size (`int` *optional*, defaults to 8):\n |              The batch size per device (GPU/TPU core/CPU...) used for training.\n |          weight_decay (`float`, *optional*, defaults to 0):\n |              The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in the\n |              optimizer.\n |          num_train_epochs(`float`, *optional*, defaults to 3.0):\n |              Total number of training epochs to perform (if not an integer, will perform the decimal part percents\n |              of the last epoch before stopping training).\n |          max_steps (`int`, *optional*, defaults to -1):\n |              If set to a positive number, the total number of training steps to perform. Overrides `num_train_epochs`.\n |              For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until\n |              `max_steps` is reached.\n |          gradient_accumulation_steps (`int`, *optional*, defaults to 1):\n |              Number of updates steps to accumulate the gradients for, before performing a backward/update pass.\n |      \n |              <Tip warning={true}>\n |      \n |              When using gradient accumulation, one step is counted as one step with backward pass. Therefore,\n |              logging, evaluation, save will be conducted every `gradient_accumulation_steps * xxx_step` training\n |              examples.\n |      \n |              </Tip>\n |      \n |          seed (`int`, *optional*, defaults to 42):\n |              Random seed that will be set at the beginning of training. To ensure reproducibility across runs, use\n |              the [`~Trainer.model_init`] function to instantiate the model if it has some randomly initialized\n |              parameters.\n |          gradient_checkpointing (`bool`, *optional*, defaults to `False`):\n |              If True, use gradient checkpointing to save memory at the expense of slower backward pass.\n |      \n |      Example:\n |      \n |      ```py\n |      >>> from transformers import TrainingArguments\n |      \n |      >>> args = TrainingArguments(\"working_dir\")\n |      >>> args = args.set_training(learning_rate=1e-4, batch_size=32)\n |      >>> args.learning_rate\n |      1e-4\n |      ```\n |  \n |  to_dict(self)\n |      Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n |      the token values by removing their value.\n |  \n |  to_json_string(self)\n |      Serializes this instance to a JSON string.\n |  \n |  to_sanitized_dict(self) -> dict[str, typing.Any]\n |      Sanitized serialization to use with TensorBoardâ€™s hparams\n |  \n |  ----------------------------------------------------------------------\n |  Readonly properties defined here:\n |  \n |  ddp_timeout_delta\n |      The actual timeout for torch.distributed.init_process_group since it expects a timedelta variable.\n |  \n |  device\n |      The device used by this process.\n |  \n |  eval_batch_size\n |      The actual batch size for evaluation (may differ from `per_gpu_eval_batch_size` in distributed training).\n |  \n |  local_process_index\n |      The index of the local process used.\n |  \n |  n_gpu\n |      The number of GPUs used by this process.\n |      \n |      Note:\n |          This will only be greater than one when you have multiple GPUs available but are not using distributed\n |          training. For distributed training, it will always be 1.\n |  \n |  parallel_mode\n |      The current mode used for parallelism if multiple GPUs/TPU cores are available. One of:\n |      \n |      - `ParallelMode.NOT_PARALLEL`: no parallelism (CPU or one GPU).\n |      - `ParallelMode.NOT_DISTRIBUTED`: several GPUs in one single process (uses `torch.nn.DataParallel`).\n |      - `ParallelMode.DISTRIBUTED`: several GPUs, each having its own process (uses\n |        `torch.nn.DistributedDataParallel`).\n |      - `ParallelMode.TPU`: several TPU cores.\n |  \n |  place_model_on_device\n |      Can be subclassed and overridden for some specific integrations.\n |  \n |  process_index\n |      The index of the current process used.\n |  \n |  should_log\n |      Whether or not the current process should produce log.\n |  \n |  should_save\n |      Whether or not the current process should write to disk, e.g., to save models and checkpoints.\n |  \n |  train_batch_size\n |      The actual batch size for training (may differ from `per_gpu_train_batch_size` in distributed training).\n |  \n |  world_size\n |      The number of processes used in parallel.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors defined here:\n |  \n |  __dict__\n |      dictionary for instance variables\n |  \n |  __weakref__\n |      list of weak references to the object\n |  \n |  ----------------------------------------------------------------------\n |  Data and other attributes defined here:\n |  \n |  __annotations__ = {'_n_gpu': <class 'int'>, 'accelerator_config': typi...\n |  \n |  __dataclass_fields__ = {'_n_gpu': Field(name='_n_gpu',type=<class 'int...\n |  \n |  __dataclass_params__ = _DataclassParams(init=True,repr=True,eq=True,or...\n |  \n |  __hash__ = None\n |  \n |  __match_args__ = ('output_dir', 'overwrite_output_dir', 'do_train', 'd...\n |  \n |  accelerator_config = None\n |  \n |  adafactor = False\n |  \n |  adam_beta1 = 0.9\n |  \n |  adam_beta2 = 0.999\n |  \n |  adam_epsilon = 1e-08\n |  \n |  auto_find_batch_size = False\n |  \n |  average_tokens_across_devices = False\n |  \n |  batch_eval_metrics = False\n |  \n |  bf16 = False\n |  \n |  bf16_full_eval = False\n |  \n |  data_seed = None\n |  \n |  dataloader_drop_last = False\n |  \n |  dataloader_num_workers = 0\n |  \n |  dataloader_persistent_workers = False\n |  \n |  dataloader_pin_memory = True\n |  \n |  dataloader_prefetch_factor = None\n |  \n |  ddp_backend = None\n |  \n |  ddp_broadcast_buffers = None\n |  \n |  ddp_bucket_cap_mb = None\n |  \n |  ddp_find_unused_parameters = None\n |  \n |  ddp_timeout = 1800\n |  \n |  debug = ''\n |  \n |  deepspeed = None\n |  \n |  default_optim = 'adamw_torch'\n |  \n |  disable_tqdm = None\n |  \n |  do_eval = False\n |  \n |  do_predict = False\n |  \n |  do_train = False\n |  \n |  eval_accumulation_steps = None\n |  \n |  eval_delay = 0\n |  \n |  eval_do_concat_batches = True\n |  \n |  eval_on_start = False\n |  \n |  eval_steps = None\n |  \n |  eval_strategy = 'no'\n |  \n |  eval_use_gather_object = False\n |  \n |  fp16 = False\n |  \n |  fp16_backend = 'auto'\n |  \n |  fp16_full_eval = False\n |  \n |  fp16_opt_level = 'O1'\n |  \n |  framework = 'pt'\n |  \n |  fsdp = ''\n |  \n |  fsdp_config = None\n |  \n |  fsdp_min_num_params = 0\n |  \n |  fsdp_transformer_layer_cls_to_wrap = None\n |  \n |  full_determinism = False\n |  \n |  gradient_accumulation_steps = 1\n |  \n |  gradient_checkpointing = False\n |  \n |  gradient_checkpointing_kwargs = None\n |  \n |  greater_is_better = None\n |  \n |  group_by_length = False\n |  \n |  half_precision_backend = 'auto'\n |  \n |  hub_always_push = False\n |  \n |  hub_model_id = None\n |  \n |  hub_private_repo = None\n |  \n |  hub_strategy = 'every_save'\n |  \n |  hub_token = None\n |  \n |  ignore_data_skip = False\n |  \n |  include_inputs_for_metrics = False\n |  \n |  include_num_input_tokens_seen = False\n |  \n |  include_tokens_per_second = False\n |  \n |  jit_mode_eval = False\n |  \n |  label_names = None\n |  \n |  label_smoothing_factor = 0.0\n |  \n |  learning_rate = 5e-05\n |  \n |  length_column_name = 'length'\n |  \n |  load_best_model_at_end = False\n |  \n |  local_rank = -1\n |  \n |  log_level = 'passive'\n |  \n |  log_level_replica = 'warning'\n |  \n |  log_on_each_node = True\n |  \n |  logging_dir = None\n |  \n |  logging_first_step = False\n |  \n |  logging_nan_inf_filter = True\n |  \n |  logging_steps = 500\n |  \n |  logging_strategy = 'steps'\n |  \n |  lr_scheduler_type = 'linear'\n |  \n |  max_grad_norm = 1.0\n |  \n |  max_steps = -1\n |  \n |  metric_for_best_model = None\n |  \n |  mp_parameters = ''\n |  \n |  neftune_noise_alpha = None\n |  \n |  no_cuda = False\n |  \n |  num_train_epochs = 3.0\n |  \n |  optim = 'adamw_torch'\n |  \n |  optim_args = None\n |  \n |  optim_target_modules = None\n |  \n |  output_dir = None\n |  \n |  overwrite_output_dir = False\n |  \n |  past_index = -1\n |  \n |  per_device_eval_batch_size = 8\n |  \n |  per_device_train_batch_size = 8\n |  \n |  per_gpu_eval_batch_size = None\n |  \n |  per_gpu_train_batch_size = None\n |  \n |  prediction_loss_only = False\n |  \n |  push_to_hub = False\n |  \n |  push_to_hub_model_id = None\n |  \n |  push_to_hub_organization = None\n |  \n |  push_to_hub_token = None\n |  \n |  ray_scope = 'last'\n |  \n |  remove_unused_columns = True\n |  \n |  report_to = None\n |  \n |  restore_callback_states_from_checkpoint = False\n |  \n |  resume_from_checkpoint = None\n |  \n |  run_name = None\n |  \n |  save_on_each_node = False\n |  \n |  save_only_model = False\n |  \n |  save_safetensors = True\n |  \n |  save_steps = 500\n |  \n |  save_strategy = 'steps'\n |  \n |  save_total_limit = None\n |  \n |  seed = 42\n |  \n |  skip_memory_metrics = True\n |  \n |  tf32 = None\n |  \n |  torch_compile = False\n |  \n |  torch_compile_backend = None\n |  \n |  torch_compile_mode = None\n |  \n |  torch_empty_cache_steps = None\n |  \n |  torchdynamo = None\n |  \n |  tpu_metrics_debug = False\n |  \n |  tpu_num_cores = None\n |  \n |  use_cpu = False\n |  \n |  use_ipex = False\n |  \n |  use_legacy_prediction_loop = False\n |  \n |  use_liger_kernel = False\n |  \n |  use_mps_device = False\n |  \n |  warmup_ratio = 0.0\n |  \n |  warmup_steps = 0\n |  \n |  weight_decay = 0.0\n\n","output_type":"stream"}],"execution_count":1},{"id":"d93d1834-fec0-4726-8e6d-895551c268df","cell_type":"code","source":"# IMDB dataset path\nimdb_csv = \"/kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\"\n\n# Bengali dataset TXT paths\npos_file = \"/kaggle/input/bengali-sentiment-classification/all_positive_8500.txt\"\nneg_file = \"/kaggle/input/bengali-sentiment-classification/all_negative_3307.txt\"\n\n# Convert Bangla TXT -> CSV\nimport pandas as pd, os\nwith open(pos_file, \"r\", encoding=\"utf-8\") as f:\n    pos_lines = f.read().splitlines()\nwith open(neg_file, \"r\", encoding=\"utf-8\") as f:\n    neg_lines = f.read().splitlines()\n\ndf_pos = pd.DataFrame({\"text\": pos_lines, \"label\": 1})\ndf_neg = pd.DataFrame({\"text\": neg_lines, \"label\": 0})\ndf_bn = pd.concat([df_pos, df_neg], ignore_index=True)\n\n# Save CSV locally so rest of pipeline can use it\nos.makedirs(\"data/bn\", exist_ok=True)\nbn_csv = \"data/bn/bengali_sentiment.csv\"\ndf_bn.to_csv(bn_csv, index=False, encoding=\"utf-8\")\n\nprint(\"IMDB CSV:\", imdb_csv)\nprint(\"Bengali CSV created:\", bn_csv)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:15:01.601629Z","iopub.execute_input":"2025-09-21T07:15:01.601948Z","iopub.status.idle":"2025-09-21T07:15:02.170741Z","shell.execute_reply.started":"2025-09-21T07:15:01.601921Z","shell.execute_reply":"2025-09-21T07:15:02.169996Z"}},"outputs":[{"name":"stdout","text":"IMDB CSV: /kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\nBengali CSV created: data/bn/bengali_sentiment.csv\n","output_type":"stream"}],"execution_count":2},{"id":"c82bd0a1","cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split  # âœ… required import\n\ndef normalize_en_label(x):\n    s = str(x).strip().lower()\n    if s in ['pos','positive','1','true','yes']:\n        return 1\n    if s in ['neg','negative','0','false','no']:\n        return 0\n    try:\n        v = int(float(s))\n        return 1 if v > 0 else 0\n    except:\n        pass\n    raise ValueError(f\"Unrecognized English label: {x}\")\n\ndef normalize_bn_label(x):\n    s = str(x).strip().lower()\n    if s in ['positive','pos','1','true','yes','à¦ªà¦œà¦¿à¦Ÿà¦¿à¦­','à¦‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•']:\n        return 1\n    if s in ['negative','neg','0','false','no','à¦¨à§‡à¦—à§‡à¦Ÿà¦¿à¦­','à¦¨à§‡à¦¤à¦¿à¦¬à¦¾à¦šà¦•']:\n        return 0\n    try:\n        v = int(float(s))\n        return 1 if v == 1 else 0\n    except:\n        pass\n    raise ValueError(f\"Unrecognized Bengali label: {x}\")\n\n# English dataset\ndf_en = pd.read_csv(imdb_csv)\nif \"review\" not in df_en.columns:\n    for cand in [\"text\", \"content\", \"review_text\", \"Review\"]:\n        if cand in df_en.columns:\n            df_en = df_en.rename(columns={cand: \"review\"})\nif \"sentiment\" not in df_en.columns:\n    for cand in [\"label\", \"target\", \"Sentiment\"]:\n        if cand in df_en.columns:\n            df_en = df_en.rename(columns={cand: \"sentiment\"})\n\ndf_en[\"label\"] = df_en[\"sentiment\"].apply(normalize_en_label)\ndf_en = df_en[[\"review\", \"label\"]].rename(columns={\"review\": \"text\"})\nen_train, en_dev = train_test_split(\n    df_en, test_size=0.2, random_state=42, stratify=df_en[\"label\"]\n)\n\n# Bengali dataset\ndf_bn = pd.read_csv(bn_csv)\nif \"label\" not in df_bn.columns:\n    # Already has text+label from TXT conversion, no normalization needed\n    pass\nelse:\n    text_col = next(\n        (c for c in [\"text\", \"content\", \"comment\", \"sentence\", \"review\", \"tweet\", \"Text\"] if c in df_bn.columns),\n        None\n    )\n    label_col = next(\n        (c for c in [\"label\", \"sentiment\", \"target\", \"polarity\", \"class\", \"Sentiment\"] if c in df_bn.columns),\n        None\n    )\n    df_bn = df_bn[[text_col, label_col]].rename(columns={text_col: \"text\", label_col: \"raw_label\"})\n    df_bn[\"label\"] = df_bn[\"raw_label\"].apply(normalize_bn_label)\n    df_bn = df_bn[[\"text\", \"label\"]]\n\nprint(\"English train/dev sizes:\", en_train.shape, en_dev.shape)\nprint(\"Bengali test size:\", df_bn.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:15:05.284623Z","iopub.execute_input":"2025-09-21T07:15:05.284969Z","iopub.status.idle":"2025-09-21T07:15:07.539199Z","shell.execute_reply.started":"2025-09-21T07:15:05.284945Z","shell.execute_reply":"2025-09-21T07:15:07.538471Z"}},"outputs":[{"name":"stdout","text":"English train/dev sizes: (40000, 2) (10000, 2)\nBengali test size: (11807, 2)\n","output_type":"stream"}],"execution_count":3},{"id":"8b59d0c8","cell_type":"code","source":"from sklearn.dummy import DummyClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import accuracy_score, classification_report\n\nbaseline_clf = make_pipeline(TfidfVectorizer(max_features=20000, ngram_range=(1,2)), DummyClassifier(strategy=\"most_frequent\"))\nbaseline_clf.fit(df_bn['text'], df_bn['label'])\nbn_pred_base = baseline_clf.predict(df_bn['text'])\n\nbn_baseline_acc = accuracy_score(df_bn['label'], bn_pred_base)\nprint(\"BN baseline accuracy:\", bn_baseline_acc)\nprint(classification_report(df_bn['label'], bn_pred_base, digits=4))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:15:11.293991Z","iopub.execute_input":"2025-09-21T07:15:11.294662Z","iopub.status.idle":"2025-09-21T07:15:11.826584Z","shell.execute_reply.started":"2025-09-21T07:15:11.294634Z","shell.execute_reply":"2025-09-21T07:15:11.825639Z"}},"outputs":[{"name":"stdout","text":"BN baseline accuracy: 0.7199119166596087\n              precision    recall  f1-score   support\n\n           0     0.0000    0.0000    0.0000      3307\n           1     0.7199    1.0000    0.8371      8500\n\n    accuracy                         0.7199     11807\n   macro avg     0.3600    0.5000    0.4186     11807\nweighted avg     0.5183    0.7199    0.6027     11807\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n","output_type":"stream"}],"execution_count":4},{"id":"9a22a49e","cell_type":"code","source":"from transformers import XLMRobertaTokenizerFast, XLMRobertaForSequenceClassification\nfrom transformers import DataCollatorWithPadding, TrainingArguments, Trainer\nfrom datasets import Dataset\nimport numpy as np\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\n# âœ… Always load from Kaggle dataset path (offline model)\nmodel_path = \"/kaggle/input/xlm-roberta-base-pytorch-model/xlm-roberta-base_pytorch_model\"\n\ntokenizer = XLMRobertaTokenizerFast.from_pretrained(model_path)\n\nid2label = {0: \"negative\", 1: \"positive\"}\nlabel2id = {\"negative\": 0, \"positive\": 1}\n\nmodel = XLMRobertaForSequenceClassification.from_pretrained(\n    model_path,\n    num_labels=2,\n    id2label=id2label,\n    label2id=label2id\n)\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n# âœ… Tokenize datasets\ntrain_ds = Dataset.from_pandas(en_train.reset_index(drop=True))\ndev_ds   = Dataset.from_pandas(en_dev.reset_index(drop=True))\n\ndef tokenize_fn(batch):\n    return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n\ntrain_tok = train_ds.map(tokenize_fn, batched=True, remove_columns=[c for c in train_ds.column_names if c not in [\"text\",\"label\"]])\ndev_tok   = dev_ds.map(tokenize_fn, batched=True, remove_columns=[c for c in dev_ds.column_names if c not in [\"text\",\"label\"]])\n\n# âœ… Metrics\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    preds = np.argmax(logits, axis=-1)\n    acc = accuracy_score(labels, preds)\n    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n\n# âœ… Training args (modern API works in 4.52.4)\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n   eval_strategy=\"epoch\",     # âœ… correct key\n    save_strategy=\"epoch\",\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=32,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    load_best_model_at_end=True,\n    metric_for_best_model=\"accuracy\",\n    logging_steps=50,\n    disable_tqdm=False,   # âœ… progress bar ON\n    report_to=\"none\"\n)\n\n# âœ… Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_tok,\n    eval_dataset=dev_tok,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)\n\n# âœ… Run training\ntrain_out = trainer.train()\nprint(\"Training complete. Best checkpoint:\", trainer.state.best_model_checkpoint)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:15:21.239147Z","iopub.execute_input":"2025-09-21T07:15:21.239429Z","iopub.status.idle":"2025-09-21T07:38:56.763656Z","shell.execute_reply.started":"2025-09-21T07:15:21.239410Z","shell.execute_reply":"2025-09-21T07:38:56.762776Z"}},"outputs":[{"name":"stderr","text":"2025-09-21 07:15:29.755406: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1758438930.084445      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1758438930.177317      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/xlm-roberta-base-pytorch-model/xlm-roberta-base_pytorch_model and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/40000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"744bbf2b38db43b1a147f06aefd37476"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8ab4bf6910624f349db9b6a9358ec463"}},"metadata":{}},{"name":"stderr","text":"/tmp/ipykernel_36/2347550879.py:60: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [1250/1250 22:27, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.222600</td>\n      <td>0.203126</td>\n      <td>0.921400</td>\n      <td>0.906600</td>\n      <td>0.939600</td>\n      <td>0.922805</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training complete. Best checkpoint: ./results/checkpoint-1250\n","output_type":"stream"}],"execution_count":5},{"id":"9822efe3","cell_type":"code","source":"dev_metrics = trainer.evaluate(eval_dataset=dev_tok)\nprint(\"EN Dev metrics:\", dev_metrics)\nen_dev_acc = dev_metrics.get(\"eval_accuracy\", None)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:41:56.790059Z","iopub.execute_input":"2025-09-21T07:41:56.790309Z","iopub.status.idle":"2025-09-21T07:43:27.066211Z","shell.execute_reply.started":"2025-09-21T07:41:56.790288Z","shell.execute_reply":"2025-09-21T07:43:27.065695Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"EN Dev metrics: {'eval_loss': 0.2031257301568985, 'eval_accuracy': 0.9214, 'eval_precision': 0.9065997684291779, 'eval_recall': 0.9396, 'eval_f1': 0.9228049499116087, 'eval_runtime': 90.2678, 'eval_samples_per_second': 110.781, 'eval_steps_per_second': 1.739, 'epoch': 1.0}\n","output_type":"stream"}],"execution_count":8},{"id":"150352b3","cell_type":"code","source":"# --- Make sure bn_tok exists, then evaluate on BN ---\n\nimport os, pandas as pd\nfrom datasets import Dataset\n\n# 1) Ensure we have a tokenize_fn\nif \"tokenize_fn\" not in globals():\n    def tokenize_fn(batch):\n        return tokenizer(batch[\"text\"], truncation=True, max_length=256)\n\n# 2) Build df_bn if it's not already in memory\nif \"df_bn\" not in globals():\n    # Try the Kaggle Bangla sentiment files you mentioned\n    pos_txt = \"/kaggle/input/bengali-sentiment-classification/all_positive_8500.txt\"\n    neg_txt = \"/kaggle/input/bengali-sentiment-classification/all_negative_3307.txt\"\n\n    if os.path.exists(pos_txt) and os.path.exists(neg_txt):\n        pos = pd.read_csv(pos_txt, header=None, names=[\"text\"])\n        pos[\"label\"] = 1\n        neg = pd.read_csv(neg_txt, header=None, names=[\"text\"])\n        neg[\"label\"] = 0\n        df_bn = pd.concat([pos, neg], ignore_index=True)\n    else:\n        raise FileNotFoundError(\n            \"Bangla files not found. Expected:\\n\"\n            f\"  {pos_txt}\\n  {neg_txt}\\n\"\n            \"If you have a CSV instead, load it into df_bn with columns ['text','label'] before running this cell.\"\n        )\n\n# 3) Create HF Dataset and tokenize\nbn_ds  = Dataset.from_pandas(df_bn.reset_index(drop=True))\nbn_tok = bn_ds.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[c for c in bn_ds.column_names if c not in [\"text\", \"label\"]],\n)\n\n# 4) Evaluate\nbn_metrics = trainer.evaluate(eval_dataset=bn_tok)\nprint(\"BN Test metrics:\", bn_metrics)\n\nbn_test_acc = bn_metrics.get(\"eval_accuracy\", None)\nprint(\"BN Test Accuracy:\", bn_test_acc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:45:16.695027Z","iopub.execute_input":"2025-09-21T07:45:16.695270Z","iopub.status.idle":"2025-09-21T07:46:11.755824Z","shell.execute_reply.started":"2025-09-21T07:45:16.695237Z","shell.execute_reply":"2025-09-21T07:46:11.755081Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/11807 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4403b9a81f1e4b53a469e366c073b882"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"BN Test metrics: {'eval_loss': 0.3763399124145508, 'eval_accuracy': 0.8284068772761921, 'eval_precision': 0.8983509721880384, 'eval_recall': 0.8588235294117647, 'eval_f1': 0.8781426681101888, 'eval_runtime': 53.9244, 'eval_samples_per_second': 218.955, 'eval_steps_per_second': 3.431, 'epoch': 1.0}\nBN Test Accuracy: 0.8284068772761921\n","output_type":"stream"}],"execution_count":11},{"id":"794ef8a4","cell_type":"code","source":"SAVE_DIR = \"./best_xlmr_en2bn_sentiment\"\ntrainer.save_model(SAVE_DIR)\ntokenizer.save_pretrained(SAVE_DIR)\nprint(\"Saved to:\", SAVE_DIR)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:46:51.889058Z","iopub.execute_input":"2025-09-21T07:46:51.889440Z","iopub.status.idle":"2025-09-21T07:46:55.848628Z","shell.execute_reply.started":"2025-09-21T07:46:51.889416Z","shell.execute_reply":"2025-09-21T07:46:55.847678Z"}},"outputs":[{"name":"stdout","text":"Saved to: ./best_xlmr_en2bn_sentiment\n","output_type":"stream"}],"execution_count":14},{"id":"0f1c52b5","cell_type":"code","source":"import matplotlib.pyplot as plt\n\nen_acc = float(en_dev_acc) if en_dev_acc is not None else 0.0\nbn_acc = float(bn_test_acc) if bn_test_acc is not None else 0.0\n\nprint(f\"EN Dev accuracy: {en_acc:.4f} | BN Test accuracy: {bn_acc:.4f}\")\n\nplt.bar([\"EN Dev\",\"BN Test\"], [en_acc, bn_acc])\nplt.title(\"Cross-lingual: EN->BN accuracy comparison (XLM-R)\")\nplt.ylabel(\"Accuracy\")\nplt.ylim(0,1)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:47:01.189531Z","iopub.execute_input":"2025-09-21T07:47:01.189799Z","iopub.status.idle":"2025-09-21T07:47:01.312503Z","shell.execute_reply.started":"2025-09-21T07:47:01.189782Z","shell.execute_reply":"2025-09-21T07:47:01.311875Z"}},"outputs":[{"name":"stdout","text":"EN Dev accuracy: 0.9214 | BN Test accuracy: 0.8284\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":15},{"id":"498b13ae-a69e-43ed-b596-43d7ed63af59","cell_type":"code","source":"# --- Load saved model + manual test + robust BN TXT loader + BN evaluation + threshold sweep ---\n\nimport os, glob, torch, numpy as np, pandas as pd\nfrom pathlib import Path\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n\n# 1) Fixed saved model directory (your path)\nMODEL_DIR = \"/kaggle/working/best_xlmr_en2bn_sentiment\"\nif not (os.path.isdir(MODEL_DIR) and os.path.exists(os.path.join(MODEL_DIR, \"config.json\"))):\n    raise FileNotFoundError(\n        f\"config.json not found in {MODEL_DIR}\\n\"\n        \"Make sure you called trainer.save_model('/kaggle/working/best_xlmr_en2bn_sentiment').\"\n    )\nprint(f\"âœ… Using model dir: {MODEL_DIR}\")\n\n# 2) Load model/tokenizer (CPU/GPU auto)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncfg = AutoConfig.from_pretrained(MODEL_DIR)\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR, use_fast=True)\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL_DIR, config=cfg).to(device)\nmodel.eval()\nid2label = getattr(cfg, \"id2label\", {0:\"negative\", 1:\"positive\"})\nlabel2id = {v:k for k,v in id2label.items()}\nprint(\"id2label:\", id2label)\n\n# 3) Helper: batch predict texts\ndef batch_predict(texts, batch_size=32, max_length=384, threshold=0.5):\n    all_logits = []\n    for i in range(0, len(texts), batch_size):\n        enc = tokenizer(\n            texts[i:i+batch_size],\n            truncation=True,\n            max_length=max_length,\n            padding=True,\n            return_tensors=\"pt\"\n        ).to(device)\n        with torch.no_grad():\n            out = model(**enc)\n        all_logits.append(out.logits.detach().cpu())\n    logits = torch.cat(all_logits, dim=0).numpy()\n    proba = torch.softmax(torch.tensor(logits), dim=-1).numpy()\n    if proba.shape[1] == 2:\n        pred = (proba[:, label2id.get(\"positive\", 1)] >= threshold).astype(int)\n    else:\n        pred = np.argmax(proba, axis=-1)\n    return {\"pred\": pred, \"proba\": proba, \"logits\": logits}\n\n# 4) Manual quick test (edit texts as you like)\nsample_texts = [\n    \"I absolutely loved this movie!\",\n    \"This was boring and a waste of time.\",\n    \"à¦›à¦¬à¦¿à¦Ÿà¦¾ à¦…à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦²à§‡à¦—à§‡à¦›à§‡à¥¤\",\n    \"à¦—à¦²à§à¦ªà¦Ÿà¦¾ à¦–à¦¾à¦°à¦¾à¦ª à¦›à¦¿à¦²à¥¤\"\n]\nres = batch_predict(sample_texts)\nfor t, p, pr in zip(sample_texts, res[\"pred\"], res[\"proba\"]):\n    pos_idx = label2id.get(\"positive\", 1)\n    neg_idx = label2id.get(\"negative\", 0)\n    print(f\"[{id2label[int(p)]}]  pos={pr[pos_idx]:.3f}  neg={pr[neg_idx]:.3f}  | {t}\")\n\n# 5) Robust loader for Bangla TXT files (one sentence per line)\npos_txt = \"/kaggle/input/bengali-sentiment-classification/all_positive_8500.txt\"\nneg_txt = \"/kaggle/input/bengali-sentiment-classification/all_negative_3307.txt\"\n\ndef load_one_sentence_per_line(path):\n    with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        lines = [ln.strip() for ln in f.read().splitlines()]\n    lines = [ln for ln in lines if ln]  # drop empty lines\n    return pd.DataFrame({\"text\": lines})\n\ndf_bn = None\nif os.path.exists(pos_txt) and os.path.exists(neg_txt):\n    pos = load_one_sentence_per_line(pos_txt); pos[\"label\"] = 1\n    neg = load_one_sentence_per_line(neg_txt); neg[\"label\"] = 0\n    df_bn = pd.concat([pos, neg], ignore_index=True)\n    print(f\"\\nLoaded BN dataset: pos={len(pos)}, neg={len(neg)}, total={len(df_bn)}\")\nelif \"df_bn\" in globals():\n    df_bn = globals()[\"df_bn\"]  # fallback if provided elsewhere\n    print(f\"\\nUsing df_bn from memory: total={len(df_bn)}\")\nelse:\n    print(\"\\n[INFO] BN TXT files not found at expected paths; skipping BN eval.\")\n\n# 6) Evaluate on BN + threshold sweep + save predictions\nif df_bn is not None and {\"text\", \"label\"} <= set(df_bn.columns):\n    # Evaluate at threshold=0.5\n    out = batch_predict(df_bn[\"text\"].tolist(), batch_size=64, max_length=384, threshold=0.5)\n    y_true = df_bn[\"label\"].to_numpy()\n    y_pred = out[\"pred\"]\n    acc  = accuracy_score(y_true, y_pred)\n    p, r, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n    print(f\"\\nBN @thr=0.5  Acc={acc:.4f}  P={p:.4f}  R={r:.4f}  F1={f1:.4f}\")\n    print(\"\\nClassification report (BN):\\n\", classification_report(y_true, y_pred, target_names=[\"negative\",\"positive\"], zero_division=0))\n    print(\"Confusion matrix (rows=true, cols=pred):\\n\", confusion_matrix(y_true, y_pred))\n\n    # Threshold sweep to maximize F1\n    proba_pos = out[\"proba\"][:, label2id.get(\"positive\", 1)]\n    best_thr, best_f1, best_acc = 0.5, -1, 0.0\n    for thr in np.linspace(0.05, 0.95, 181):\n        yp = (proba_pos >= thr).astype(int)\n        p_, r_, f1_, _ = precision_recall_fscore_support(y_true, yp, average=\"binary\", zero_division=0)\n        if f1_ > best_f1:\n            best_f1, best_thr, best_acc = f1_, thr, accuracy_score(y_true, yp)\n    print(f\"\\nBN best threshold â†’ thr={best_thr:.3f}, F1={best_f1:.4f}, Acc={best_acc:.4f}\")\n\n    # Save predictions\n    out_csv = Path(\"./bn_eval_preds.csv\")\n    pd.DataFrame({\n        \"text\": df_bn[\"text\"],\n        \"label\": y_true,\n        \"proba_neg\": out[\"proba\"][:, label2id.get(\"negative\", 0)],\n        \"proba_pos\": proba_pos,\n        \"pred@0.5\": y_pred,\n        f\"pred@{best_thr:.3f}\": (proba_pos >= best_thr).astype(int)\n    }).to_csv(out_csv, index=False)\n    print(f\"Saved BN predictions to: {out_csv.resolve()}\")\nelse:\n    print(\"\\n[INFO] BN DataFrame missing or lacks ['text','label'] columns; BN evaluation skipped.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-21T07:47:05.779056Z","iopub.execute_input":"2025-09-21T07:47:05.779334Z","iopub.status.idle":"2025-09-21T07:48:22.239123Z","shell.execute_reply.started":"2025-09-21T07:47:05.779316Z","shell.execute_reply":"2025-09-21T07:48:22.238463Z"}},"outputs":[{"name":"stdout","text":"âœ… Using model dir: /kaggle/working/best_xlmr_en2bn_sentiment\nid2label: {0: 'negative', 1: 'positive'}\n[positive]  pos=0.992  neg=0.008  | I absolutely loved this movie!\n[negative]  pos=0.025  neg=0.975  | This was boring and a waste of time.\n[positive]  pos=0.989  neg=0.011  | à¦›à¦¬à¦¿à¦Ÿà¦¾ à¦…à¦¸à¦¾à¦§à¦¾à¦°à¦£ à¦²à§‡à¦—à§‡à¦›à§‡à¥¤\n[negative]  pos=0.009  neg=0.991  | à¦—à¦²à§à¦ªà¦Ÿà¦¾ à¦–à¦¾à¦°à¦¾à¦ª à¦›à¦¿à¦²à¥¤\n\nLoaded BN dataset: pos=8500, neg=3307, total=11807\n\nBN @thr=0.5  Acc=0.8285  P=0.8985  R=0.8588  F1=0.8782\n\nClassification report (BN):\n               precision    recall  f1-score   support\n\n    negative       0.67      0.75      0.71      3307\n    positive       0.90      0.86      0.88      8500\n\n    accuracy                           0.83     11807\n   macro avg       0.79      0.80      0.79     11807\nweighted avg       0.84      0.83      0.83     11807\n\nConfusion matrix (rows=true, cols=pred):\n [[2482  825]\n [1200 7300]]\n\nBN best threshold â†’ thr=0.125, F1=0.8894, Acc=0.8299\nSaved BN predictions to: /kaggle/working/bn_eval_preds.csv\n","output_type":"stream"}],"execution_count":16},{"id":"5700894a-9923-4eac-967a-3046adda76d9","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}